{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key thing I'll work towards here is a `Model` that has a dictionary of `params` and a `forward` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the `Tensor` class. \n",
    "\n",
    "There are two concepts here: there is a Tensor that takes in Tensors as inputs and produces Tensors as outputs, and a Tensor that instead produces a _number_ as an output.\n",
    "\n",
    "The first we'll call a `Node`, and the second we'll call a `Loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import List, NamedTuple, Callable, Optional, Union, Iterator\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arrayable = Union[float, list, np.ndarray]\n",
    "\n",
    "Tensorable = Union['Tensor', float, np.ndarray]\n",
    "\n",
    "def ensure_tensor(tensorable: Tensorable) -> 'Tensor':\n",
    "    if isinstance(tensorable, Tensor):\n",
    "        return tensorable\n",
    "    else:\n",
    "        return Tensor(tensorable)\n",
    "    \n",
    "def ensure_array(arrayable: Arrayable) -> np.ndarray:\n",
    "    if isinstance(arrayable, np.ndarray):\n",
    "        return arrayable\n",
    "    else:\n",
    "        return np.array(arrayable)\n",
    "\n",
    "class Dependency(NamedTuple):\n",
    "    tensor: 'Tensor'\n",
    "    grad_fn: Callable[[np.ndarray], np.ndarray]\n",
    "        \n",
    "def collapse_sum(grad: np.ndarray,\n",
    "                 t: 'Tensor') -> np.ndarray:\n",
    "\n",
    "    # Sum out added dims\n",
    "    ndims_added = grad.ndim - t.data.ndim\n",
    "    for _ in range(ndims_added):\n",
    "        grad = grad.sum(axis=0)\n",
    "\n",
    "    # Sum across broadcasted (but non-added dims)\n",
    "    for i, dim in enumerate(t.shape):\n",
    "        if dim == 1:\n",
    "            grad = grad.sum(axis=i, keepdims=True)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data: np.ndarray,\n",
    "                 depends_on: List[Dependency] = None,\n",
    "                 no_grad: bool = False) -> None:\n",
    "        self.data = ensure_array(data)\n",
    "        self.depends_on = depends_on or []\n",
    "        self.no_grad = no_grad\n",
    "        self.shape = self.data.shape\n",
    "        self.grad: Optional['Tensor'] = None\n",
    "        if not self.no_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({self.data})\"\n",
    "            \n",
    "    def __add__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"gets called if I do t + other\"\"\"\n",
    "        return _add(self, ensure_tensor(other))\n",
    "\n",
    "    def __radd__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"gets called if I do other + t\"\"\"\n",
    "        return _add(ensure_tensor(other), self)\n",
    "\n",
    "    def __iadd__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"when we do t += other\"\"\"\n",
    "        self.data = self.data + ensure_tensor(other).data\n",
    "        return self    \n",
    "\n",
    "    def __isub__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"when we do t -= other\"\"\"\n",
    "        self.data = self.data - ensure_tensor(other).data\n",
    "        return self\n",
    "    \n",
    "    def __imul__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"when we do t *= other\"\"\"\n",
    "        self.data = self.data * ensure_tensor(other).data\n",
    "        return self\n",
    "\n",
    "    def __mul__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _mul(self, ensure_tensor(other))\n",
    "\n",
    "    def __rmul__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _mul(ensure_tensor(other), self)\n",
    "\n",
    "    def __matmul__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _matmul(self, other)\n",
    "\n",
    "    def __neg__(self) -> 'Tensor':\n",
    "        return _neg(self)\n",
    "\n",
    "    def __sub__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _sub(self, ensure_tensor(other))\n",
    "\n",
    "    def __rsub__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _sub(ensure_tensor(other), self)\n",
    "    \n",
    "    def __getitem__(self, idxs) -> 'Tensor':\n",
    "        return _slice(self, idxs)\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        self.grad = Tensor(np.zeros_like(self.data, dtype=np.float64),\n",
    "                           no_grad = True)\n",
    "    \n",
    "    def sum(self) -> 'Tensor':\n",
    "        return tensor_sum(self)\n",
    "    \n",
    "    def backward(self, grad: 'Tensor' = None) -> None:\n",
    "        # backward mostly going to be called on the loss after calling \"sum\"\n",
    "        if self.no_grad:\n",
    "            return\n",
    "        \n",
    "        if self.shape == ():\n",
    "            grad = Tensor(np.array(1.0))\n",
    "\n",
    "        \n",
    "        self.grad.data = self.grad.data + grad.data\n",
    "\n",
    "        for dependency in self.depends_on:\n",
    "            backward_grad = dependency.grad_fn(grad.data)\n",
    "            dependency.tensor.backward(Tensor(backward_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "\n",
    "    def _forward(t1: Tensor, t2: Tensor):\n",
    "        return t1.data + t2.data\n",
    "\n",
    "    def t1_grad(grad: np.ndarray):\n",
    "\n",
    "        grad = collapse_sum(grad, t1)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def t2_grad(grad: np.ndarray):\n",
    "\n",
    "        grad = collapse_sum(grad, t2)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    data = _forward(t1, t2)\n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def _mul(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "\n",
    "    def _forward(t1: Tensor, t2: Tensor):\n",
    "        return t1.data * t2.data\n",
    "\n",
    "    def t1_grad(grad: np.ndarray):\n",
    "        grad = grad * t2.data\n",
    "        grad = collapse_sum(grad, t1)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def t2_grad(grad: np.ndarray):\n",
    "        grad = grad * t1.data\n",
    "        grad = collapse_sum(grad, t2)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    data = _forward(t1, t2)\n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def _matmul(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "\n",
    "    assert t1.shape[1] == t2.shape[0]\n",
    "\n",
    "    def _forward(t1: Tensor, t2: Tensor):\n",
    "        return t1.data @ t2.data\n",
    "\n",
    "    def t1_grad(grad: np.ndarray):\n",
    "        grad = grad @ t2.data.T\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def t2_grad(grad: np.ndarray):\n",
    "        grad = t1.data.T @ grad\n",
    "\n",
    "        return grad\n",
    "\n",
    "    data = _forward(t1, t2)\n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def _neg(t: Tensor) -> Tensor:\n",
    "\n",
    "    def _forward(t: Tensor):\n",
    "        return -t.data\n",
    "\n",
    "    def t_grad(grad: np.ndarray):\n",
    "        return -grad\n",
    "\n",
    "    data = _forward(t)\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad),\n",
    "    ]\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def _sub(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "    return t1 + -t2\n",
    "\n",
    "def _slice(t: Tensor, idxs: slice) -> Tensor:\n",
    "\n",
    "    def _forward(t: Tensor):\n",
    "        return t.data[idxs]\n",
    "\n",
    "    data = _forward(t)    \n",
    "    \n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        bigger_grad = np.zeros_like(data)\n",
    "        try:\n",
    "            bigger_grad[idxs] = grad\n",
    "        except:\n",
    "            import pdb; pdb.set_trace()\n",
    "        return bigger_grad\n",
    "\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad),\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def tensor_sum(t: Tensor) -> Tensor:\n",
    "\n",
    "    def _forward(t: Tensor):\n",
    "        return t.data.sum()\n",
    "\n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad * np.ones_like(t.data)\n",
    "\n",
    "    data = _forward(t)\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad),\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Tensor):\n",
    "    def __init__(self, *shape) -> None:\n",
    "        data = np.random.randn(*shape)\n",
    "        super().__init__(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def parameters(self) -> Iterator[Parameter]:\n",
    "        for name, value in inspect.getmembers(self):\n",
    "            if isinstance(value, Parameter):\n",
    "                yield value\n",
    "            elif isinstance(value, Model):\n",
    "                yield from value.parameters()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for parameter in self.parameters():\n",
    "            parameter.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr: float = 0.01) -> None:\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, model: Model) -> None:\n",
    "        for parameter in model.parameters():\n",
    "            parameter -= parameter.grad * self.lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(t: Tensor) -> Tensor:\n",
    "    def _forward(t: Tensor):\n",
    "        return np.tanh(t.data)\n",
    "\n",
    "    data = _forward(t)\n",
    "    \n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad * (1 - data * data)\n",
    "\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad)\n",
    "    ]\n",
    "    \n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "y_train, y_test = y_train.reshape((-1,1)), y_test.reshape((-1,1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = (Tensor(X_train, no_grad=True),\n",
    "                                    Tensor(X_test, no_grad=True),\n",
    "                                    Tensor(y_train, no_grad=True),\n",
    "                                    Tensor(y_test, no_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonModel(Model):\n",
    "    def __init__(self, num_hidden: int = 13) -> None:\n",
    "        self.w1 = Parameter(13, num_hidden)\n",
    "        self.b1 = Parameter(num_hidden)\n",
    "\n",
    "        self.w2 = Parameter(num_hidden, 1)\n",
    "        self.b2 = Parameter(1)\n",
    "\n",
    "    def predict(self, inputs: Tensor) -> Tensor:\n",
    "        # inputs will be (batch_size, 13)\n",
    "        x1 = inputs @ self.w1 + self.b1  # (batch_size, num_hidden)\n",
    "        x2 = tanh(x1)                    # (batch_size, num_hidden)\n",
    "        x3 = x2 @ self.w2 + self.b2      # (batch_size, 4)\n",
    "\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Tensor(62876.462152720575)\n",
      "1 Tensor(43271.918829249815)\n",
      "2 Tensor(29745.67211898419)\n",
      "3 Tensor(20476.232541083977)\n",
      "4 Tensor(14789.277680349456)\n",
      "5 Tensor(11691.356242168664)\n",
      "6 Tensor(9993.953047534596)\n",
      "7 Tensor(9008.462128137302)\n",
      "8 Tensor(8370.862590280502)\n",
      "9 Tensor(7904.851412313703)\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(lr=0.0001)\n",
    "batch_size = 32\n",
    "model = BostonModel()\n",
    "train_size = X_train.shape[0]\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for start in range(0, train_size, batch_size):\n",
    "        end = start + batch_size\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        inputs = X_train[start:end]\n",
    "        inputs.no_grad = True\n",
    "\n",
    "        predicted = model.predict(inputs)\n",
    "        actual = y_train[start:end]\n",
    "        actual.no_grad = True\n",
    "        errors = predicted - actual\n",
    "        loss = (errors * errors).sum()\n",
    "#         import pdb; pdb.set_trace()\n",
    "        loss.backward()\n",
    "        optimizer.step(model)\n",
    "        \n",
    "        \n",
    "        # test predictions\n",
    "    predicted = model.predict(X_test)\n",
    "    errors = predicted - y_test\n",
    "    loss = (errors * errors).sum()\n",
    "        \n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
