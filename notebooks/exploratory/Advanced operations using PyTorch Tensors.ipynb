{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Lincoln to system path\n",
    "import sys\n",
    "sys.path.append(\"/Users/seth/development/lincoln/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operation with multiple inputs, gradients\n",
    "class Operation(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, *inputs) -> Tuple[Tensor]:\n",
    "\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "\n",
    "    def backward(self, *output_grads) -> Tuple[Tensor]:\n",
    "\n",
    "        assert_same_shapes(self.outputs, output_grads)\n",
    "\n",
    "        self._compute_grads(output_grads)\n",
    "\n",
    "        assert_same_shapes(self.inputs, self.input_grads)\n",
    "        return self.input_grads\n",
    "\n",
    "\n",
    "    def _compute_grads(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "        \n",
    "        self.input_grads = self._input_grads(output_grads)\n",
    "\n",
    "        return self.input_grads\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grads(self, *output_grads) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParamOperation with multiple gradients\n",
    "class ParamOperation(Operation):\n",
    "\n",
    "    def __init__(self, param: Tensor) -> Tensor:\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "\n",
    "    def _compute_grads(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "        self.input_grad = self._input_grad(output_grads)\n",
    "        self.param_grad = self._param_grad(output_grads)\n",
    "\n",
    "\n",
    "    def _param_grad(self, output_grad: Tensor) -> Tensor:\n",
    "        raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply class\n",
    "class Multiply(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise multiplication\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        return self.inputs[0] * self.inputs[1]\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        return self.inputs[1] * output_grads[0],\\\n",
    "               self.inputs[0] * output_grads[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert_same_shapes function\n",
    "def assert_same_shapes(tensors: Tuple[Tensor],\n",
    "                       tensor_grads: Tuple[Tensor]):\n",
    "    if len(tensors) == 1:\n",
    "        tensors = tensors[0]\n",
    "    if len(tensor_grads) == 1:\n",
    "        tensor_grads = tensor_grads[0]        \n",
    "\n",
    "    for tensor, tensor_grad in zip(tensors, tensor_grads):\n",
    "        assert tensor.shape == tensor_grad.shape, \\\n",
    "        '''\n",
    "        Two tensors should have the same shape; instead, first Tensor's shape is {0}\n",
    "        and second Tensor's shape is {1}.\n",
    "        '''.format(tuple(tensor_grad.shape), tuple(tensor.shape))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inputs to Multiply Operation\n",
    "m = Multiply()\n",
    "torch.manual_seed(102218)\n",
    "m1, m2 = torch.rand(2,2), torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8841, 0.2608],\n",
       "        [0.1206, 0.4546]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.forward(m1, m2) # tuple of length 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9344, 0.4609],\n",
       "         [0.4948, 0.5119]]), tensor([[0.9462, 0.5658],\n",
       "         [0.2438, 0.8881]]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_grad = torch.ones_like(m1)\n",
    "m.backward(m_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch `Multiply`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9462, 0.5658],\n",
      "        [0.2438, 0.8881]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(m1)\n",
    "print(m1.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detach to copy values\n",
    "m1_g = m1.detach()\n",
    "m2_g = m2.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require gradients\n",
    "m1_g.requires_grad = True\n",
    "m2_g.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform operation\n",
    "out = m1_g * m2_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define grad\n",
    "mul_grad = torch.ones_like(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send gradient backward\n",
    "out.backward(gradient=mul_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9344, 0.4609],\n",
      "        [0.4948, 0.5119]])\n",
      "tensor([[0.9462, 0.5658],\n",
      "        [0.2438, 0.8881]])\n"
     ]
    }
   ],
   "source": [
    "# same gradients as above!\n",
    "print(m1_g.grad)\n",
    "print(m2_g.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PyTorchOperation` base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Operation class\n",
    "class PyTorchOperation(Operation):\n",
    "\n",
    "    def __init__(self) -> Tensor:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, *inputs) -> Tuple[Tensor]:\n",
    "\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.inputs_with_grad = self._inputs_autograd()\n",
    "        \n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "        \n",
    "        \n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "    def _inputs_autograd(self) -> Tuple[Tensor]:\n",
    "        inputs_with_grad = tuple(inp.detach() for inp in self.inputs)\n",
    "        for inp in inputs_with_grad:\n",
    "            inp.requires_grad = True\n",
    "        return inputs_with_grad\n",
    "        \n",
    "\n",
    "    def _input_grads(self, output_grads: Tensor) -> Tensor:\n",
    "\n",
    "        for out, grad in zip(self.outputs, output_grads):\n",
    "            out.backward(gradient=grad)\n",
    "\n",
    "        input_grads = tuple()\n",
    "        for inp in self.inputs_with_grad:\n",
    "            input_grads = input_grads + (inp.grad,)\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply_PyTorch class\n",
    "class Multiply_PyTorch(PyTorchOperation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        \n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        return self.inputs_with_grad[0] * self.inputs_with_grad[1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Multiply stuff\n",
    "m = Multiply_PyTorch()\n",
    "mp1, mp2 = torch.rand(2,2), torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3988, 0.1799],\n",
      "        [0.6886, 0.0705]]) tensor([[0.3258, 0.4976],\n",
      "        [0.6935, 0.2254]])\n"
     ]
    }
   ],
   "source": [
    "print(mp1, mp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1299, 0.0895],\n",
       "         [0.4775, 0.0159]], grad_fn=<ThMulBackward>),)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward\n",
    "m.forward(mp1, mp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3258, 0.4976],\n",
       "         [0.6935, 0.2254]]), tensor([[0.3988, 0.1799],\n",
       "         [0.6886, 0.0705]]))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize grad and backward\n",
    "m.backward(torch.ones_like(mp1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complicated example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Divide two inputs into:\n",
    "    * `A`, `B`\n",
    "    * `A` gets copied into `A1` and `A2`\n",
    "    * `A1` and `A2` each get multiplied, bias added, activation, to create `C1` and `C2`.\n",
    "    * These get added together to create `D`.\n",
    "    * `B` and `D` get multiplied to create `E`. \n",
    "    \n",
    "Then the question is: what is the gradient of `E` with respect to `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy class\n",
    "class Copy(Operation):\n",
    "   \n",
    "    def __init__(self, num=2):\n",
    "        self.num = num\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise multiplication\n",
    "        '''\n",
    "        output = tuple()\n",
    "        for i in range(self.num):\n",
    "            output = output + self.inputs\n",
    "            \n",
    "        return output\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "        input_grad = torch.zeros_like(output_grads[0])\n",
    "        for grad in output_grads:\n",
    "             input_grad = input_grad + grad\n",
    "        return input_grad, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add2(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise addition\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        return self.inputs[0] + self.inputs[1]\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        return output_grads[0], output_grads[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tensors\n",
    "torch.manual_seed(102218)\n",
    "a, b = torch.rand(2,2), torch.rand(2,2)\n",
    "w1, w2 = torch.rand(2,2), torch.rand(2,2)\n",
    "b1, b2 = torch.rand(1,2), torch.rand(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set requires_grad = True for all\n",
    "for t in [a, b, w1, w2, b1, b2]:\n",
    "    t.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define operations\n",
    "c = Copy()\n",
    "add2 = Add2()\n",
    "mul = Multiply()\n",
    "wm1 = WeightMultiply(w1)\n",
    "ba1 = BiasAdd(b1)\n",
    "wm2 = WeightMultiply(w2)\n",
    "ba2 = BiasAdd(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define forward path\n",
    "a1, a2 = c.forward(a)\n",
    "c1 = ba1.forward(wm1.forward(a1))\n",
    "c2 = ba2.forward(wm2.forward(a2))\n",
    "d = add2.forward(c1, c2)\n",
    "e = mul.forward(b, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4230, 1.1577],\n",
       "        [1.2516, 1.0909]], grad_fn=<ThMulBackward>)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9232, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(e) # sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing gradient backwards\n",
    "e.backward(gradient=torch.ones_like(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9893, 1.4278],\n",
      "        [0.7053, 0.8353]])\n",
      "tensor([[2.5932, 2.5116],\n",
      "        [2.5298, 2.1312]])\n",
      "tensor([[1.0048, 0.5609],\n",
      "        [0.9681, 0.7154]])\n",
      "tensor([[1.4291, 0.9728]])\n"
     ]
    }
   ],
   "source": [
    "# all grads work\n",
    "print(a.grad)\n",
    "\n",
    "print(b.grad)\n",
    "\n",
    "print(w1.grad)\n",
    "\n",
    "print(b1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that increasing `a[0][0]` by `0.1` will increase the sum from `5.9232` to `5.9232 + 0.1 * 0.9893`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.02213"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define what sum with \"a_new\" should be: \n",
    "5.9232 + 0.1 * 0.9893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper function\n",
    "def sum_with_a(a):\n",
    "    a1, a2 = c.forward(a)\n",
    "    c1 = ba1.forward(wm1.forward(a1))\n",
    "    c2 = ba2.forward(wm2.forward(a2))\n",
    "    d = add2.forward(c1, c2)\n",
    "    e = mul.forward(b, d)\n",
    "    return torch.sum(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new version of a with one value incremented\n",
    "a_new = a.clone()\n",
    "a_new[0][0] += 0.1\n",
    "print(a_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9232, grad_fn=<SumBackward0>)\n",
      "tensor(6.0222, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sum_with_a(a))\n",
    "print(sum_with_a(a_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning this into a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Layer` class:\n",
    "\n",
    "* Has forward and backward methods. These simply loop through the operations, passing things forwards or backards.\n",
    "* On setup, requires a `num_in`\n",
    "\n",
    "`AutogradBlock` class:\n",
    "\n",
    "* Has forward method, which calls the `_output` method and `_setup` if it is the first iteration.\n",
    "* On setup, requires we define all ops in a `Dict[Operation]`. In addition, we define weights and give them a gradient.\n",
    "* On `_output`, we actually compute the output from the inputs.\n",
    "* We get the `param_grads` in a similar way we get them from `Layer`s - looping through a dictionary now instead of through a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouble is: this solution begs the question: why not write an autograd library and do everything that way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for autograd from scratch:\n",
    "\n",
    "* Could invest a week and unpack the example at `autodiff` in README.\n",
    "   * He makes heavy use of `einsum`, which is its own overhead.\n",
    "* Just use PyTorch (bad - why the operations? Might be ok to illustrate LSTM). \n",
    "\n",
    "Point is to illustrate how things work. Period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lincoln.operations.operations import WeightMultiply\n",
    "from lincoln.operations.operations import BiasAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutogradBlock with multiple gradients\n",
    "class AutogradBlock(object):\n",
    "\n",
    "    def __init__(self) -> Tensor:\n",
    "        super().__init__()\n",
    "        self.params: Dict[Tensor] = {}\n",
    "        self.ops: Dict[Operation] = {}\n",
    "        self.first: bool = True\n",
    "\n",
    "            \n",
    "    def _setup_block(self) -> Tuple[Tensor]:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def forward(self, *inputs) -> Tuple[Tensor]:\n",
    "\n",
    "        if self.first:\n",
    "            self._setup_block()\n",
    "            \n",
    "        self.inputs = inputs\n",
    "        \n",
    "        self.inputs_with_grad = self._inputs_autograd()\n",
    "\n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "    def _inputs_autograd(self) -> Tuple[Tensor]:\n",
    "        inputs_with_grad = tuple(inp.detach() for inp in self.inputs)\n",
    "        for inp in inputs_with_grad:\n",
    "            inp.requires_grad = True\n",
    "        return inputs_with_grad\n",
    "    \n",
    "    \n",
    "    def backward(self, *output_grads) -> Tuple[Tensor]:\n",
    "\n",
    "        assert_same_shapes(self.outputs, output_grads)\n",
    "\n",
    "        self.input_grads = self._input_grads(output_grads)\n",
    "    \n",
    "        assert_same_shapes(self.inputs, self.input_grads)\n",
    "        return self.input_grads\n",
    "\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "        \n",
    "        if len(output_grads) == 1:\n",
    "            self.outputs.backward(output_grads)\n",
    "        else: \n",
    "            for out, grad in zip(self.outputs, output_grads):\n",
    "                out.backward(gradient=grad)\n",
    "\n",
    "        input_grads = tuple()\n",
    "        for inp in self.inputs_with_grad:\n",
    "            input_grads = input_grads + (inp.grad,)\n",
    "        \n",
    "        return input_grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToyExample class\n",
    "class ToyExample(AutogradBlock):\n",
    "   \n",
    "    def __init__(self, \n",
    "                 seed=12345):\n",
    "        super().__init__()\n",
    "        self.seed = seed\n",
    "    \n",
    "        \n",
    "    def _setup_block(self) -> Tuple[Tensor]:\n",
    "\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.params['w1'] = torch.rand(2,2)\n",
    "        self.params['w2'] = torch.rand(2,2)\n",
    "        self.params['b1'] = torch.rand(1,2)\n",
    "        self.params['b2'] = torch.rand(1,2)\n",
    "        \n",
    "        self.ops['cp'] = Copy()\n",
    "        self.ops['add2'] = Add2()\n",
    "        self.ops['mul'] = Multiply()\n",
    "        self.ops['wm1'] = WeightMultiply(self.params['w1'])\n",
    "        self.ops['ba1'] = BiasAdd(self.params['b1'])        \n",
    "        self.ops['wm2'] = WeightMultiply(self.params['w2'])\n",
    "        self.ops['ba2'] = BiasAdd(self.params['b2'])\n",
    "        \n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        \n",
    "        a, b = self.inputs_with_grad \n",
    "        a1, a2 = self.ops['cp'].forward(a) \n",
    "        c1 = self.ops['ba1'].forward(self.ops['wm1'].forward(a1))\n",
    "        c2 = self.ops['ba2'].forward(self.ops['wm2'].forward(a2))\n",
    "        d = self.ops['add2'].forward(c1, c2)\n",
    "        \n",
    "        return self.ops['mul'].forward(b, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate\n",
    "t = ToyExample(seed=102218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9462, 0.5658],\n",
      "        [0.2438, 0.8881]])\n",
      "tensor([[0.9344, 0.4609],\n",
      "        [0.4948, 0.5119]])\n"
     ]
    }
   ],
   "source": [
    "# check a and b\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new input tensors\n",
    "torch.manual_seed(102218)\n",
    "a = torch.rand(2,2)\n",
    "b = torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.0692, 0.9283],\n",
      "        [1.0894, 0.8928]], grad_fn=<ThMulBackward>)\n",
      "tensor(5.9798, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "out = t.forward(a, b)\n",
    "print(out)\n",
    "print(torch.sum(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad # no grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.2304, 1.3354],\n",
       "         [1.4560, 1.0821]]), tensor([[3.2847, 2.0140],\n",
       "         [2.2020, 1.7442]]))"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.backward(torch.ones_like(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad # still no grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0462, 0.5658],\n",
      "        [0.2438, 0.8881]])\n"
     ]
    }
   ],
   "source": [
    "# new version of a\n",
    "a_new = a.clone()\n",
    "a_new[0][0] += 0.1\n",
    "print(a_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.2449, 0.9756],\n",
      "        [1.0894, 0.8928]], grad_fn=<ThMulBackward>)\n",
      "tensor(6.2028, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_new = t.forward(a_new, b)\n",
    "print(out_new)\n",
    "print(torch.sum(out_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.20284"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted value\n",
    "# old_sum + 0.1 * gradient\n",
    "5.9798 + 0.1 * 2.2304"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs and beyond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Node will be an `AutogradBlock` with three inputs and three outputs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM Layer` \n",
    "\n",
    "Input: series of word embeddings, cell state.\n",
    "\n",
    "Output: series of embeddings of the same size as the word, hidden state.\n",
    "\n",
    "Cell state will be an attribute of the layer.\n",
    "\n",
    "LSTM layer will have an additional function \"reset state\" that resets the hidden state to zero.\n",
    "\n",
    "## `LSTM Node` \n",
    "\n",
    "Each LSTM layer will have a series of a special kind of operation, `LSTM Node`. These nodes will have a series of special operations: they will take in as input an embedding and a hidden state and pass out an embedding and an updated hidden state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations of LSTM Node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Receives as input:\n",
    "\n",
    "* X (batch size x embedding dim)\n",
    "* H_prev (batch size x hidden dim)\n",
    "* C_prev (batch size x hidden dim)\n",
    "\n",
    "Diff components:\n",
    "\n",
    "Create \"Z\"\n",
    "Copy \"Z\"\n",
    "\n",
    "\n",
    "1. Z = Concat(X, H)\n",
    "1. Z1, Z2, Z3, Z4 = Copy(Z)\n",
    "1. F = WeightMultiply(Z1, W_f)\n",
    "1. F = BiasAdd(F, B_f)\n",
    "1. F_out = Sigmoid(F)\n",
    "1. I = WeightMultiply(Z2, W_i)\n",
    "1. I = BiasAdd(I, B_i)\n",
    "1. I_out = Sigmoid(I)\n",
    "1. C = WeightMultiply(Z3, W_c)\n",
    "1. C = BiasAdd(C, B_c)\n",
    "1. C_bar = Tanh(C, B_c)\n",
    "1. C1 = Multiply(F_out, C_prev)\n",
    "1. C2 = Multiply(I_out, C_bar)\n",
    "1. C_new = Add(C1, C2)\n",
    "1. O = WeightMutiply(Z4, W_o)\n",
    "1. O = Add(O, B_o)\n",
    "1. O_out = Sigmoid(O)\n",
    "1. C_tan = Tanh(C_new)\n",
    "1. H_new = Multiply(O_out, C_tan)\n",
    "1. H_out = WeightMultiply(H_new, W_v)\n",
    "1. X_out = BiasAdd(H_out, B_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues:\n",
    "\n",
    "* Concat operations needs to produce four distinct outputs. \n",
    "    * Potential solution: `Copy` operation that sums gradients.\n",
    "\n",
    "* Need a way to handle branching:\n",
    "* After `Copy`, there will be four `forward` operations happening.\n",
    "* Can't write `for operation in self.operations: operation.forward(X)`\n",
    "    * Maybe I can\n",
    "\n",
    "That's it!        \n",
    "1. Parameters are initialized at the `LSTMLayer` level. \n",
    "2. `LSTMNode` has a series of `ParamOperation`s.\n",
    "3. At the beginning of each operation's `forward` and `backward` methods, the nodes get parameters from the `LSTMLayer`.\n",
    "4. On the backward pass, they:\n",
    "    * Provide `param_grad`s for each `ParamOperation`.\n",
    "    * `LSTMNode` will need a function to return all of its `param_grad`s.\n",
    "5. The `LSTMLayer` can get the `param_grad`s from all of the `LSTMNode`s, aggregate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNode(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, \n",
    "                X_input: Tensor, \n",
    "                H_in: Tensor,\n",
    "                C_in: Tensor, \n",
    "                params: Dict[Tensor]):\n",
    "        \n",
    "        operations_1 = [\n",
    "            Concat(X_input, H_in),\n",
    "            Copy(4), \n",
    "        ] \n",
    "        \n",
    "        operations_2 = [\n",
    "            WeightMultiply(params['W_f']),\n",
    "            BiasAdd(params['B_f']),\n",
    "            Sigmoid()\n",
    "        ] # outputs F_out\n",
    "        \n",
    "        operations_3 = [\n",
    "            WeightMultiply(params['W_i']),\n",
    "            BiasAdd(params['B_i']),\n",
    "            Sigmoid()\n",
    "        ] # outputs I_out\n",
    "        \n",
    "        operations_4 = [\n",
    "            WeightMultiply(params['W_c']),\n",
    "            BiasAdd(params['B_c']),\n",
    "            Tanh()\n",
    "        ] # outputs C_bar\n",
    "        \n",
    "        operations_5 = [\n",
    "            Multiply(), # outputs C1\n",
    "            Multiply()\n",
    "        ]\n",
    "\n",
    "        operation_blocks = [\n",
    "            [operations_1, \n",
    "             [operations_2, operations_3]\n",
    "             , operations_4, oper]\n",
    "            \n",
    "        ]\n",
    "        self.input = input\n",
    "\n",
    "        self.output = self._output()\n",
    "\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, output_grad: Tensor) -> Tensor:\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self._compute_grads(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "\n",
    "    def _compute_grads(self, output_grad: Tensor) -> Tensor:\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> Tensor:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self, output_grad: Tensor) -> Tensor:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Z, Z = Concat(X, H_in) # Concat takes in two inputs produces one output, will have X_grad and H_in_grad\n",
    "\n",
    "Z_1, Z_2, Z_3 = Copy(Z, 3)\n",
    "\n",
    "F = WeightMultiply(Z_1, W_f) \n",
    "F = BiasAdd(F, B_f)\n",
    "F_out = Sigmoid(F)\n",
    "\n",
    "I = WeightMultiply(Z_2, W_i) \n",
    "I = BiasAdd(I, B_i)\n",
    "I_out = Sigmoid(I)\n",
    "\n",
    "C = WeightMultiply(Z_3, W_c)\n",
    "C = BiasAdd(C, B_c)\n",
    "C_bar = Tanh(C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations are the smallest units that have inputs and outputs defined by having a input and output.\n",
    "\n",
    "Going to need to build operations that:\n",
    "\n",
    "* Can take in one input and produce multiple outputs\n",
    "* If it produces multiple outputs, _gradients will need to accumulate_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Divide two inputs into:\n",
    "    * `A`, `B`\n",
    "    * `A` gets copied into `A1` and `A2`\n",
    "    * `A1` and `A2` each get multiplied, bias added, activation, to create `C1` and `C2`.\n",
    "    * These get added together to create `D`.\n",
    "    * `B` and `D` get multiplied to create `E`. \n",
    "    \n",
    "Then the question is: what is the gradient of `E` with respect to `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult(*args):\n",
    "    num = 1\n",
    "    print(type(args))\n",
    "    for arg in args:\n",
    "        num *= arg\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shapes(tensors: Tuple[Tensor],\n",
    "                       tensor_grads: Tuple[Tensor]):\n",
    "    for tensor, tensor_grad in zip(tensors, tensor_grads):\n",
    "        assert tensor.shape == tensor_grad.shape, \\\n",
    "        '''\n",
    "        Two tensors should have the same shape; instead, first Tensor's shape is {0}\n",
    "        and second Tensor's shape is {1}.\n",
    "        '''.format(tuple(tensor_grad.shape), tuple(tensor.shape))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New `Operation` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, *inputs) -> Tuple[Tensor]:\n",
    "\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "\n",
    "    def backward(self, *output_grads) -> Tuple[Tensor]:\n",
    "#         import pdb; pdb.set_trace()\n",
    "        assert_same_shapes(self.outputs, output_grads)\n",
    "\n",
    "        self._compute_grads(output_grads)\n",
    "\n",
    "        assert_same_shapes(self.inputs, self.input_grads)\n",
    "        return self.input_grads\n",
    "\n",
    "\n",
    "    def _compute_grads(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "        \n",
    "        self.input_grads = self._input_grads(output_grads)\n",
    "\n",
    "        return self.input_grads\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grads(self, *output_grads) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 8.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor([1,2])\n",
    "b = Tensor([3,4])\n",
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Multiply`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiply(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise multiplication\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        return (self.inputs[0] * self.inputs[1],)\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        return self.inputs[1] * output_grads[0],\\\n",
    "               self.inputs[0] * output_grads[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Multiply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([3., 8.]),)\n",
      "(tensor([3., 4.]), tensor([1., 2.]))\n"
     ]
    }
   ],
   "source": [
    "print(m.forward(a,b))\n",
    "print(m.backward(Tensor([1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Add2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add2(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise addition\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        return (self.inputs[0] + self.inputs[1],)\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        return output_grads[0], output_grads[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "add = Add2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 6.,  8.],\n",
      "        [10., 12.]]),)\n",
      "(tensor([[1., 1.],\n",
      "        [1., 1.]]), tensor([[1., 1.],\n",
      "        [1., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "print(add.forward(a,b))\n",
    "print(add.backward(torch.ones_like(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Concat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([[1,2],[3,4]])\n",
    "b = Tensor([[5,6],[7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.cat([a,b], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2.],\n",
       "         [3., 4.]]), tensor([[5., 6.],\n",
       "         [7., 8.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(c, [2, 2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(inp.shape[1] for inp in (a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concat2(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise multiplication\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        self.input_shapes = [inp.shape[1] for inp in self.inputs]\n",
    "        \n",
    "        return (torch.cat([a,b], dim=1), )\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "        return torch.split(output_grads[0], \n",
    "                           self.input_shapes,\n",
    "                           dim=1)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]]),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 1.]]), tensor([[1., 1.],\n",
       "         [1., 1.]]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Concat2()\n",
    "out = c.forward(a,b)\n",
    "print(out)\n",
    "c.backward(torch.ones_like(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.add?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 4.],\n",
       "         [6., 8.]]),)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp = Copy()\n",
    "out = cp.forward(a)\n",
    "cp.backward(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Divide two inputs into:\n",
    "    * `A`, `B`\n",
    "    * `A` gets copied into `A1` and `A2`\n",
    "    * `A1` and `A2` each get multiplied, bias added, activation, to create `C1` and `C2`.\n",
    "    * These get added together to create `D`.\n",
    "    * `B` and `D` get multiplied to create `E`. \n",
    "    \n",
    "Then the question is: what is the gradient of `E` with respect to `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderedDict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_1 = [Copy()]\n",
    "ops_2 = [[[WeightMultiply(w1), BiasAdd(b1), Sigmoid()],\n",
    "          [WeightMultiply(w1), BiasAdd(b1), Sigmoid()]],\n",
    "         Add2()]\n",
    "ops_3 = [Multiply()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Copy object at 0x1208a4c50>\n",
      "(tensor([[1., 2.],\n",
      "        [3., 4.]]),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor([[1., 2.],\n",
       "          [3., 4.]]),), (tensor([[1., 2.],\n",
       "          [3., 4.]]),))"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_op(ops_1, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three cases:\n",
    "\n",
    "* If `op` is an operation, simply call forward.\n",
    "* If `op` is a list and `op[0]` is an operation, call `forward_op` on all the operations in `op`\n",
    "* If `op` is a list and `op[0]` is a list, call `forward_op` on `op` and `inputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_op(ops, *inputs):\n",
    "    out = inputs\n",
    "    for op in ops:\n",
    "        if isinstance(op, list):\n",
    "            if isinstance(op[0], list):\n",
    "                out = tuple(forward_op(operation, inp) for operation, inp in zip(op, out))\n",
    "            else:\n",
    "                \n",
    "\n",
    "            \n",
    "        if len(out) == 1:\n",
    "            out = op.forward(out[0])\n",
    "        else:\n",
    "            out = op.forward(out)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If type of op is list, that means we want to apply the first element of the list to the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Copy object at 0x1208a4c50>\n",
      "(tensor([[1., 2.],\n",
      "        [3., 4.]]),)\n",
      "[[<__main__.WeightMultiply object at 0x1208a4f28>, <__main__.BiasAdd object at 0x1208a4b70>, Sigmoid], [<__main__.WeightMultiply object at 0x1208a4dd8>, <__main__.BiasAdd object at 0x1208a4da0>, Sigmoid]]\n",
      "((tensor([[1., 2.],\n",
      "        [3., 4.]]), tensor([[1., 2.],\n",
      "        [3., 4.]])),)\n",
      "2\n",
      "2\n",
      "[<__main__.WeightMultiply object at 0x1208a4f28>, <__main__.BiasAdd object at 0x1208a4b70>, Sigmoid]\n",
      "((tensor([[1., 2.],\n",
      "        [3., 4.]]), tensor([[1., 2.],\n",
      "        [3., 4.]])),)\n",
      "3\n",
      "2\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-275-ec34c8cd4396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mforward_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-274-1ec9260be5c6>\u001b[0m in \u001b[0;36mforward_op\u001b[0;34m(ops, *inputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-274-1ec9260be5c6>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-274-1ec9260be5c6>\u001b[0m in \u001b[0;36mforward_op\u001b[0;34m(ops, *inputs)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "forward_op(ops_2, forward_op(ops_1, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'forward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-c86f73cf34d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-233-c50c09d8f094>\u001b[0m in \u001b[0;36mforward_op\u001b[0;34m(ops, *inputs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'forward'"
     ]
    }
   ],
   "source": [
    "\n",
    "e = forward_op(ops_3, b, forward_op(ops_2, forward_op(ops_1, a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult(start, *args):\n",
    "    res = start\n",
    "    for num in args:\n",
    "        res *= num\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now how to do the backward pass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "\n",
    "    def __init__(self, param: Tensor) -> Tensor:\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "\n",
    "    def _compute_grads(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "        self.input_grad = self._input_grad(output_grads)\n",
    "        self.param_grad = self._param_grad(output_grads)\n",
    "\n",
    "\n",
    "    def _param_grad(self, output_grad: Tensor) -> Tensor:\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "\n",
    "    def __init__(self, W: Tensor):\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _outputs(self) -> Tensor:\n",
    "        return (torch.mm(self.inputs[0], self.param), )\n",
    "\n",
    "    def _input_grads(self, output_grad: Tensor) -> Tensor:\n",
    "        return (torch.mm(output_grad[0], self.param.transpose(0, 1)), )\n",
    "\n",
    "    def _param_grad(self, output_grad: Tensor) -> Tensor:\n",
    "        return torch.mm(self.inputs[0].transpose(0, 1), output_grad[0])\n",
    "\n",
    "\n",
    "class BiasAdd(ParamOperation):\n",
    "\n",
    "    def __init__(self,\n",
    "                 B: Tensor):\n",
    "        super().__init__(B)\n",
    "\n",
    "\n",
    "    def _outputs(self) -> Tensor:\n",
    "        return (torch.add(self.inputs[0], self.param), )\n",
    "\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "        return (torch.ones_like(self.inputs[0]) * output_grad[0], )\n",
    "\n",
    "\n",
    "    def _param_grad(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "        param_grad = torch.ones_like(self.param) * output_grad[0]\n",
    "        return torch.sum(param_grad, dim=0).reshape(1, param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    '''\n",
    "    Sigmoid activation function\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        return (1.0/(1.0+torch.exp(-1.0 * self.input)), )\n",
    "\n",
    "\n",
    "    def _input_grad(self, output_grad: Tensor) -> Tensor:\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return (input_grad, )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Sigmoid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: copy, and then weight multiply each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101218)\n",
    "op1 = Copy()\n",
    "w1, w2 = torch.rand(2,2), torch.rand(2,2)\n",
    "b1, b2 = torch.rand(1,2), torch.rand(1,2)\n",
    "op2, op3 = WeightMultiply(w1), WeightMultiply(w2)\n",
    "ops2 = [op2, op3]\n",
    "stages = [[op1], ops2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_in = (a,)\n",
    "for stage in stages:\n",
    "    stage_out = tuple()\n",
    "    for i, op in enumerate(stage):\n",
    "        stage_out = stage_out + op.forward(stage_in[i])\n",
    "    stage_in = stage_out\n",
    "final_out = stage_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ones_like_multiple(output: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "    return tuple(torch.ones_like(tensor) for tensor in output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<__main__.Copy at 0x12063a390>,\n",
       "  [[<__main__.WeightMultiply at 0x12063a3c8>,\n",
       "    <__main__.BiasAdd at 0x12063a5f8>,\n",
       "    Sigmoid],\n",
       "   [<__main__.WeightMultiply at 0x12063a4a8>,\n",
       "    <__main__.BiasAdd at 0x12063a438>,\n",
       "    Sigmoid]],\n",
       "  [<__main__.Add2 at 0x12063a2e8>]],\n",
       " <__main__.Multiply at 0x12063a7f0>]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops = [[[WeightMultiply(w1), BiasAdd(b1), Sigmoid()], \n",
    "             [WeightMultiply(w1), BiasAdd(b1), Sigmoid()]], Add2()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: full_ops[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<__main__.WeightMultiply at 0x12072ca58>,\n",
       "  <__main__.BiasAdd at 0x12072c3c8>,\n",
       "  Sigmoid],\n",
       " [<__main__.WeightMultiply at 0x12072ccf8>,\n",
       "  <__main__.BiasAdd at 0x12072c400>,\n",
       "  Sigmoid]]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ops[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a list of lists. It has length 2. \n",
    "\n",
    "That means ultimately, we want the first input to go through the first argument, and the second input to go through the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.WeightMultiply at 0x12072ca58>,\n",
       " <__main__.BiasAdd at 0x12072c3c8>,\n",
       " Sigmoid]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ops[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a non-nested list. This signals: take the first argument, and feed it through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.WeightMultiply at 0x12072ccf8>,\n",
       " <__main__.BiasAdd at 0x12072c400>,\n",
       " Sigmoid]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ops[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have another non-nested list. We must similarly: take the second argument, and feed it through. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll have two outputs, at the end of `full_ops[0]`. We move on the `full_ops[1]`, which indeed is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Add2 at 0x12072c9b0>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ops[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nested_list(nested_list):\n",
    "    for el in nested_list:\n",
    "        if isinstance(el, list):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual walkthrough: `full_ops_advanced`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops_advanced = [Copy(), \n",
    "  [[WeightMultiply(w1), BiasAdd(b1), Sigmoid()], \n",
    "   [WeightMultiply(w1), BiasAdd(b1), Sigmoid()]],\n",
    "  Add2()],\n",
    " Multiply()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<__main__.Copy at 0x12063a320>,\n",
       "  [[<__main__.WeightMultiply at 0x12063a160>,\n",
       "    <__main__.BiasAdd at 0x12083c8d0>,\n",
       "    Sigmoid],\n",
       "   [<__main__.WeightMultiply at 0x12083c2e8>,\n",
       "    <__main__.BiasAdd at 0x12083c208>,\n",
       "    Sigmoid]],\n",
       "  [<__main__.Add2 at 0x12083ceb8>]],\n",
       " <__main__.Multiply at 0x12083c5c0>]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ops_advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a list of length 2. This means: pass the first element to the first branch, and the second element to the second branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Copy at 0x12063a320>,\n",
       " [[<__main__.WeightMultiply at 0x12063a160>,\n",
       "   <__main__.BiasAdd at 0x12083c8d0>,\n",
       "   Sigmoid],\n",
       "  [<__main__.WeightMultiply at 0x12083c2e8>,\n",
       "   <__main__.BiasAdd at 0x12083c208>,\n",
       "   Sigmoid]],\n",
       " [<__main__.Add2 at 0x12083ceb8>]]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ops_advanced[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still a nested list. However, the first element is an operation. So, we want to start by feeding this element through the `Operation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Copy at 0x12063a320>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ops_advanced[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outputs two things, and indeed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<__main__.WeightMultiply at 0x12063a160>,\n",
       "  <__main__.BiasAdd at 0x12083c8d0>,\n",
       "  Sigmoid],\n",
       " [<__main__.WeightMultiply at 0x12083c2e8>,\n",
       "  <__main__.BiasAdd at 0x12083c208>,\n",
       "  Sigmoid]]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ops_advanced[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is of length two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have:\n",
    "\n",
    "* If the first element in the list is an operation, feed the input through it, and keep doing so until you reach an element in the list that is not an `Operation`.\n",
    "* At some point, you'll have outputs from the `Operation`s, and another (possibly nested) list of operations to deal with. At this point, you should call the main function again, with the `output`s from the first part, and the new operations. This function should return a number of outputs equal to the length of the list.\n",
    "* At this point, we'll have two outputs, and we'll move on to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Add2 at 0x12083cfd0>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ops_advanced[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take the outputs from the prior section and feed them through this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we reach another function. Since this function is next in the list, our code will have to assume that it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This nested list structure cannot support branching. Need to try another approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Multiply at 0x12083c128>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ops_advanced[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# New approach: using PyTorch's `autograd` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
