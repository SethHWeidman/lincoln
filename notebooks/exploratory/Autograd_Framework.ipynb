{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key thing I'll work towards here is a `Model` that has a dictionary of `params` and a `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import List, NamedTuple, Callable, Optional, Union, Iterator, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arrayable = Union[float, list, np.ndarray]\n",
    "\n",
    "Tensorable = Union['Tensor', float, np.ndarray]\n",
    "\n",
    "def ensure_tensor(tensorable: Tensorable) -> 'Tensor':\n",
    "    if isinstance(tensorable, Tensor):\n",
    "        return tensorable\n",
    "    else:\n",
    "        return Tensor(tensorable)\n",
    "    \n",
    "def ensure_array(arrayable: Arrayable) -> np.ndarray:\n",
    "    if isinstance(arrayable, np.ndarray):\n",
    "        return arrayable\n",
    "    else:\n",
    "        return np.array(arrayable)\n",
    "\n",
    "class Dependency(NamedTuple):\n",
    "    tensor: 'Tensor'\n",
    "    grad_fn: Callable[[np.ndarray], np.ndarray]\n",
    "        \n",
    "def collapse_sum(grad: np.ndarray,\n",
    "                 t: 'Tensor') -> np.ndarray:\n",
    "\n",
    "    # Sum out added dims\n",
    "    ndims_added = grad.ndim - t.data.ndim\n",
    "    for _ in range(ndims_added):\n",
    "        grad = grad.sum(axis=0)\n",
    "\n",
    "    # Sum across broadcasted (but non-added dims)\n",
    "    for i, dim in enumerate(t.shape):\n",
    "        if dim == 1:\n",
    "            grad = grad.sum(axis=i, keepdims=True)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data: np.ndarray,\n",
    "                 depends_on: List[Dependency] = None,\n",
    "                 no_grad: bool = False) -> None:\n",
    "        self.data = ensure_array(data)\n",
    "        self.depends_on = depends_on or []\n",
    "        self.no_grad = no_grad\n",
    "        self.shape = self.data.shape\n",
    "        self.grad: Optional['Tensor'] = None\n",
    "        if not self.no_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({np.round(self.data, 4)})\"\n",
    "            \n",
    "    def __add__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"gets called if I do t + other\"\"\"\n",
    "        return _add(self, ensure_tensor(other))\n",
    "\n",
    "    def __radd__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"gets called if I do other + t\"\"\"\n",
    "        return _add(ensure_tensor(other), self)\n",
    "\n",
    "    def __iadd__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"when we do t += other\"\"\"\n",
    "        self.data = self.data + ensure_tensor(other).data\n",
    "        return self    \n",
    "\n",
    "    def __isub__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"when we do t -= other\"\"\"\n",
    "        self.data = self.data - ensure_tensor(other).data\n",
    "        return self\n",
    "    \n",
    "    def __imul__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"when we do t *= other\"\"\"\n",
    "        self.data = self.data * ensure_tensor(other).data\n",
    "        return self\n",
    "\n",
    "    def __mul__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _mul(self, ensure_tensor(other))\n",
    "\n",
    "    def __rmul__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _mul(ensure_tensor(other), self)\n",
    "\n",
    "    def __matmul__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _matmul(self, other)\n",
    "\n",
    "    def __neg__(self) -> 'Tensor':\n",
    "        return _neg(self)\n",
    "\n",
    "    def __sub__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _sub(self, ensure_tensor(other))\n",
    "\n",
    "    def __rsub__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _sub(ensure_tensor(other), self)\n",
    "    \n",
    "    def __getitem__(self, idxs) -> 'Tensor':\n",
    "        return _slice(self, idxs)\n",
    "    \n",
    "    def concat(self, other: Tensorable) -> 'Tensor':\n",
    "        return _concat(self, ensure_tensor(other))\n",
    "    \n",
    "    def repeat(self, repeats: int) -> 'Tensor':\n",
    "        return _repeat(self, repeats)\n",
    "\n",
    "    def expand_dims_axis_1(self) -> 'Tensor':\n",
    "        return _expand_dims_axis_1(self)\n",
    "\n",
    "    def append_axis_1(self, other: Tensorable) -> 'Tensor':\n",
    "        return _append_axis_1(self, ensure_tensor(other))\n",
    "\n",
    "    def select_index_axis_1(self, ind: int) -> 'Tensor':\n",
    "        return _select_index_axis_1(self, ind)\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        self.grad = Tensor(np.zeros_like(self.data, dtype=np.float64),\n",
    "                           no_grad = True)\n",
    "    \n",
    "    def sum(self) -> 'Tensor':\n",
    "        return tensor_sum(self)\n",
    "    \n",
    "    def backward(self, grad: 'Tensor' = None) -> None:\n",
    "        # backward mostly going to be called on the loss after calling \"sum\"\n",
    "        if self.no_grad:\n",
    "            return\n",
    "        \n",
    "        if self.shape == ():\n",
    "            grad = Tensor(np.array(1.0))\n",
    "        \n",
    "        self.grad.data = self.grad.data + grad.data\n",
    "\n",
    "        for dependency in self.depends_on:\n",
    "            backward_grad = dependency.grad_fn(grad.data)\n",
    "            dependency.tensor.backward(Tensor(backward_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "\n",
    "    def _forward(t1: Tensor, t2: Tensor) -> np.ndarray:\n",
    "        return t1.data + t2.data\n",
    "\n",
    "    def t1_grad(grad: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        grad = collapse_sum(grad, t1)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def t2_grad(grad: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        grad = collapse_sum(grad, t2)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    data = _forward(t1, t2)\n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def _mul(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "\n",
    "    def _forward(t1: Tensor, t2: Tensor) -> np.ndarray:\n",
    "        return t1.data * t2.data\n",
    "\n",
    "    def t1_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        grad = grad * t2.data\n",
    "        grad = collapse_sum(grad, t1)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def t2_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        grad = grad * t1.data\n",
    "        grad = collapse_sum(grad, t2)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    data = _forward(t1, t2)\n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def _matmul(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "\n",
    "    assert t1.shape[1] == t2.shape[0]\n",
    "\n",
    "    def _forward(t1: Tensor, t2: Tensor) -> np.ndarray:\n",
    "        return t1.data @ t2.data\n",
    "\n",
    "    def t1_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        grad = grad @ t2.data.T\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def t2_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        grad = t1.data.T @ grad\n",
    "\n",
    "        return grad\n",
    "\n",
    "    data = _forward(t1, t2)\n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def _neg(t: Tensor) -> Tensor:\n",
    "\n",
    "    def _forward(t: Tensor) -> np.ndarray:\n",
    "        return -t.data\n",
    "\n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return -grad\n",
    "\n",
    "    data = _forward(t)\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad),\n",
    "    ]\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def _sub(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "    return t1 + -t2\n",
    "\n",
    "def _slice(t: Tensor, idxs: slice) -> Tensor:\n",
    "\n",
    "    def _forward(t: Tensor, idxs):\n",
    "        return t.data[idxs]\n",
    "\n",
    "    data = _forward(t, idxs)    \n",
    "    \n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        bigger_grad = np.zeros_like(t.data)\n",
    "        bigger_grad[idxs] = grad\n",
    "        return bigger_grad\n",
    "\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad),\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on, t.no_grad)\n",
    "\n",
    "def tensor_sum(t: Tensor) -> Tensor:\n",
    "\n",
    "    def _forward(t: Tensor):\n",
    "        return t.data.sum()\n",
    "\n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad * np.ones_like(t.data)\n",
    "\n",
    "    data = _forward(t)\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad),\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on)        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _concat(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "    \n",
    "    assert t1.shape[0] == t2.shape[0],\\\n",
    "    \"Concatenated Tensors must have the same shape along first dimension\"\n",
    "    \n",
    "    def _forward(t1: Tensor, t2: Tensor):\n",
    "        return np.concatenate([t1.data, t2.data], axis=1)    \n",
    "    \n",
    "    def t1_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad[:,:t1.shape[1]]\n",
    "    \n",
    "    def t2_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad[:,t1.shape[1]:]\n",
    "\n",
    "    data = _forward(t1, t2)\n",
    "    \n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _repeat(t: Tensor, repeats: int) -> Tensor:\n",
    "\n",
    "    assert t.shape[0] == 1,\\\n",
    "    \"Repeat operation should only be used on rows\"\n",
    "    \n",
    "    def _forward(t: Tensor, repeats: int) -> np.ndarray:\n",
    "        return np.repeat(t.data, repeats, axis=0)    \n",
    "    \n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad.sum(axis=0)\n",
    "\n",
    "    data = _forward(t, repeats)\n",
    "    \n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad)\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stack(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "    '''\n",
    "    Stacks two 2d Tensors along axis 1, creating a \n",
    "    '''\n",
    "    assert t.shape[0] == 1,\\\n",
    "    \"Repeat operation should only be used on rows\"\n",
    "    \n",
    "    def _forward(t: Tensor, repeats: int) -> np.ndarray:\n",
    "        return np.repeat(t.data, repeats, axis=0)    \n",
    "    \n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad.sum(axis=0)\n",
    "\n",
    "    data = _forward(t, repeats)\n",
    "    \n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad)\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_dims_axis_1(t: Tensor) -> Tensor:\n",
    "    \n",
    "    assert t.data.ndim == 2\n",
    "    \n",
    "    def _forward(t: Tensor) -> np.ndarray:\n",
    "        return np.expand_dims(t.data, axis=1)    \n",
    "    \n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        assert grad.ndim == 3\n",
    "        \n",
    "        return grad[:, 0, :]\n",
    "\n",
    "    data = _forward(t)\n",
    "    \n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad)\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _append_axis_1(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "    \n",
    "    assert t1.data.ndim == t2.data.ndim == 3\n",
    "    \n",
    "    def _forward(t1: Tensor, t2: Tensor) -> np.ndarray:\n",
    "        return np.append(t1.data, t2.data, axis=1)    \n",
    "    \n",
    "    def t1_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        assert grad.ndim == 3\n",
    "        \n",
    "        return grad[:, :t1.shape[1], :]\n",
    "    \n",
    "    def t2_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        assert grad.ndim == 3\n",
    "        \n",
    "        return grad[:, t1.shape[1]:, :]\n",
    "\n",
    "    data = _forward(t1, t2)\n",
    "    \n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_index_axis_1(t: Tensor, ind: int) -> Tensor:\n",
    "    \n",
    "    assert t.data.ndim == 3\n",
    "    \n",
    "    def _forward(t: Tensor, ind: int) -> np.ndarray:\n",
    "        return t.data[:, ind, :]    \n",
    "\n",
    "    data = _forward(t, ind)\n",
    "    \n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        assert grad.ndim == 2\n",
    "        \n",
    "        bigger_grad = np.zeros_like(t.data)\n",
    "        bigger_grad[:, ind, :] = grad\n",
    "        return bigger_grad\n",
    "\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad)\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on, t.no_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `concat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing concat forward\n",
    "\n",
    "a = np.random.randn(2, 3)\n",
    "b = np.random.randn(2, 5)\n",
    "\n",
    "c = np.concatenate([a, b], axis=1)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat grad calculations\n",
    "a_grad = c[:,:a.shape[1]]\n",
    "b_grad = c[:,a.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 5)\n"
     ]
    }
   ],
   "source": [
    "print(a_grad.shape)\n",
    "print(b_grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing concat forward\n",
    "\n",
    "aT = Tensor(a)\n",
    "bT = Tensor(b)\n",
    "c = aT.concat(bT)\n",
    "\n",
    "d = c.sum()\n",
    "\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[1. 1. 1.]\n",
      " [1. 1. 1.]])\n",
      "Tensor([[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(aT.grad)\n",
    "print(bT.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `repeat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8143,  0.8007, -0.0606],\n",
       "       [-0.8143,  0.8007, -0.0606]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing repeat forward\n",
    "\n",
    "a = np.random.randn(1, 3)\n",
    "\n",
    "np.repeat(a, 2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2144, -4.5926, -0.6135])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat backward\n",
    "\n",
    "a = np.random.randn(1, 3)\n",
    "\n",
    "a2 = np.repeat(a, 2, axis=0)\n",
    "\n",
    "a2.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat with tensors\n",
    "\n",
    "a = np.random.randn(1, 3)\n",
    "b = np.random.randn(3, 3)\n",
    "\n",
    "aT = Tensor(a)\n",
    "bT = Tensor(b)\n",
    "a2 = aT.repeat(3)\n",
    "c = (a2 + bT).sum()\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[3. 3. 3.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aT.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing `slice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.random.randn(3,3,4))\n",
    "b = a[1:2, :, :]\n",
    "print(b.shape)\n",
    "c = b.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(-0.3911)\n"
     ]
    }
   ],
   "source": [
    "print(c)\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[[0. 0. 0. 0.]\n",
       "  [0. 0. 0. 0.]\n",
       "  [0. 0. 0. 0.]]\n",
       "\n",
       " [[1. 1. 1. 1.]\n",
       "  [1. 1. 1. 1.]\n",
       "  [1. 1. 1. 1.]]\n",
       "\n",
       " [[0. 0. 0. 0.]\n",
       "  [0. 0. 0. 0.]\n",
       "  [0. 0. 0. 0.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing `select_index_i`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.random.randn(3,3,4))\n",
    "b = a.select_index_axis_1(1)\n",
    "print(b.shape)\n",
    "c = b.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[[0. 0. 0. 0.]\n",
       "  [1. 1. 1. 1.]\n",
       "  [0. 0. 0. 0.]]\n",
       "\n",
       " [[0. 0. 0. 0.]\n",
       "  [1. 1. 1. 1.]\n",
       "  [0. 0. 0. 0.]]\n",
       "\n",
       " [[0. 0. 0. 0.]\n",
       "  [1. 1. 1. 1.]\n",
       "  [0. 0. 0. 0.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Tensor):\n",
    "    def __init__(self, *shape) -> None:\n",
    "        data = np.random.randn(*shape)\n",
    "        super().__init__(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def parameters(self) -> Iterator[Parameter]:\n",
    "        for name, value in inspect.getmembers(self):\n",
    "            if isinstance(value, Parameter):\n",
    "                yield value\n",
    "            elif isinstance(value, Model):\n",
    "                yield from value.parameters()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for parameter in self.parameters():\n",
    "            parameter.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr: float = 0.01) -> None:\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, model: Model) -> None:\n",
    "        for parameter in model.parameters():\n",
    "            parameter -= parameter.grad * self.lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(t: Tensor) -> Tensor:\n",
    "    def _forward(t: Tensor):\n",
    "        return np.tanh(t.data)\n",
    "\n",
    "    data = _forward(t)\n",
    "    \n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad * (1 - data * data)\n",
    "\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad)\n",
    "    ]\n",
    "    \n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t: Tensor) -> Tensor:\n",
    "    \n",
    "    def _forward(t: Tensor) -> np.ndarray:\n",
    "        return 1.0 / (1.0 + np.exp(-(t.data)))\n",
    "\n",
    "    data = _forward(t)\n",
    "    \n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad * data * (1.0 - data)\n",
    "\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad)\n",
    "    ]\n",
    "    \n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "y_train, y_test = y_train.reshape((-1,1)), y_test.reshape((-1,1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = (Tensor(X_train, no_grad=True),\n",
    "                                    Tensor(X_test, no_grad=True),\n",
    "                                    Tensor(y_train, no_grad=True),\n",
    "                                    Tensor(y_test, no_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonModel(Model):\n",
    "    def __init__(self, \n",
    "                 num_hidden: int = 13,\n",
    "                 seed: int = 1) -> None:\n",
    "        np.random.seed(seed)\n",
    "        self.w1 = Parameter(13, num_hidden)\n",
    "        self.b1 = Parameter(num_hidden)\n",
    "\n",
    "        self.w2 = Parameter(num_hidden, 1)\n",
    "        self.b2 = Parameter(1)\n",
    "\n",
    "    def predict(self, inputs: Tensor) -> Tensor:\n",
    "        # inputs will be (batch_size, 13)\n",
    "        x1 = inputs @ self.w1 + self.b1  # (batch_size, num_hidden)\n",
    "        x2 = tanh(x1)                    # (batch_size, num_hidden)\n",
    "        x3 = x2 @ self.w2 + self.b2      # (batch_size, 4)\n",
    "\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Tensor(10929.9298)\n",
      "1 Tensor(5931.3808)\n",
      "2 Tensor(5032.0439)\n",
      "3 Tensor(4765.6974)\n",
      "4 Tensor(4596.3881)\n",
      "5 Tensor(4585.5644)\n",
      "6 Tensor(4554.1706)\n",
      "7 Tensor(4486.7719)\n",
      "8 Tensor(4400.3648)\n",
      "9 Tensor(4251.6124)\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(lr=0.001)\n",
    "batch_size = 32\n",
    "model = BostonModel(seed=112418)\n",
    "train_size = X_train.shape[0]\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for start in range(0, train_size, batch_size):\n",
    "        end = start + batch_size\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        inputs = X_train[start:end]\n",
    "#         inputs.no_grad = True\n",
    "\n",
    "        predicted = model.predict(inputs)\n",
    "        actual = y_train[start:end]\n",
    "#         actual.no_grad = True\n",
    "\n",
    "        errors = predicted - actual\n",
    "        loss = (errors * errors).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step(model)\n",
    "        \n",
    "        \n",
    "        # test predictions\n",
    "    predicted = model.predict(X_test)\n",
    "    errors = predicted - y_test\n",
    "    loss = (errors * errors).sum()\n",
    "        \n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Tensor Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple forward with addition\n",
    "\n",
    "```\n",
    "a \n",
    " \\\n",
    "  c - s\n",
    " /\n",
    "b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0. 0.]\n",
      " [0. 0.]])\n",
      "Tensor([[1. 1.]\n",
      " [1. 1.]])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112518)\n",
    "a = Tensor(np.random.randn(2,2))\n",
    "b = Tensor(np.random.randn(2,2))\n",
    "c = a + b\n",
    "s = c.sum()\n",
    "\n",
    "print(a.grad)\n",
    "s.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple forward with branching\n",
    "\n",
    "```\n",
    "a \n",
    " \\\n",
    "  c1 \n",
    " /  \\\n",
    "b\n",
    "      s\n",
    "a \n",
    " \\   /\n",
    "  c2 \n",
    " /\n",
    "b \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0. 0.]\n",
      " [0. 0.]])\n",
      "Tensor([[0.6462 2.4838]\n",
      " [1.0027 1.1719]])\n",
      "\n",
      "a: Tensor([[ 2.1617 -0.4313]\n",
      " [ 0.3818  0.7105]])\n",
      "s: Tensor(2.8455)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112518)\n",
    "a = Tensor(np.random.randn(2,2))\n",
    "b = Tensor(np.random.randn(2,2))\n",
    "c1 = a + b\n",
    "c2 = a * b\n",
    "s = (c1 + c2).sum()\n",
    "\n",
    "print(a.grad)\n",
    "s.backward()\n",
    "print(a.grad)\n",
    "print()\n",
    "print(\"a:\", a)\n",
    "print(\"s:\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that increasing `a[0][0]` by 0.1 from `2.1617` to `2.2617` would increase `s` from `2.8455` to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.91012"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.8455 + 0.1 * 0.6462"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sum(a_first_val):\n",
    "    \n",
    "    np.random.seed(112518)\n",
    "    a = np.random.randn(2,2)\n",
    "    b = np.random.randn(2,2)\n",
    "    a2 = a.copy()\n",
    "    a2[0][0] = a_first_val\n",
    "    c1 = a2 + b\n",
    "    c2 = a2 * b\n",
    "    return (c1 + c2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8454767470862894\n",
      "2.910093928197016\n"
     ]
    }
   ],
   "source": [
    "print(check_sum(2.1617))\n",
    "print(check_sum(2.2617))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the same array multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: Tensor([[-0.0683  0.6959]\n",
      " [ 1.4791  0.5237]])\n",
      "s: Tensor(-1.2067)\n",
      "w.grad: Tensor([[-0.5692 -0.3029]\n",
      " [-0.0847 -1.7367]])\n"
     ]
    }
   ],
   "source": [
    "w = Tensor(np.random.randn(2,2))\n",
    "a = Tensor(np.random.randn(2,2))\n",
    "b = Tensor(np.random.randn(2,2))\n",
    "c = w + a\n",
    "s = (w * b).sum()\n",
    "\n",
    "s.backward()\n",
    "\n",
    "print(\"w:\", w)\n",
    "print(\"s:\", s)\n",
    "print(\"w.grad:\", w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031700000000000006"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.0683 + 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that increasing `w[0][0]` by 0.1 from `-0.0683` to `0.0317` would increase `s` from `-1.2067` to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2636200000000002"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1.2067 + 0.1 * -0.5692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sum(w_first_val):\n",
    "    \n",
    "    w2 = w.data.copy()\n",
    "    w2[0][0] = w_first_val\n",
    "    c.data = w2 + a.data\n",
    "    return (w2 * b.data).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.206689062693263\n",
      "-1.263613361478857\n"
     ]
    }
   ],
   "source": [
    "print(check_sum(-0.0683))\n",
    "print(check_sum(0.0317))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding bias multiple times\n",
    "\n",
    "Will mimic adding hidden state multiple times in an LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultimate goal:\n",
    "\n",
    "```python\n",
    "def lstm_node(inputs: Tensor, \n",
    "              hiddens: Tensor, \n",
    "              cells: Tensor, \n",
    "              params: Dict[str, Tensor]):\n",
    "    \n",
    "    assert input.shape[0] == hidden.shape[0] == cell.shape[0]\n",
    "    \n",
    "    Z = inputs.concat(hidden)\n",
    "    \n",
    "    forget = sigmoid(Z @ params['Wf'] + params['Bf'])\n",
    "\n",
    "    ingate = sigmoid(Z @ params['Wi'] + params['Bi'])\n",
    "    \n",
    "    outgate = sigmoid(Z @ params['Wo'] + params['Bo'])\n",
    "    \n",
    "    change = tanh(Z @ params['Wc'] + params['Bc'])\n",
    "    \n",
    "    cells = cells * forget + ingate * change\n",
    "    \n",
    "    hiddens = outgate * tanh(cells)\n",
    "    \n",
    "    outputs = hidden @ params['Wv'] + params['Bv']\n",
    "    \n",
    "    return outputs, hiddens, cells\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(batch_size: int,\n",
    "                state_size: int,\n",
    "                vocab_size: int) -> Dict[str, Parameter]:\n",
    "    \n",
    "    params = {}\n",
    "    params['Wf'] = Parameter(state_size + vocab_size, state_size)\n",
    "    params['Wi'] = Parameter(state_size + vocab_size, state_size)\n",
    "    params['Wo'] = Parameter(state_size + vocab_size, state_size)\n",
    "    params['Wc'] = Parameter(state_size + vocab_size, state_size)\n",
    "    params['Wv'] = Parameter(state_size, vocab_size)\n",
    "    \n",
    "    params['Bf'] = Parameter(state_size)\n",
    "    params['Bi'] = Parameter(state_size)\n",
    "    params['Bo'] = Parameter(state_size)\n",
    "    params['Bc'] = Parameter(state_size)\n",
    "    params['Bv'] = Parameter(vocab_size)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "batch_size = 12\n",
    "state_size = 20\n",
    "vocab_size = 30\n",
    "\n",
    "np.random.seed(112518)\n",
    "\n",
    "# initial data\n",
    "h_init = Tensor(np.random.randn(1, state_size))\n",
    "c_init = Tensor(np.random.randn(1, state_size))\n",
    "inputs = Tensor(np.random.randn(batch_size, vocab_size))\n",
    "\n",
    "# initialize params\n",
    "params = init_params(batch_size, state_size, vocab_size)\n",
    "\n",
    "# repeat cell and hidden\n",
    "hiddens = h_init.repeat(batch_size)\n",
    "cells = c_init.repeat(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_node(inputs: Tensor, \n",
    "              hiddens: Tensor, \n",
    "              cells: Tensor, \n",
    "              params: Dict[str, Parameter]):\n",
    "\n",
    "    assert inputs.shape[0] == hiddens.shape[0] == cells.shape[0]\n",
    "\n",
    "    Z = inputs.concat(hiddens)\n",
    "\n",
    "    forget = sigmoid(Z @ params['Wf'] + params['Bf'])\n",
    "\n",
    "    ingate = sigmoid(Z @ params['Wi'] + params['Bi'])\n",
    "\n",
    "    outgate = sigmoid(Z @ params['Wo'] + params['Bo'])\n",
    "\n",
    "    change = tanh(Z @ params['Wc'] + params['Bc'])\n",
    "\n",
    "    cells = cells * forget + ingate * change\n",
    "\n",
    "    hiddens = outgate * tanh(cells)\n",
    "\n",
    "    outputs = hiddens @ params['Wv'] + params['Bv']\n",
    "\n",
    "    return outputs, hiddens, cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_out, h_out, c_out = lstm_node(inputs, hiddens, cells, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 30)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_out.shape # (batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = o_out.sum()\n",
    "s.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(70.9797)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[ 2.1617 -0.4313  0.3818  0.7105 -0.3538  1.4838  0.0027  0.1719 -0.0683\n",
      "   0.6959  1.4791  0.5237  1.0117  0.0821 -0.2638 -1.4929 -0.5692 -0.3029\n",
      "  -0.0847 -1.7367]])\n",
      "Tensor([[  5.8457  -3.3987   3.0234   9.0145   7.7679  -2.7426   3.7362 -25.35\n",
      "   -7.5784 -19.3007  17.2762   8.8289 -21.3072 -16.8012 -21.413   19.6036\n",
      "    1.4624  10.1015   4.6027   2.511 ]])\n"
     ]
    }
   ],
   "source": [
    "print(h_init)\n",
    "print(h_init.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that if I increase `h_init[0]` from `2.1617` to `2.2617`, `s` will change to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.56427"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "70.9797 + 0.1 * 5.8457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_node_np(inputs: np.ndarray, \n",
    "              hiddens: np.ndarray, \n",
    "              cells: np.ndarray, \n",
    "              params: Dict[str, Tensor]):\n",
    "\n",
    "    assert inputs.shape[0] == hiddens.shape[0] == cells.shape[0]\n",
    "\n",
    "    def _sigmoid(arr: np.ndarray) -> np.ndarray:\n",
    "        return 1.0 / (1.0 + np.exp(-arr))\n",
    "        \n",
    "    Z = np.concatenate([inputs, hiddens], axis=1)\n",
    "\n",
    "    forget = _sigmoid(Z @ params['Wf'].data + params['Bf'].data)\n",
    "\n",
    "    ingate = _sigmoid(Z @ params['Wi'].data + params['Bi'].data)\n",
    "\n",
    "    outgate = _sigmoid(Z @ params['Wo'].data + params['Bo'].data)\n",
    "\n",
    "    change = np.tanh(Z @ params['Wc'].data + params['Bc'].data)\n",
    "\n",
    "    cells = cells * forget + ingate * change\n",
    "\n",
    "    hiddens = outgate * np.tanh(cells)\n",
    "\n",
    "    outputs = hiddens @ params['Wv'].data + params['Bv'].data\n",
    "\n",
    "    return outputs, hiddens, cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.1617 -0.4313  0.3818  0.7105 -0.3538  1.4838  0.0027  0.1719 -0.0683\n",
      "   0.6959  1.4791  0.5237  1.0117  0.0821 -0.2638 -1.4929 -0.5692 -0.3029\n",
      "  -0.0847 -1.7367]]\n",
      "[[ 2.2617 -0.4313  0.3818  0.7105 -0.3538  1.4838  0.0027  0.1719 -0.0683\n",
      "   0.6959  1.4791  0.5237  1.0117  0.0821 -0.2638 -1.4929 -0.5692 -0.3029\n",
      "  -0.0847 -1.7367]]\n"
     ]
    }
   ],
   "source": [
    "h_init_np = h_init.data\n",
    "print(h_init_np)\n",
    "h_init_np[0][0] += 0.1\n",
    "print(h_init_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 20)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens = np.repeat(h_init_np, batch_size, axis=0)\n",
    "hiddens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_np, h_np, c_np = lstm_node_np(inputs.data, \n",
    "                                  hiddens,\n",
    "                                  cells.data,\n",
    "                                  params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 30)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.57204981645923"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_np.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will pass weights through `lstm_node` function twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_node_test_weights(inputs: Tensor, \n",
    "              hiddens: Tensor, \n",
    "              cells: Tensor, \n",
    "              params: Dict[str, Parameter],\n",
    "              params_Bc: Tensor):\n",
    "\n",
    "    assert inputs.shape[0] == hiddens.shape[0] == cells.shape[0]\n",
    "\n",
    "    Z = inputs.concat(hiddens)\n",
    "\n",
    "    forget = sigmoid(Z @ params['Wf'] + params['Bf'])\n",
    "\n",
    "    ingate = sigmoid(Z @ params['Wi'] + params['Bi'])\n",
    "\n",
    "    outgate = sigmoid(Z @ params['Wo'] + params['Bo'])\n",
    "\n",
    "    change = tanh(Z @ params['Wc'] + params_Bc)\n",
    "\n",
    "    cells = cells * forget + ingate * change\n",
    "\n",
    "    hiddens = outgate * tanh(cells)\n",
    "\n",
    "    outputs = hiddens @ params['Wv'] + params['Bv']\n",
    "\n",
    "    return outputs, hiddens, cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(112518)\n",
    "\n",
    "# # initial data\n",
    "# h_init = Tensor(np.random.randn(1, state_size))\n",
    "# c_init = Tensor(np.random.randn(1, state_size))\n",
    "# inputs = Tensor(np.random.randn(batch_size, vocab_size))\n",
    "\n",
    "# # initialize params\n",
    "# params = init_params(batch_size, state_size, vocab_size)\n",
    "\n",
    "# # repeat cell and hidden\n",
    "# hiddens = h_init.repeat(batch_size)\n",
    "# cells = c_init.repeat(batch_size)\n",
    "\n",
    "# # params to test changing\n",
    "# params_Bc = params['Bc'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(112518)\n",
    "\n",
    "# initial data\n",
    "h_init = Tensor(np.random.randn(1, state_size))\n",
    "c_init = Tensor(np.random.randn(1, state_size))\n",
    "inputs = Tensor(np.random.randn(batch_size, vocab_size))\n",
    "\n",
    "# initialize params\n",
    "params = init_params(batch_size, state_size, vocab_size)\n",
    "\n",
    "# repeat cell and hidden\n",
    "hiddens = h_init.repeat(batch_size)\n",
    "cells = c_init.repeat(batch_size)\n",
    "\n",
    "# params to test changing\n",
    "params_Bc = Parameter(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two iterations of lstm_node\n",
    "outputs_1, hiddens_1, cells_1 = lstm_node_test_weights(inputs, \n",
    "                                                       hiddens, \n",
    "                                                       cells, \n",
    "                                                       params, params_Bc)\n",
    "\n",
    "outputs_2, hiddens_2, cells_2 = lstm_node_test_weights(outputs_1, \n",
    "                                                       hiddens_1, \n",
    "                                                       cells_1, \n",
    "                                                       params, \n",
    "                                                       params_Bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "s = outputs_2.sum()\n",
    "s.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(181.6665)\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([-0.3151 -0.6871  1.6564 -2.2223  0.4094  0.7848 -0.0607  0.4536  0.1576\n",
      "  0.5422 -1.9662 -0.5593  0.6145  0.3875  0.8967 -0.0577 -0.3317  1.0005\n",
      " -1.8615  1.6397])\n",
      "Tensor([ -0.6133  -2.2566   0.0367  -4.0249  -0.1883  -1.6268   0.0197   0.2878\n",
      " -16.2883   0.2083  -7.0469  -0.02    -0.2627 -11.7528  -0.0853  -0.3949\n",
      "  -0.0702   0.2086   0.0351   5.2966])\n"
     ]
    }
   ],
   "source": [
    "print(params_Bc)\n",
    "print(params_Bc.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that increasing `params_Bc[0]` from `-0.3151` to `-0.2151` will decrease `s` to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181.60517000000002"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "181.6665 + 0.1 * -0.6133"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing initial weight change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_node_test_weights_np(inputs: np.ndarray, \n",
    "              hiddens: np.ndarray, \n",
    "              cells: np.ndarray,\n",
    "              params: Dict[str, Tensor],\n",
    "                           params_Bc: np.ndarray) -> Tuple[Tensor]:\n",
    "\n",
    "    assert inputs.shape[0] == hiddens.shape[0] == cells.shape[0]\n",
    "\n",
    "    def _sigmoid(arr: np.ndarray) -> np.ndarray:\n",
    "        return 1.0 / (1.0 + np.exp(-arr))\n",
    "        \n",
    "    Z = np.concatenate([inputs, hiddens], axis=1)\n",
    "\n",
    "    forget = _sigmoid(Z @ params['Wf'].data + params['Bf'].data)\n",
    "\n",
    "    ingate = _sigmoid(Z @ params['Wi'].data + params['Bi'].data)\n",
    "\n",
    "    outgate = _sigmoid(Z @ params['Wo'].data + params['Bo'].data)\n",
    "\n",
    "    change = np.tanh(Z @ params['Wc'].data + params_Bc)\n",
    "\n",
    "    cells = cells * forget + ingate * change\n",
    "\n",
    "    hiddens = outgate * np.tanh(cells)\n",
    "\n",
    "    outputs = hiddens @ params['Wv'].data + params['Bv'].data\n",
    "\n",
    "    return outputs, hiddens, cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_Bc_2 = params_Bc.data.copy()\n",
    "params_Bc_2[0] += 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two iterations of lstm_node\n",
    "outputs_1, hiddens_1, cells_1 = lstm_node_test_weights_np(inputs.data, \n",
    "                                                       hiddens.data, \n",
    "                                                       cells.data, \n",
    "                                                       params, params_Bc.data)\n",
    "\n",
    "outputs_2, hiddens_2, cells_2 = lstm_node_test_weights_np(outputs_1, \n",
    "                                                       hiddens_1, \n",
    "                                                       cells_1, \n",
    "                                                       params, \n",
    "                                                       params_Bc.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181.66652035171722"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181.60007968835646"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two iterations of lstm_node\n",
    "outputs_1, hiddens_1, cells_1 = lstm_node_test_weights_np(inputs.data, \n",
    "                                                       hiddens.data, \n",
    "                                                       cells.data, \n",
    "                                                       params, params_Bc_2)\n",
    "\n",
    "outputs_2, hiddens_2, cells_2 = lstm_node_test_weights_np(outputs_1, \n",
    "                                                       hiddens_1, \n",
    "                                                       cells_1, \n",
    "                                                       params, \n",
    "                                                       params_Bc_2)\n",
    "\n",
    "outputs_2.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Passing weights twice worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Input -> 3D Output\n",
    "\n",
    "Will need to build output up incrementally (as in example from autograd package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_node(inputs: Tensor, \n",
    "              hiddens: Tensor, \n",
    "              cells: Tensor, \n",
    "              params: Dict[str, Parameter]):\n",
    "\n",
    "    assert inputs.shape[0] == hiddens.shape[0] == cells.shape[0]\n",
    "\n",
    "    Z = inputs.concat(hiddens)\n",
    "    \n",
    "#     import pdb; pdb.set_trace()\n",
    "    forget = sigmoid(Z @ params['Wf'] + params['Bf'])\n",
    "\n",
    "    ingate = sigmoid(Z @ params['Wi'] + params['Bi'])\n",
    "\n",
    "    outgate = sigmoid(Z @ params['Wo'] + params['Bo'])\n",
    "\n",
    "    change = tanh(Z @ params['Wc'] + params['Bc'])\n",
    "\n",
    "    cells = cells * forget + ingate * change\n",
    "\n",
    "    hiddens = outgate * tanh(cells)\n",
    "\n",
    "    outputs = hiddens @ params['Wv'] + params['Bv']\n",
    "\n",
    "    return outputs, hiddens, cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(112518)\n",
    "\n",
    "sequence_length = 1\n",
    "\n",
    "# initial data\n",
    "h_init = Tensor(np.random.randn(1, state_size))\n",
    "c_init = Tensor(np.random.randn(1, state_size))\n",
    "\n",
    "# 3d input\n",
    "inputs = Tensor(np.random.randn(batch_size, sequence_length, vocab_size))\n",
    "\n",
    "# initialize params\n",
    "params = init_params(batch_size, state_size, vocab_size)\n",
    "\n",
    "# repeat cell and hidden\n",
    "hiddens = h_init.repeat(batch_size)\n",
    "cells = c_init.repeat(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 30)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1, 30)\n"
     ]
    }
   ],
   "source": [
    "for i in range(inputs.shape[1]):\n",
    "    if i == 0:\n",
    "        outputs_single, hiddens, cells = lstm_node(inputs.select_index_axis_1(i), \n",
    "                                                   hiddens, cells, params)\n",
    "        outputs = outputs_single.expand_dims_axis_1()\n",
    "        print(outputs.shape)\n",
    "    else:\n",
    "        output_single, hiddens, cells = lstm_node(inputs.select_index_axis_1(i), \n",
    "                                                  hiddens, cells, params)\n",
    "        output = output_single.expand_dims_axis_1()\n",
    "        outputs = outputs.append_axis_1(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape\n",
    "s = outputs.sum()\n",
    "s.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(70.9797)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1, 30)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(-0.1656)\n",
      "Tensor(-1.4009)\n"
     ]
    }
   ],
   "source": [
    "print(inputs[1,0,1])\n",
    "print(inputs.grad[1,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that if we increase `inputs[1][0][1]` by 0.1, `s` will increase to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.83961"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "70.9797 + -1.4009 * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_node_test_input(inputs: np.ndarray, \n",
    "              hiddens: Tensor, \n",
    "              cells: Tensor, \n",
    "              params: Dict[str, Parameter]):\n",
    "\n",
    "    Z = Tensor(np.concatenate([inputs, hiddens.data], axis=1))\n",
    "    \n",
    "    forget = sigmoid(Z @ params['Wf'] + params['Bf'])\n",
    "\n",
    "    ingate = sigmoid(Z @ params['Wi'] + params['Bi'])\n",
    "\n",
    "    outgate = sigmoid(Z @ params['Wo'] + params['Bo'])\n",
    "\n",
    "    change = tanh(Z @ params['Wc'] + params['Bc'])\n",
    "\n",
    "    cells = cells * forget + ingate * change\n",
    "\n",
    "    hiddens = outgate * tanh(cells)\n",
    "\n",
    "    outputs = hiddens @ params['Wv'] + params['Bv']\n",
    "\n",
    "    return outputs, hiddens, cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_input_val(input_val):\n",
    "    np.random.seed(112518)\n",
    "\n",
    "    sequence_length = 1\n",
    "\n",
    "    # initial data\n",
    "    h_init = Tensor(np.random.randn(1, state_size))\n",
    "    c_init = Tensor(np.random.randn(1, state_size))\n",
    "\n",
    "    # 3d input\n",
    "    inputs = Tensor(np.random.randn(batch_size, sequence_length, vocab_size))\n",
    "\n",
    "    # initialize params\n",
    "    params = init_params(batch_size, state_size, vocab_size)\n",
    "\n",
    "    # repeat cell and hidden\n",
    "    hiddens = h_init.repeat(batch_size)\n",
    "    cells = c_init.repeat(batch_size)\n",
    "    \n",
    "    inputs_data = inputs.data\n",
    "    inputs_data_2 = inputs_data.copy()\n",
    "    inputs_data_2[1][0][1] = input_val\n",
    "    \n",
    "    for i in range(inputs_data.shape[1]):\n",
    "        if i == 0:\n",
    "            outputs_single, hiddens, cells = lstm_node_test_input(inputs_data_2[:, i, :], \n",
    "                                                       hiddens, cells, params)\n",
    "            outputs = outputs_single.expand_dims_axis_1()\n",
    "        else:\n",
    "            output_single, hiddens, cells = lstm_node_test_input(inputs_data_2[:, i, :], \n",
    "                                                      hiddens, cells, params)\n",
    "            output = output_single.expand_dims_axis_1()\n",
    "            outputs = outputs.append_axis_1(output)\n",
    "        \n",
    "    return outputs.sum().data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71.33558110395718,\n",
       " 71.17716269928641,\n",
       " 71.02855643442268,\n",
       " 70.89009869126181,\n",
       " 70.76228974043892]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sum_input_val(x) for x in np.arange(-0.4, 0.1, 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3845774316087045"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(70.89009869126181 - 71.02855643442268) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grad is correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence length 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(112518)\n",
    "\n",
    "sequence_length = 6 \n",
    "\n",
    "# initial data\n",
    "h_init = Tensor(np.random.randn(1, state_size))\n",
    "c_init = Tensor(np.random.randn(1, state_size))\n",
    "\n",
    "# 3d input\n",
    "inputs = Tensor(np.random.randn(batch_size, sequence_length, vocab_size), no_grad=True)\n",
    "\n",
    "# initialize params\n",
    "params = init_params(batch_size, state_size, vocab_size)\n",
    "\n",
    "# repeat cell and hidden\n",
    "hiddens = h_init.repeat(batch_size)\n",
    "cells = c_init.repeat(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(inputs.shape[1]):\n",
    "    if i == 0:\n",
    "        outputs_single, hiddens, cells = lstm_node(inputs.select_index_axis_1(i), \n",
    "                                                   hiddens, cells, params)\n",
    "        outputs = outputs_single.expand_dims_axis_1()\n",
    "    else:\n",
    "        output_single, hiddens, cells = lstm_node(inputs.select_index_axis_1(i), \n",
    "                                                  hiddens, cells, params)\n",
    "        output = output_single.expand_dims_axis_1()\n",
    "        outputs = outputs.append_axis_1(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(405.8718)\n"
     ]
    }
   ],
   "source": [
    "s = outputs.sum()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.9740800857543945}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "s.backward()\n",
    "print({time.time() - start})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test time vs. sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_vs_sequence_length(seq_len):\n",
    "    \n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(112518)\n",
    "\n",
    "sequence_length = 5\n",
    "\n",
    "# initial data\n",
    "h_init = Tensor(np.random.randn(1, state_size))\n",
    "c_init = Tensor(np.random.randn(1, state_size))\n",
    "\n",
    "# 3d input\n",
    "inputs = Tensor(np.random.randn(batch_size, sequence_length, vocab_size))\n",
    "\n",
    "# initialize params\n",
    "params = init_params(batch_size, state_size, vocab_size)\n",
    "\n",
    "# repeat cell and hidden\n",
    "hiddens = h_init.repeat(batch_size)\n",
    "cells = c_init.repeat(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_input_val(input_val, sequence_length):\n",
    "    np.random.seed(112518)\n",
    "\n",
    "    # initial data\n",
    "    h_init = Tensor(np.random.randn(1, state_size))\n",
    "    c_init = Tensor(np.random.randn(1, state_size))\n",
    "\n",
    "    # 3d input\n",
    "    inputs = Tensor(np.random.randn(batch_size, sequence_length, vocab_size))\n",
    "\n",
    "    # initialize params\n",
    "    params = init_params(batch_size, state_size, vocab_size)\n",
    "\n",
    "    # repeat cell and hidden\n",
    "    hiddens = h_init.repeat(batch_size)\n",
    "    cells = c_init.repeat(batch_size)\n",
    "    \n",
    "    inputs_data = inputs.data\n",
    "    inputs_data_2 = inputs_data.copy()\n",
    "    inputs_data_2[1][sequence_length-1][1] = input_val\n",
    "    \n",
    "    for i in range(inputs_data.shape[1]):\n",
    "        if i == 0:\n",
    "            outputs_single, hiddens, cells = lstm_node_test_input(inputs_data_2[:, i, :], \n",
    "                                                       hiddens, cells, params)\n",
    "            outputs = outputs_single.expand_dims_axis_1()\n",
    "        else:\n",
    "            output_single, hiddens, cells = lstm_node_test_input(inputs_data_2[:, i, :], \n",
    "                                                      hiddens, cells, params)\n",
    "            output = output_single.expand_dims_axis_1()\n",
    "            outputs = outputs.append_axis_1(output)\n",
    "    \n",
    "    s = outputs.sum()    \n",
    "    \n",
    "    s.backward()\n",
    "    \n",
    "    return s.data.item(), inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11.052709279981144,\n",
       "  Tensor([[[-4.9570e-01 -1.2251e+00  9.8840e-01  2.7400e-02  8.6630e-01\n",
       "     -5.2580e-01  1.0630e-01  2.7198e+00 -1.1489e+00 -3.6740e-01\n",
       "      4.4880e-01 -7.3370e-01 -2.0529e+00 -2.0540e-01 -1.1125e+00\n",
       "      6.1160e-01  6.1490e-01  1.3727e+00 -8.8930e-01  4.7830e-01\n",
       "     -1.0350e+00 -8.4850e-01 -2.5330e-01  7.3520e-01  1.5818e+00\n",
       "     -1.0196e+00  1.3350e-01 -7.7300e-02  1.1130e+00  1.5340e-01]\n",
       "    [ 1.2619e+00 -1.6560e-01  9.8200e-02  1.4630e+00 -3.2620e-01\n",
       "     -2.6550e-01 -1.1930e-01 -1.0223e+00 -5.6000e-03  1.0546e+00\n",
       "      9.7970e-01 -9.3050e-01 -5.0980e-01  1.0708e+00  2.3520e-01\n",
       "     -8.4260e-01  4.9080e-01  8.9230e-01 -2.8100e-02 -2.3800e-02\n",
       "     -5.5800e-02  7.1210e-01 -2.7930e-01 -1.0040e-01 -5.5520e-01\n",
       "      6.8150e-01  1.4822e+00 -4.3980e-01  2.0110e-01 -4.3610e-01]]\n",
       "  \n",
       "   [[ 2.6810e-01  3.8080e-01 -1.2532e+00  1.2541e+00 -2.3490e-01\n",
       "     -8.6770e-01  7.5060e-01 -1.9595e+00  8.5710e-01 -7.4740e-01\n",
       "      1.5610e-01  5.8700e-02  6.5660e-01 -6.8740e-01 -4.1030e-01\n",
       "      1.0423e+00  1.9370e+00  1.7559e+00  1.8491e+00 -1.0288e+00\n",
       "     -7.3380e-01  1.0210e-01  1.8350e-01 -2.4600e-02  1.6540e-01\n",
       "      1.2254e+00 -1.5680e+00 -2.8910e-01  5.5310e-01 -1.1989e+00]\n",
       "    [ 1.9326e+00  1.6654e+00  1.7300e-02  1.1582e+00 -9.4560e-01\n",
       "     -8.4600e-01 -1.0812e+00  1.1281e+00  5.7660e-01 -9.4780e-01\n",
       "      1.0602e+00 -8.2820e-01  1.0715e+00 -5.0930e-01  2.9840e-01\n",
       "     -1.9980e-01  1.4456e+00  1.0330e-01  1.1895e+00 -1.5410e+00\n",
       "     -1.2109e+00 -9.4320e-01  7.5420e-01  2.1950e-01  8.5650e-01\n",
       "     -2.7700e-02 -1.1373e+00  1.2440e-01 -1.1789e+00  2.8720e-01]]\n",
       "  \n",
       "   [[ 1.7030e-01 -4.1460e-01  4.9790e-01 -1.1103e+00  1.5740e-01\n",
       "      7.6700e-02  1.2117e+00 -9.4970e-01 -3.7430e-01  8.1360e-01\n",
       "      7.2800e-02 -1.3942e+00 -5.7000e-01  8.6830e-01 -1.6350e-01\n",
       "      8.3550e-01  8.8470e-01  7.5580e-01  3.4080e-01 -7.0170e-01\n",
       "      4.3930e-01 -1.4942e+00 -1.0374e+00 -7.0810e-01 -1.0585e+00\n",
       "     -6.4710e-01 -8.7160e-01  7.6490e-01 -1.5280e+00 -4.5210e-01]\n",
       "    [ 5.1630e-01 -1.0800e+00 -1.0421e+00  1.7998e+00  5.1490e-01\n",
       "     -1.3175e+00 -6.6620e-01 -1.2347e+00  1.0819e+00 -1.5629e+00\n",
       "      8.4280e-01  4.9470e-01  1.4201e+00  1.4188e+00 -1.9790e-01\n",
       "      9.0100e-01  5.4310e-01  1.8221e+00 -3.7740e-01  4.3880e-01\n",
       "     -4.2360e-01 -1.6484e+00  4.9210e-01 -6.8000e-03 -1.9000e+00\n",
       "     -1.8784e+00  3.8540e-01  9.4630e-01  1.0188e+00  8.2490e-01]]\n",
       "  \n",
       "   [[-3.1390e-01 -2.1900e-02  1.9500e-02  1.8810e-01  7.3350e-01\n",
       "     -1.2990e-01 -4.2150e-01  4.1030e-01  1.6973e+00  8.8630e-01\n",
       "     -1.5414e+00  3.4820e-01 -9.1480e-01  1.6800e-01  8.1900e-02\n",
       "      7.9600e-01 -2.4600e-02  1.9740e-01 -8.2870e-01  8.4810e-01\n",
       "      5.4110e-01  5.9080e-01 -1.5415e+00 -7.0060e-01 -5.6500e-01\n",
       "     -5.4260e-01 -6.4740e-01 -1.2311e+00 -5.7280e-01 -3.6600e-02]\n",
       "    [ 4.3620e-01 -2.3300e-02  1.1268e+00 -4.0070e-01 -5.2640e-01\n",
       "     -1.1444e+00  1.5110e-01 -7.6220e-01 -9.3740e-01  4.4870e-01\n",
       "      1.1643e+00 -4.4700e-01 -3.8620e-01  2.7840e-01  9.8010e-01\n",
       "      2.5000e-03  7.6700e-01  3.3110e-01 -1.4410e-01  1.9780e-01\n",
       "     -1.1335e+00  1.0260e+00  1.0908e+00 -1.2373e+00  2.7030e-01\n",
       "      3.0310e-01 -9.2100e-02 -1.2510e+00 -1.4881e+00 -7.0000e-02]]\n",
       "  \n",
       "   [[-2.1346e+00 -9.5190e-01 -1.1240e-01 -5.7710e-01  1.6034e+00\n",
       "      1.2005e+00 -7.7820e-01 -1.1885e+00 -6.8680e-01 -4.4390e-01\n",
       "      1.2584e+00  1.8700e-01 -1.7278e+00 -1.0174e+00  3.6650e-01\n",
       "      1.4224e+00 -8.4420e-01  2.1801e+00  5.5160e-01 -2.0152e+00\n",
       "      1.8423e+00 -1.2078e+00  1.5774e+00 -5.4400e-02 -2.2350e+00\n",
       "      4.4780e-01  5.0010e-01  5.1500e-02 -1.2297e+00  2.1460e-01]\n",
       "    [ 1.2736e+00  7.8890e-01 -1.5609e+00 -2.6960e-01  8.4570e-01\n",
       "      3.9450e-01 -2.4758e+00 -8.2360e-01 -1.2769e+00  1.0303e+00\n",
       "     -1.0883e+00  2.2187e+00 -9.0220e-01  5.6360e-01 -1.2649e+00\n",
       "      2.0491e+00 -7.6180e-01  5.7860e-01 -1.2315e+00 -1.1017e+00\n",
       "      1.9041e+00  1.1334e+00  6.6350e-01  4.4800e-02  1.6730e-01\n",
       "      1.4942e+00 -2.3280e+00  4.6880e-01  3.0800e-01 -1.7250e-01]]\n",
       "  \n",
       "   [[ 3.7230e-01 -9.1190e-01  1.0292e+00 -1.1220e-01  1.3897e+00\n",
       "      9.8040e-01 -1.3030e-01  2.4561e+00 -7.7200e-02  2.1954e+00\n",
       "     -1.2989e+00 -9.1450e-01 -1.1100e-01  1.2630e-01  1.0310e-01\n",
       "     -1.8608e+00  1.1753e+00 -6.6580e-01  1.1280e+00 -7.8600e-02\n",
       "      1.2628e+00  4.9300e-01 -5.0850e-01  1.1710e-01 -1.4025e+00\n",
       "     -1.1242e+00 -1.2399e+00  8.8770e-01  2.5040e-01 -2.2040e-01]\n",
       "    [ 6.3550e-01 -7.0000e-03  1.5405e+00 -6.3900e-02 -8.8490e-01\n",
       "      4.6300e-02  1.9591e+00 -5.9600e-01 -2.5408e+00  1.0300e+00\n",
       "      1.9236e+00  8.2000e-01 -6.3950e-01 -4.5630e-01 -1.6073e+00\n",
       "     -5.0130e-01  3.7530e-01  6.5210e-01 -5.2000e-03 -8.2840e-01\n",
       "     -8.5310e-01  5.7520e-01 -2.2440e-01  1.3243e+00 -9.8600e-02\n",
       "      1.8190e-01 -5.6900e-02 -3.5270e-01 -1.8270e-01  8.8670e-01]]\n",
       "  \n",
       "   [[-1.8614e+00 -4.2240e-01 -2.5710e-01  7.4590e-01  2.8820e-01\n",
       "      9.4020e-01  1.5883e+00  8.8070e-01  5.1080e-01 -7.3430e-01\n",
       "     -9.2820e-01 -8.8280e-01  6.2340e-01  1.6467e+00  3.2100e-01\n",
       "      3.7370e-01  2.9090e-01 -1.1201e+00 -2.2000e-01 -2.5020e-01\n",
       "     -2.3610e-01 -4.8060e-01  9.7620e-01 -1.5446e+00  5.3190e-01\n",
       "      3.8500e-02  3.5510e-01 -8.5350e-01 -9.6800e-02  9.4940e-01]\n",
       "    [ 4.3980e-01 -2.0366e+00  1.0579e+00  1.0765e+00 -5.5700e-01\n",
       "     -2.2718e+00 -4.2390e-01 -4.0460e-01 -2.4680e-01  9.8040e-01\n",
       "      5.2780e-01 -1.3402e+00  4.6490e-01  2.9580e-01 -2.7920e-01\n",
       "     -6.5640e-01 -1.0630e-01 -1.7900e-02  8.8790e-01  3.9900e-01\n",
       "      1.1248e+00 -6.2770e-01 -9.7830e-01  3.0672e+00  4.5000e-03\n",
       "     -1.6990e-01 -2.7460e-01  8.5550e-01 -2.9000e-01 -1.9140e-01]]\n",
       "  \n",
       "   [[ 2.0890e-01  1.1704e+00  1.4531e+00 -8.6200e-02 -1.4739e+00\n",
       "      1.0685e+00  1.0260e-01 -9.7400e-02 -1.3200e-02  1.1549e+00\n",
       "      2.3170e-01  3.3160e-01 -2.1640e-01 -9.4390e-01 -4.2570e-01\n",
       "      2.0142e+00 -2.0090e-01  2.8790e-01  5.7080e-01  3.4930e-01\n",
       "      3.2910e-01 -7.0810e-01  9.7410e-01  2.6920e-01 -1.0490e+00\n",
       "     -4.0650e-01 -9.0360e-01 -6.7700e-02  4.5550e-01 -4.8590e-01]\n",
       "    [-8.4890e-01  2.2286e+00  3.3770e-01 -3.5640e-01 -1.2340e-01\n",
       "     -1.1789e+00  2.1612e+00 -2.8810e-01 -9.2300e-02 -5.4100e-01\n",
       "      1.4058e+00 -7.0480e-01  2.7693e+00  1.5950e-01  9.6650e-01\n",
       "     -2.1623e+00  2.0180e-01 -7.5820e-01  1.1641e+00 -1.4971e+00\n",
       "      1.8786e+00 -4.9280e-01  1.7970e-01 -5.9300e-01  4.6460e-01\n",
       "     -3.2120e-01  2.4330e-01  3.3280e-01 -3.7070e-01 -1.6568e+00]]\n",
       "  \n",
       "   [[ 2.2310e-01 -2.1500e-02 -3.1390e-01 -4.6570e-01  6.7670e-01\n",
       "      2.2544e+00 -1.5913e+00  5.1100e-01 -7.5530e-01 -1.1589e+00\n",
       "      3.8050e-01 -3.0076e+00 -1.4753e+00  2.3500e-02  1.2733e+00\n",
       "      1.0164e+00 -4.6660e-01 -9.4600e-02  3.5830e-01  6.7620e-01\n",
       "     -2.6020e-01  1.6223e+00 -4.4850e-01  1.0237e+00  2.2250e-01\n",
       "     -1.8925e+00 -1.1991e+00  5.5300e-02 -1.4080e-01  7.1030e-01]\n",
       "    [ 1.6651e+00 -5.2900e-02 -8.9130e-01 -3.3320e-01  2.3650e-01\n",
       "      1.9940e-01  2.0270e-01  6.9910e-01 -1.6030e+00  6.4580e-01\n",
       "      2.3790e-01  5.9490e-01  6.8780e-01  9.9430e-01 -4.4870e-01\n",
       "      8.6870e-01 -1.0978e+00  2.1410e-01 -4.0020e-01  1.6441e+00\n",
       "      2.5780e+00  6.0790e-01 -1.0320e-01 -3.8960e-01 -1.1309e+00\n",
       "      9.5950e-01  1.4122e+00 -3.9450e-01 -7.0800e-02  7.1750e-01]]\n",
       "  \n",
       "   [[-6.5600e-01 -6.0230e-01 -1.7000e-02 -2.5280e-01 -4.0480e-01\n",
       "     -6.2650e-01  6.1840e-01  1.1778e+00 -9.2380e-01  1.1700e-02\n",
       "      1.6457e+00 -8.5360e-01  2.8720e-01  1.4870e-01 -3.8610e-01\n",
       "     -1.7279e+00 -7.9320e-01 -1.2116e+00 -9.0800e-01 -5.8950e-01\n",
       "     -9.0180e-01 -1.1482e+00 -1.1410e-01  8.5520e-01  8.9760e-01\n",
       "      1.0365e+00  6.0320e-01  1.7217e+00  2.6550e-01  5.7800e-01]\n",
       "    [-1.6390e-01  4.1930e-01 -9.3040e-01 -1.0540e-01 -1.0198e+00\n",
       "     -4.2310e-01 -5.6760e-01  1.4290e-01 -1.4513e+00 -9.6610e-01\n",
       "      5.8300e-02  1.5094e+00 -6.9060e-01 -4.4200e-02 -1.1366e+00\n",
       "     -4.3220e-01 -1.5480e-01  4.0330e-01  9.0920e-01 -1.1269e+00\n",
       "     -4.2060e-01 -7.4400e-01 -5.1620e-01  1.4570e+00 -1.6788e+00\n",
       "     -1.5250e-01 -7.5530e-01  3.7360e-01 -8.4400e-02 -8.2580e-01]]\n",
       "  \n",
       "   [[ 1.9070e+00 -4.3420e-01 -2.1665e+00  2.5982e+00  4.4760e-01\n",
       "      4.0730e-01  5.5060e-01 -4.8980e-01  5.9060e-01 -3.6480e-01\n",
       "     -1.4034e+00 -2.5380e-01  1.8552e+00 -7.9740e-01  3.6110e-01\n",
       "     -4.0600e-01 -7.3970e-01 -1.0464e+00  5.4800e-02 -6.6270e-01\n",
       "     -2.2696e+00 -9.0830e-01 -1.1317e+00 -3.4855e+00 -1.5822e+00\n",
       "      6.0000e-01 -1.7445e+00  7.6000e-03 -1.2245e+00  1.0287e+00]\n",
       "    [-1.2370e-01 -1.4980e-01 -1.3146e+00  8.8070e-01  5.6530e-01\n",
       "     -9.7100e-02 -2.6850e-01 -2.2650e-01  7.2420e-01  2.5000e-03\n",
       "      1.5025e+00 -4.4910e-01 -2.4000e-03  7.4280e-01  4.7950e-01\n",
       "     -7.3200e-02 -1.6539e+00 -1.6140e-01  2.1580e-01  9.6570e-01\n",
       "      1.4980e-01 -1.3661e+00  4.3530e-01  1.5165e+00  8.8100e-02\n",
       "      2.4890e-01  4.9330e-01 -2.3476e+00  6.6350e-01 -2.6860e-01]]\n",
       "  \n",
       "   [[ 7.6600e-01  1.2944e+00 -2.4330e-01  2.5610e-01  1.8751e+00\n",
       "      6.4280e-01  1.9780e-01 -7.4290e-01  1.1549e+00  4.3310e-01\n",
       "     -2.1390e-01 -2.4660e-01  3.5460e-01  1.1660e-01 -1.8678e+00\n",
       "     -1.4323e+00  6.1550e-01  1.8100e-01  2.2700e-02 -1.5867e+00\n",
       "     -1.3959e+00 -1.6073e+00  3.8050e-01 -5.0200e-02 -5.2390e-01\n",
       "      1.0628e+00 -6.0500e-01  5.5020e-01 -2.8710e-01 -1.0069e+00]\n",
       "    [ 6.6400e-02 -4.8380e-01 -1.8870e-01  1.6417e+00 -1.6320e-01\n",
       "      1.2198e+00  6.9200e-02  7.2090e-01 -7.9440e-01  2.5397e+00\n",
       "      1.3034e+00 -4.3350e-01  8.6220e-01  1.1659e+00  1.8459e+00\n",
       "     -3.6100e-01 -1.0925e+00 -4.0660e-01  2.0613e+00  4.7580e-01\n",
       "     -1.1295e+00 -5.6260e-01  1.2751e+00  1.5768e+00 -1.8678e+00\n",
       "      3.3400e-02 -9.1780e-01 -9.8350e-01 -4.3880e-01 -4.1000e-02]]])),\n",
       " (11.206741802427725,\n",
       "  Tensor([[[-4.9570e-01 -1.2251e+00  9.8840e-01  2.7400e-02  8.6630e-01\n",
       "     -5.2580e-01  1.0630e-01  2.7198e+00 -1.1489e+00 -3.6740e-01\n",
       "      4.4880e-01 -7.3370e-01 -2.0529e+00 -2.0540e-01 -1.1125e+00\n",
       "      6.1160e-01  6.1490e-01  1.3727e+00 -8.8930e-01  4.7830e-01\n",
       "     -1.0350e+00 -8.4850e-01 -2.5330e-01  7.3520e-01  1.5818e+00\n",
       "     -1.0196e+00  1.3350e-01 -7.7300e-02  1.1130e+00  1.5340e-01]\n",
       "    [ 1.2619e+00 -1.6560e-01  9.8200e-02  1.4630e+00 -3.2620e-01\n",
       "     -2.6550e-01 -1.1930e-01 -1.0223e+00 -5.6000e-03  1.0546e+00\n",
       "      9.7970e-01 -9.3050e-01 -5.0980e-01  1.0708e+00  2.3520e-01\n",
       "     -8.4260e-01  4.9080e-01  8.9230e-01 -2.8100e-02 -2.3800e-02\n",
       "     -5.5800e-02  7.1210e-01 -2.7930e-01 -1.0040e-01 -5.5520e-01\n",
       "      6.8150e-01  1.4822e+00 -4.3980e-01  2.0110e-01 -4.3610e-01]]\n",
       "  \n",
       "   [[ 2.6810e-01  3.8080e-01 -1.2532e+00  1.2541e+00 -2.3490e-01\n",
       "     -8.6770e-01  7.5060e-01 -1.9595e+00  8.5710e-01 -7.4740e-01\n",
       "      1.5610e-01  5.8700e-02  6.5660e-01 -6.8740e-01 -4.1030e-01\n",
       "      1.0423e+00  1.9370e+00  1.7559e+00  1.8491e+00 -1.0288e+00\n",
       "     -7.3380e-01  1.0210e-01  1.8350e-01 -2.4600e-02  1.6540e-01\n",
       "      1.2254e+00 -1.5680e+00 -2.8910e-01  5.5310e-01 -1.1989e+00]\n",
       "    [ 1.9326e+00  1.6654e+00  1.7300e-02  1.1582e+00 -9.4560e-01\n",
       "     -8.4600e-01 -1.0812e+00  1.1281e+00  5.7660e-01 -9.4780e-01\n",
       "      1.0602e+00 -8.2820e-01  1.0715e+00 -5.0930e-01  2.9840e-01\n",
       "     -1.9980e-01  1.4456e+00  1.0330e-01  1.1895e+00 -1.5410e+00\n",
       "     -1.2109e+00 -9.4320e-01  7.5420e-01  2.1950e-01  8.5650e-01\n",
       "     -2.7700e-02 -1.1373e+00  1.2440e-01 -1.1789e+00  2.8720e-01]]\n",
       "  \n",
       "   [[ 1.7030e-01 -4.1460e-01  4.9790e-01 -1.1103e+00  1.5740e-01\n",
       "      7.6700e-02  1.2117e+00 -9.4970e-01 -3.7430e-01  8.1360e-01\n",
       "      7.2800e-02 -1.3942e+00 -5.7000e-01  8.6830e-01 -1.6350e-01\n",
       "      8.3550e-01  8.8470e-01  7.5580e-01  3.4080e-01 -7.0170e-01\n",
       "      4.3930e-01 -1.4942e+00 -1.0374e+00 -7.0810e-01 -1.0585e+00\n",
       "     -6.4710e-01 -8.7160e-01  7.6490e-01 -1.5280e+00 -4.5210e-01]\n",
       "    [ 5.1630e-01 -1.0800e+00 -1.0421e+00  1.7998e+00  5.1490e-01\n",
       "     -1.3175e+00 -6.6620e-01 -1.2347e+00  1.0819e+00 -1.5629e+00\n",
       "      8.4280e-01  4.9470e-01  1.4201e+00  1.4188e+00 -1.9790e-01\n",
       "      9.0100e-01  5.4310e-01  1.8221e+00 -3.7740e-01  4.3880e-01\n",
       "     -4.2360e-01 -1.6484e+00  4.9210e-01 -6.8000e-03 -1.9000e+00\n",
       "     -1.8784e+00  3.8540e-01  9.4630e-01  1.0188e+00  8.2490e-01]]\n",
       "  \n",
       "   [[-3.1390e-01 -2.1900e-02  1.9500e-02  1.8810e-01  7.3350e-01\n",
       "     -1.2990e-01 -4.2150e-01  4.1030e-01  1.6973e+00  8.8630e-01\n",
       "     -1.5414e+00  3.4820e-01 -9.1480e-01  1.6800e-01  8.1900e-02\n",
       "      7.9600e-01 -2.4600e-02  1.9740e-01 -8.2870e-01  8.4810e-01\n",
       "      5.4110e-01  5.9080e-01 -1.5415e+00 -7.0060e-01 -5.6500e-01\n",
       "     -5.4260e-01 -6.4740e-01 -1.2311e+00 -5.7280e-01 -3.6600e-02]\n",
       "    [ 4.3620e-01 -2.3300e-02  1.1268e+00 -4.0070e-01 -5.2640e-01\n",
       "     -1.1444e+00  1.5110e-01 -7.6220e-01 -9.3740e-01  4.4870e-01\n",
       "      1.1643e+00 -4.4700e-01 -3.8620e-01  2.7840e-01  9.8010e-01\n",
       "      2.5000e-03  7.6700e-01  3.3110e-01 -1.4410e-01  1.9780e-01\n",
       "     -1.1335e+00  1.0260e+00  1.0908e+00 -1.2373e+00  2.7030e-01\n",
       "      3.0310e-01 -9.2100e-02 -1.2510e+00 -1.4881e+00 -7.0000e-02]]\n",
       "  \n",
       "   [[-2.1346e+00 -9.5190e-01 -1.1240e-01 -5.7710e-01  1.6034e+00\n",
       "      1.2005e+00 -7.7820e-01 -1.1885e+00 -6.8680e-01 -4.4390e-01\n",
       "      1.2584e+00  1.8700e-01 -1.7278e+00 -1.0174e+00  3.6650e-01\n",
       "      1.4224e+00 -8.4420e-01  2.1801e+00  5.5160e-01 -2.0152e+00\n",
       "      1.8423e+00 -1.2078e+00  1.5774e+00 -5.4400e-02 -2.2350e+00\n",
       "      4.4780e-01  5.0010e-01  5.1500e-02 -1.2297e+00  2.1460e-01]\n",
       "    [ 1.2736e+00  7.8890e-01 -1.5609e+00 -2.6960e-01  8.4570e-01\n",
       "      3.9450e-01 -2.4758e+00 -8.2360e-01 -1.2769e+00  1.0303e+00\n",
       "     -1.0883e+00  2.2187e+00 -9.0220e-01  5.6360e-01 -1.2649e+00\n",
       "      2.0491e+00 -7.6180e-01  5.7860e-01 -1.2315e+00 -1.1017e+00\n",
       "      1.9041e+00  1.1334e+00  6.6350e-01  4.4800e-02  1.6730e-01\n",
       "      1.4942e+00 -2.3280e+00  4.6880e-01  3.0800e-01 -1.7250e-01]]\n",
       "  \n",
       "   [[ 3.7230e-01 -9.1190e-01  1.0292e+00 -1.1220e-01  1.3897e+00\n",
       "      9.8040e-01 -1.3030e-01  2.4561e+00 -7.7200e-02  2.1954e+00\n",
       "     -1.2989e+00 -9.1450e-01 -1.1100e-01  1.2630e-01  1.0310e-01\n",
       "     -1.8608e+00  1.1753e+00 -6.6580e-01  1.1280e+00 -7.8600e-02\n",
       "      1.2628e+00  4.9300e-01 -5.0850e-01  1.1710e-01 -1.4025e+00\n",
       "     -1.1242e+00 -1.2399e+00  8.8770e-01  2.5040e-01 -2.2040e-01]\n",
       "    [ 6.3550e-01 -7.0000e-03  1.5405e+00 -6.3900e-02 -8.8490e-01\n",
       "      4.6300e-02  1.9591e+00 -5.9600e-01 -2.5408e+00  1.0300e+00\n",
       "      1.9236e+00  8.2000e-01 -6.3950e-01 -4.5630e-01 -1.6073e+00\n",
       "     -5.0130e-01  3.7530e-01  6.5210e-01 -5.2000e-03 -8.2840e-01\n",
       "     -8.5310e-01  5.7520e-01 -2.2440e-01  1.3243e+00 -9.8600e-02\n",
       "      1.8190e-01 -5.6900e-02 -3.5270e-01 -1.8270e-01  8.8670e-01]]\n",
       "  \n",
       "   [[-1.8614e+00 -4.2240e-01 -2.5710e-01  7.4590e-01  2.8820e-01\n",
       "      9.4020e-01  1.5883e+00  8.8070e-01  5.1080e-01 -7.3430e-01\n",
       "     -9.2820e-01 -8.8280e-01  6.2340e-01  1.6467e+00  3.2100e-01\n",
       "      3.7370e-01  2.9090e-01 -1.1201e+00 -2.2000e-01 -2.5020e-01\n",
       "     -2.3610e-01 -4.8060e-01  9.7620e-01 -1.5446e+00  5.3190e-01\n",
       "      3.8500e-02  3.5510e-01 -8.5350e-01 -9.6800e-02  9.4940e-01]\n",
       "    [ 4.3980e-01 -2.0366e+00  1.0579e+00  1.0765e+00 -5.5700e-01\n",
       "     -2.2718e+00 -4.2390e-01 -4.0460e-01 -2.4680e-01  9.8040e-01\n",
       "      5.2780e-01 -1.3402e+00  4.6490e-01  2.9580e-01 -2.7920e-01\n",
       "     -6.5640e-01 -1.0630e-01 -1.7900e-02  8.8790e-01  3.9900e-01\n",
       "      1.1248e+00 -6.2770e-01 -9.7830e-01  3.0672e+00  4.5000e-03\n",
       "     -1.6990e-01 -2.7460e-01  8.5550e-01 -2.9000e-01 -1.9140e-01]]\n",
       "  \n",
       "   [[ 2.0890e-01  1.1704e+00  1.4531e+00 -8.6200e-02 -1.4739e+00\n",
       "      1.0685e+00  1.0260e-01 -9.7400e-02 -1.3200e-02  1.1549e+00\n",
       "      2.3170e-01  3.3160e-01 -2.1640e-01 -9.4390e-01 -4.2570e-01\n",
       "      2.0142e+00 -2.0090e-01  2.8790e-01  5.7080e-01  3.4930e-01\n",
       "      3.2910e-01 -7.0810e-01  9.7410e-01  2.6920e-01 -1.0490e+00\n",
       "     -4.0650e-01 -9.0360e-01 -6.7700e-02  4.5550e-01 -4.8590e-01]\n",
       "    [-8.4890e-01  2.2286e+00  3.3770e-01 -3.5640e-01 -1.2340e-01\n",
       "     -1.1789e+00  2.1612e+00 -2.8810e-01 -9.2300e-02 -5.4100e-01\n",
       "      1.4058e+00 -7.0480e-01  2.7693e+00  1.5950e-01  9.6650e-01\n",
       "     -2.1623e+00  2.0180e-01 -7.5820e-01  1.1641e+00 -1.4971e+00\n",
       "      1.8786e+00 -4.9280e-01  1.7970e-01 -5.9300e-01  4.6460e-01\n",
       "     -3.2120e-01  2.4330e-01  3.3280e-01 -3.7070e-01 -1.6568e+00]]\n",
       "  \n",
       "   [[ 2.2310e-01 -2.1500e-02 -3.1390e-01 -4.6570e-01  6.7670e-01\n",
       "      2.2544e+00 -1.5913e+00  5.1100e-01 -7.5530e-01 -1.1589e+00\n",
       "      3.8050e-01 -3.0076e+00 -1.4753e+00  2.3500e-02  1.2733e+00\n",
       "      1.0164e+00 -4.6660e-01 -9.4600e-02  3.5830e-01  6.7620e-01\n",
       "     -2.6020e-01  1.6223e+00 -4.4850e-01  1.0237e+00  2.2250e-01\n",
       "     -1.8925e+00 -1.1991e+00  5.5300e-02 -1.4080e-01  7.1030e-01]\n",
       "    [ 1.6651e+00 -5.2900e-02 -8.9130e-01 -3.3320e-01  2.3650e-01\n",
       "      1.9940e-01  2.0270e-01  6.9910e-01 -1.6030e+00  6.4580e-01\n",
       "      2.3790e-01  5.9490e-01  6.8780e-01  9.9430e-01 -4.4870e-01\n",
       "      8.6870e-01 -1.0978e+00  2.1410e-01 -4.0020e-01  1.6441e+00\n",
       "      2.5780e+00  6.0790e-01 -1.0320e-01 -3.8960e-01 -1.1309e+00\n",
       "      9.5950e-01  1.4122e+00 -3.9450e-01 -7.0800e-02  7.1750e-01]]\n",
       "  \n",
       "   [[-6.5600e-01 -6.0230e-01 -1.7000e-02 -2.5280e-01 -4.0480e-01\n",
       "     -6.2650e-01  6.1840e-01  1.1778e+00 -9.2380e-01  1.1700e-02\n",
       "      1.6457e+00 -8.5360e-01  2.8720e-01  1.4870e-01 -3.8610e-01\n",
       "     -1.7279e+00 -7.9320e-01 -1.2116e+00 -9.0800e-01 -5.8950e-01\n",
       "     -9.0180e-01 -1.1482e+00 -1.1410e-01  8.5520e-01  8.9760e-01\n",
       "      1.0365e+00  6.0320e-01  1.7217e+00  2.6550e-01  5.7800e-01]\n",
       "    [-1.6390e-01  4.1930e-01 -9.3040e-01 -1.0540e-01 -1.0198e+00\n",
       "     -4.2310e-01 -5.6760e-01  1.4290e-01 -1.4513e+00 -9.6610e-01\n",
       "      5.8300e-02  1.5094e+00 -6.9060e-01 -4.4200e-02 -1.1366e+00\n",
       "     -4.3220e-01 -1.5480e-01  4.0330e-01  9.0920e-01 -1.1269e+00\n",
       "     -4.2060e-01 -7.4400e-01 -5.1620e-01  1.4570e+00 -1.6788e+00\n",
       "     -1.5250e-01 -7.5530e-01  3.7360e-01 -8.4400e-02 -8.2580e-01]]\n",
       "  \n",
       "   [[ 1.9070e+00 -4.3420e-01 -2.1665e+00  2.5982e+00  4.4760e-01\n",
       "      4.0730e-01  5.5060e-01 -4.8980e-01  5.9060e-01 -3.6480e-01\n",
       "     -1.4034e+00 -2.5380e-01  1.8552e+00 -7.9740e-01  3.6110e-01\n",
       "     -4.0600e-01 -7.3970e-01 -1.0464e+00  5.4800e-02 -6.6270e-01\n",
       "     -2.2696e+00 -9.0830e-01 -1.1317e+00 -3.4855e+00 -1.5822e+00\n",
       "      6.0000e-01 -1.7445e+00  7.6000e-03 -1.2245e+00  1.0287e+00]\n",
       "    [-1.2370e-01 -1.4980e-01 -1.3146e+00  8.8070e-01  5.6530e-01\n",
       "     -9.7100e-02 -2.6850e-01 -2.2650e-01  7.2420e-01  2.5000e-03\n",
       "      1.5025e+00 -4.4910e-01 -2.4000e-03  7.4280e-01  4.7950e-01\n",
       "     -7.3200e-02 -1.6539e+00 -1.6140e-01  2.1580e-01  9.6570e-01\n",
       "      1.4980e-01 -1.3661e+00  4.3530e-01  1.5165e+00  8.8100e-02\n",
       "      2.4890e-01  4.9330e-01 -2.3476e+00  6.6350e-01 -2.6860e-01]]\n",
       "  \n",
       "   [[ 7.6600e-01  1.2944e+00 -2.4330e-01  2.5610e-01  1.8751e+00\n",
       "      6.4280e-01  1.9780e-01 -7.4290e-01  1.1549e+00  4.3310e-01\n",
       "     -2.1390e-01 -2.4660e-01  3.5460e-01  1.1660e-01 -1.8678e+00\n",
       "     -1.4323e+00  6.1550e-01  1.8100e-01  2.2700e-02 -1.5867e+00\n",
       "     -1.3959e+00 -1.6073e+00  3.8050e-01 -5.0200e-02 -5.2390e-01\n",
       "      1.0628e+00 -6.0500e-01  5.5020e-01 -2.8710e-01 -1.0069e+00]\n",
       "    [ 6.6400e-02 -4.8380e-01 -1.8870e-01  1.6417e+00 -1.6320e-01\n",
       "      1.2198e+00  6.9200e-02  7.2090e-01 -7.9440e-01  2.5397e+00\n",
       "      1.3034e+00 -4.3350e-01  8.6220e-01  1.1659e+00  1.8459e+00\n",
       "     -3.6100e-01 -1.0925e+00 -4.0660e-01  2.0613e+00  4.7580e-01\n",
       "     -1.1295e+00 -5.6260e-01  1.2751e+00  1.5768e+00 -1.8678e+00\n",
       "      3.3400e-02 -9.1780e-01 -9.8350e-01 -4.3880e-01 -4.1000e-02]]])),\n",
       " (11.37184627929096,\n",
       "  Tensor([[[-4.9570e-01 -1.2251e+00  9.8840e-01  2.7400e-02  8.6630e-01\n",
       "     -5.2580e-01  1.0630e-01  2.7198e+00 -1.1489e+00 -3.6740e-01\n",
       "      4.4880e-01 -7.3370e-01 -2.0529e+00 -2.0540e-01 -1.1125e+00\n",
       "      6.1160e-01  6.1490e-01  1.3727e+00 -8.8930e-01  4.7830e-01\n",
       "     -1.0350e+00 -8.4850e-01 -2.5330e-01  7.3520e-01  1.5818e+00\n",
       "     -1.0196e+00  1.3350e-01 -7.7300e-02  1.1130e+00  1.5340e-01]\n",
       "    [ 1.2619e+00 -1.6560e-01  9.8200e-02  1.4630e+00 -3.2620e-01\n",
       "     -2.6550e-01 -1.1930e-01 -1.0223e+00 -5.6000e-03  1.0546e+00\n",
       "      9.7970e-01 -9.3050e-01 -5.0980e-01  1.0708e+00  2.3520e-01\n",
       "     -8.4260e-01  4.9080e-01  8.9230e-01 -2.8100e-02 -2.3800e-02\n",
       "     -5.5800e-02  7.1210e-01 -2.7930e-01 -1.0040e-01 -5.5520e-01\n",
       "      6.8150e-01  1.4822e+00 -4.3980e-01  2.0110e-01 -4.3610e-01]]\n",
       "  \n",
       "   [[ 2.6810e-01  3.8080e-01 -1.2532e+00  1.2541e+00 -2.3490e-01\n",
       "     -8.6770e-01  7.5060e-01 -1.9595e+00  8.5710e-01 -7.4740e-01\n",
       "      1.5610e-01  5.8700e-02  6.5660e-01 -6.8740e-01 -4.1030e-01\n",
       "      1.0423e+00  1.9370e+00  1.7559e+00  1.8491e+00 -1.0288e+00\n",
       "     -7.3380e-01  1.0210e-01  1.8350e-01 -2.4600e-02  1.6540e-01\n",
       "      1.2254e+00 -1.5680e+00 -2.8910e-01  5.5310e-01 -1.1989e+00]\n",
       "    [ 1.9326e+00  1.6654e+00  1.7300e-02  1.1582e+00 -9.4560e-01\n",
       "     -8.4600e-01 -1.0812e+00  1.1281e+00  5.7660e-01 -9.4780e-01\n",
       "      1.0602e+00 -8.2820e-01  1.0715e+00 -5.0930e-01  2.9840e-01\n",
       "     -1.9980e-01  1.4456e+00  1.0330e-01  1.1895e+00 -1.5410e+00\n",
       "     -1.2109e+00 -9.4320e-01  7.5420e-01  2.1950e-01  8.5650e-01\n",
       "     -2.7700e-02 -1.1373e+00  1.2440e-01 -1.1789e+00  2.8720e-01]]\n",
       "  \n",
       "   [[ 1.7030e-01 -4.1460e-01  4.9790e-01 -1.1103e+00  1.5740e-01\n",
       "      7.6700e-02  1.2117e+00 -9.4970e-01 -3.7430e-01  8.1360e-01\n",
       "      7.2800e-02 -1.3942e+00 -5.7000e-01  8.6830e-01 -1.6350e-01\n",
       "      8.3550e-01  8.8470e-01  7.5580e-01  3.4080e-01 -7.0170e-01\n",
       "      4.3930e-01 -1.4942e+00 -1.0374e+00 -7.0810e-01 -1.0585e+00\n",
       "     -6.4710e-01 -8.7160e-01  7.6490e-01 -1.5280e+00 -4.5210e-01]\n",
       "    [ 5.1630e-01 -1.0800e+00 -1.0421e+00  1.7998e+00  5.1490e-01\n",
       "     -1.3175e+00 -6.6620e-01 -1.2347e+00  1.0819e+00 -1.5629e+00\n",
       "      8.4280e-01  4.9470e-01  1.4201e+00  1.4188e+00 -1.9790e-01\n",
       "      9.0100e-01  5.4310e-01  1.8221e+00 -3.7740e-01  4.3880e-01\n",
       "     -4.2360e-01 -1.6484e+00  4.9210e-01 -6.8000e-03 -1.9000e+00\n",
       "     -1.8784e+00  3.8540e-01  9.4630e-01  1.0188e+00  8.2490e-01]]\n",
       "  \n",
       "   [[-3.1390e-01 -2.1900e-02  1.9500e-02  1.8810e-01  7.3350e-01\n",
       "     -1.2990e-01 -4.2150e-01  4.1030e-01  1.6973e+00  8.8630e-01\n",
       "     -1.5414e+00  3.4820e-01 -9.1480e-01  1.6800e-01  8.1900e-02\n",
       "      7.9600e-01 -2.4600e-02  1.9740e-01 -8.2870e-01  8.4810e-01\n",
       "      5.4110e-01  5.9080e-01 -1.5415e+00 -7.0060e-01 -5.6500e-01\n",
       "     -5.4260e-01 -6.4740e-01 -1.2311e+00 -5.7280e-01 -3.6600e-02]\n",
       "    [ 4.3620e-01 -2.3300e-02  1.1268e+00 -4.0070e-01 -5.2640e-01\n",
       "     -1.1444e+00  1.5110e-01 -7.6220e-01 -9.3740e-01  4.4870e-01\n",
       "      1.1643e+00 -4.4700e-01 -3.8620e-01  2.7840e-01  9.8010e-01\n",
       "      2.5000e-03  7.6700e-01  3.3110e-01 -1.4410e-01  1.9780e-01\n",
       "     -1.1335e+00  1.0260e+00  1.0908e+00 -1.2373e+00  2.7030e-01\n",
       "      3.0310e-01 -9.2100e-02 -1.2510e+00 -1.4881e+00 -7.0000e-02]]\n",
       "  \n",
       "   [[-2.1346e+00 -9.5190e-01 -1.1240e-01 -5.7710e-01  1.6034e+00\n",
       "      1.2005e+00 -7.7820e-01 -1.1885e+00 -6.8680e-01 -4.4390e-01\n",
       "      1.2584e+00  1.8700e-01 -1.7278e+00 -1.0174e+00  3.6650e-01\n",
       "      1.4224e+00 -8.4420e-01  2.1801e+00  5.5160e-01 -2.0152e+00\n",
       "      1.8423e+00 -1.2078e+00  1.5774e+00 -5.4400e-02 -2.2350e+00\n",
       "      4.4780e-01  5.0010e-01  5.1500e-02 -1.2297e+00  2.1460e-01]\n",
       "    [ 1.2736e+00  7.8890e-01 -1.5609e+00 -2.6960e-01  8.4570e-01\n",
       "      3.9450e-01 -2.4758e+00 -8.2360e-01 -1.2769e+00  1.0303e+00\n",
       "     -1.0883e+00  2.2187e+00 -9.0220e-01  5.6360e-01 -1.2649e+00\n",
       "      2.0491e+00 -7.6180e-01  5.7860e-01 -1.2315e+00 -1.1017e+00\n",
       "      1.9041e+00  1.1334e+00  6.6350e-01  4.4800e-02  1.6730e-01\n",
       "      1.4942e+00 -2.3280e+00  4.6880e-01  3.0800e-01 -1.7250e-01]]\n",
       "  \n",
       "   [[ 3.7230e-01 -9.1190e-01  1.0292e+00 -1.1220e-01  1.3897e+00\n",
       "      9.8040e-01 -1.3030e-01  2.4561e+00 -7.7200e-02  2.1954e+00\n",
       "     -1.2989e+00 -9.1450e-01 -1.1100e-01  1.2630e-01  1.0310e-01\n",
       "     -1.8608e+00  1.1753e+00 -6.6580e-01  1.1280e+00 -7.8600e-02\n",
       "      1.2628e+00  4.9300e-01 -5.0850e-01  1.1710e-01 -1.4025e+00\n",
       "     -1.1242e+00 -1.2399e+00  8.8770e-01  2.5040e-01 -2.2040e-01]\n",
       "    [ 6.3550e-01 -7.0000e-03  1.5405e+00 -6.3900e-02 -8.8490e-01\n",
       "      4.6300e-02  1.9591e+00 -5.9600e-01 -2.5408e+00  1.0300e+00\n",
       "      1.9236e+00  8.2000e-01 -6.3950e-01 -4.5630e-01 -1.6073e+00\n",
       "     -5.0130e-01  3.7530e-01  6.5210e-01 -5.2000e-03 -8.2840e-01\n",
       "     -8.5310e-01  5.7520e-01 -2.2440e-01  1.3243e+00 -9.8600e-02\n",
       "      1.8190e-01 -5.6900e-02 -3.5270e-01 -1.8270e-01  8.8670e-01]]\n",
       "  \n",
       "   [[-1.8614e+00 -4.2240e-01 -2.5710e-01  7.4590e-01  2.8820e-01\n",
       "      9.4020e-01  1.5883e+00  8.8070e-01  5.1080e-01 -7.3430e-01\n",
       "     -9.2820e-01 -8.8280e-01  6.2340e-01  1.6467e+00  3.2100e-01\n",
       "      3.7370e-01  2.9090e-01 -1.1201e+00 -2.2000e-01 -2.5020e-01\n",
       "     -2.3610e-01 -4.8060e-01  9.7620e-01 -1.5446e+00  5.3190e-01\n",
       "      3.8500e-02  3.5510e-01 -8.5350e-01 -9.6800e-02  9.4940e-01]\n",
       "    [ 4.3980e-01 -2.0366e+00  1.0579e+00  1.0765e+00 -5.5700e-01\n",
       "     -2.2718e+00 -4.2390e-01 -4.0460e-01 -2.4680e-01  9.8040e-01\n",
       "      5.2780e-01 -1.3402e+00  4.6490e-01  2.9580e-01 -2.7920e-01\n",
       "     -6.5640e-01 -1.0630e-01 -1.7900e-02  8.8790e-01  3.9900e-01\n",
       "      1.1248e+00 -6.2770e-01 -9.7830e-01  3.0672e+00  4.5000e-03\n",
       "     -1.6990e-01 -2.7460e-01  8.5550e-01 -2.9000e-01 -1.9140e-01]]\n",
       "  \n",
       "   [[ 2.0890e-01  1.1704e+00  1.4531e+00 -8.6200e-02 -1.4739e+00\n",
       "      1.0685e+00  1.0260e-01 -9.7400e-02 -1.3200e-02  1.1549e+00\n",
       "      2.3170e-01  3.3160e-01 -2.1640e-01 -9.4390e-01 -4.2570e-01\n",
       "      2.0142e+00 -2.0090e-01  2.8790e-01  5.7080e-01  3.4930e-01\n",
       "      3.2910e-01 -7.0810e-01  9.7410e-01  2.6920e-01 -1.0490e+00\n",
       "     -4.0650e-01 -9.0360e-01 -6.7700e-02  4.5550e-01 -4.8590e-01]\n",
       "    [-8.4890e-01  2.2286e+00  3.3770e-01 -3.5640e-01 -1.2340e-01\n",
       "     -1.1789e+00  2.1612e+00 -2.8810e-01 -9.2300e-02 -5.4100e-01\n",
       "      1.4058e+00 -7.0480e-01  2.7693e+00  1.5950e-01  9.6650e-01\n",
       "     -2.1623e+00  2.0180e-01 -7.5820e-01  1.1641e+00 -1.4971e+00\n",
       "      1.8786e+00 -4.9280e-01  1.7970e-01 -5.9300e-01  4.6460e-01\n",
       "     -3.2120e-01  2.4330e-01  3.3280e-01 -3.7070e-01 -1.6568e+00]]\n",
       "  \n",
       "   [[ 2.2310e-01 -2.1500e-02 -3.1390e-01 -4.6570e-01  6.7670e-01\n",
       "      2.2544e+00 -1.5913e+00  5.1100e-01 -7.5530e-01 -1.1589e+00\n",
       "      3.8050e-01 -3.0076e+00 -1.4753e+00  2.3500e-02  1.2733e+00\n",
       "      1.0164e+00 -4.6660e-01 -9.4600e-02  3.5830e-01  6.7620e-01\n",
       "     -2.6020e-01  1.6223e+00 -4.4850e-01  1.0237e+00  2.2250e-01\n",
       "     -1.8925e+00 -1.1991e+00  5.5300e-02 -1.4080e-01  7.1030e-01]\n",
       "    [ 1.6651e+00 -5.2900e-02 -8.9130e-01 -3.3320e-01  2.3650e-01\n",
       "      1.9940e-01  2.0270e-01  6.9910e-01 -1.6030e+00  6.4580e-01\n",
       "      2.3790e-01  5.9490e-01  6.8780e-01  9.9430e-01 -4.4870e-01\n",
       "      8.6870e-01 -1.0978e+00  2.1410e-01 -4.0020e-01  1.6441e+00\n",
       "      2.5780e+00  6.0790e-01 -1.0320e-01 -3.8960e-01 -1.1309e+00\n",
       "      9.5950e-01  1.4122e+00 -3.9450e-01 -7.0800e-02  7.1750e-01]]\n",
       "  \n",
       "   [[-6.5600e-01 -6.0230e-01 -1.7000e-02 -2.5280e-01 -4.0480e-01\n",
       "     -6.2650e-01  6.1840e-01  1.1778e+00 -9.2380e-01  1.1700e-02\n",
       "      1.6457e+00 -8.5360e-01  2.8720e-01  1.4870e-01 -3.8610e-01\n",
       "     -1.7279e+00 -7.9320e-01 -1.2116e+00 -9.0800e-01 -5.8950e-01\n",
       "     -9.0180e-01 -1.1482e+00 -1.1410e-01  8.5520e-01  8.9760e-01\n",
       "      1.0365e+00  6.0320e-01  1.7217e+00  2.6550e-01  5.7800e-01]\n",
       "    [-1.6390e-01  4.1930e-01 -9.3040e-01 -1.0540e-01 -1.0198e+00\n",
       "     -4.2310e-01 -5.6760e-01  1.4290e-01 -1.4513e+00 -9.6610e-01\n",
       "      5.8300e-02  1.5094e+00 -6.9060e-01 -4.4200e-02 -1.1366e+00\n",
       "     -4.3220e-01 -1.5480e-01  4.0330e-01  9.0920e-01 -1.1269e+00\n",
       "     -4.2060e-01 -7.4400e-01 -5.1620e-01  1.4570e+00 -1.6788e+00\n",
       "     -1.5250e-01 -7.5530e-01  3.7360e-01 -8.4400e-02 -8.2580e-01]]\n",
       "  \n",
       "   [[ 1.9070e+00 -4.3420e-01 -2.1665e+00  2.5982e+00  4.4760e-01\n",
       "      4.0730e-01  5.5060e-01 -4.8980e-01  5.9060e-01 -3.6480e-01\n",
       "     -1.4034e+00 -2.5380e-01  1.8552e+00 -7.9740e-01  3.6110e-01\n",
       "     -4.0600e-01 -7.3970e-01 -1.0464e+00  5.4800e-02 -6.6270e-01\n",
       "     -2.2696e+00 -9.0830e-01 -1.1317e+00 -3.4855e+00 -1.5822e+00\n",
       "      6.0000e-01 -1.7445e+00  7.6000e-03 -1.2245e+00  1.0287e+00]\n",
       "    [-1.2370e-01 -1.4980e-01 -1.3146e+00  8.8070e-01  5.6530e-01\n",
       "     -9.7100e-02 -2.6850e-01 -2.2650e-01  7.2420e-01  2.5000e-03\n",
       "      1.5025e+00 -4.4910e-01 -2.4000e-03  7.4280e-01  4.7950e-01\n",
       "     -7.3200e-02 -1.6539e+00 -1.6140e-01  2.1580e-01  9.6570e-01\n",
       "      1.4980e-01 -1.3661e+00  4.3530e-01  1.5165e+00  8.8100e-02\n",
       "      2.4890e-01  4.9330e-01 -2.3476e+00  6.6350e-01 -2.6860e-01]]\n",
       "  \n",
       "   [[ 7.6600e-01  1.2944e+00 -2.4330e-01  2.5610e-01  1.8751e+00\n",
       "      6.4280e-01  1.9780e-01 -7.4290e-01  1.1549e+00  4.3310e-01\n",
       "     -2.1390e-01 -2.4660e-01  3.5460e-01  1.1660e-01 -1.8678e+00\n",
       "     -1.4323e+00  6.1550e-01  1.8100e-01  2.2700e-02 -1.5867e+00\n",
       "     -1.3959e+00 -1.6073e+00  3.8050e-01 -5.0200e-02 -5.2390e-01\n",
       "      1.0628e+00 -6.0500e-01  5.5020e-01 -2.8710e-01 -1.0069e+00]\n",
       "    [ 6.6400e-02 -4.8380e-01 -1.8870e-01  1.6417e+00 -1.6320e-01\n",
       "      1.2198e+00  6.9200e-02  7.2090e-01 -7.9440e-01  2.5397e+00\n",
       "      1.3034e+00 -4.3350e-01  8.6220e-01  1.1659e+00  1.8459e+00\n",
       "     -3.6100e-01 -1.0925e+00 -4.0660e-01  2.0613e+00  4.7580e-01\n",
       "     -1.1295e+00 -5.6260e-01  1.2751e+00  1.5768e+00 -1.8678e+00\n",
       "      3.3400e-02 -9.1780e-01 -9.8350e-01 -4.3880e-01 -4.1000e-02]]])),\n",
       " (11.546474090837279,\n",
       "  Tensor([[[-4.9570e-01 -1.2251e+00  9.8840e-01  2.7400e-02  8.6630e-01\n",
       "     -5.2580e-01  1.0630e-01  2.7198e+00 -1.1489e+00 -3.6740e-01\n",
       "      4.4880e-01 -7.3370e-01 -2.0529e+00 -2.0540e-01 -1.1125e+00\n",
       "      6.1160e-01  6.1490e-01  1.3727e+00 -8.8930e-01  4.7830e-01\n",
       "     -1.0350e+00 -8.4850e-01 -2.5330e-01  7.3520e-01  1.5818e+00\n",
       "     -1.0196e+00  1.3350e-01 -7.7300e-02  1.1130e+00  1.5340e-01]\n",
       "    [ 1.2619e+00 -1.6560e-01  9.8200e-02  1.4630e+00 -3.2620e-01\n",
       "     -2.6550e-01 -1.1930e-01 -1.0223e+00 -5.6000e-03  1.0546e+00\n",
       "      9.7970e-01 -9.3050e-01 -5.0980e-01  1.0708e+00  2.3520e-01\n",
       "     -8.4260e-01  4.9080e-01  8.9230e-01 -2.8100e-02 -2.3800e-02\n",
       "     -5.5800e-02  7.1210e-01 -2.7930e-01 -1.0040e-01 -5.5520e-01\n",
       "      6.8150e-01  1.4822e+00 -4.3980e-01  2.0110e-01 -4.3610e-01]]\n",
       "  \n",
       "   [[ 2.6810e-01  3.8080e-01 -1.2532e+00  1.2541e+00 -2.3490e-01\n",
       "     -8.6770e-01  7.5060e-01 -1.9595e+00  8.5710e-01 -7.4740e-01\n",
       "      1.5610e-01  5.8700e-02  6.5660e-01 -6.8740e-01 -4.1030e-01\n",
       "      1.0423e+00  1.9370e+00  1.7559e+00  1.8491e+00 -1.0288e+00\n",
       "     -7.3380e-01  1.0210e-01  1.8350e-01 -2.4600e-02  1.6540e-01\n",
       "      1.2254e+00 -1.5680e+00 -2.8910e-01  5.5310e-01 -1.1989e+00]\n",
       "    [ 1.9326e+00  1.6654e+00  1.7300e-02  1.1582e+00 -9.4560e-01\n",
       "     -8.4600e-01 -1.0812e+00  1.1281e+00  5.7660e-01 -9.4780e-01\n",
       "      1.0602e+00 -8.2820e-01  1.0715e+00 -5.0930e-01  2.9840e-01\n",
       "     -1.9980e-01  1.4456e+00  1.0330e-01  1.1895e+00 -1.5410e+00\n",
       "     -1.2109e+00 -9.4320e-01  7.5420e-01  2.1950e-01  8.5650e-01\n",
       "     -2.7700e-02 -1.1373e+00  1.2440e-01 -1.1789e+00  2.8720e-01]]\n",
       "  \n",
       "   [[ 1.7030e-01 -4.1460e-01  4.9790e-01 -1.1103e+00  1.5740e-01\n",
       "      7.6700e-02  1.2117e+00 -9.4970e-01 -3.7430e-01  8.1360e-01\n",
       "      7.2800e-02 -1.3942e+00 -5.7000e-01  8.6830e-01 -1.6350e-01\n",
       "      8.3550e-01  8.8470e-01  7.5580e-01  3.4080e-01 -7.0170e-01\n",
       "      4.3930e-01 -1.4942e+00 -1.0374e+00 -7.0810e-01 -1.0585e+00\n",
       "     -6.4710e-01 -8.7160e-01  7.6490e-01 -1.5280e+00 -4.5210e-01]\n",
       "    [ 5.1630e-01 -1.0800e+00 -1.0421e+00  1.7998e+00  5.1490e-01\n",
       "     -1.3175e+00 -6.6620e-01 -1.2347e+00  1.0819e+00 -1.5629e+00\n",
       "      8.4280e-01  4.9470e-01  1.4201e+00  1.4188e+00 -1.9790e-01\n",
       "      9.0100e-01  5.4310e-01  1.8221e+00 -3.7740e-01  4.3880e-01\n",
       "     -4.2360e-01 -1.6484e+00  4.9210e-01 -6.8000e-03 -1.9000e+00\n",
       "     -1.8784e+00  3.8540e-01  9.4630e-01  1.0188e+00  8.2490e-01]]\n",
       "  \n",
       "   [[-3.1390e-01 -2.1900e-02  1.9500e-02  1.8810e-01  7.3350e-01\n",
       "     -1.2990e-01 -4.2150e-01  4.1030e-01  1.6973e+00  8.8630e-01\n",
       "     -1.5414e+00  3.4820e-01 -9.1480e-01  1.6800e-01  8.1900e-02\n",
       "      7.9600e-01 -2.4600e-02  1.9740e-01 -8.2870e-01  8.4810e-01\n",
       "      5.4110e-01  5.9080e-01 -1.5415e+00 -7.0060e-01 -5.6500e-01\n",
       "     -5.4260e-01 -6.4740e-01 -1.2311e+00 -5.7280e-01 -3.6600e-02]\n",
       "    [ 4.3620e-01 -2.3300e-02  1.1268e+00 -4.0070e-01 -5.2640e-01\n",
       "     -1.1444e+00  1.5110e-01 -7.6220e-01 -9.3740e-01  4.4870e-01\n",
       "      1.1643e+00 -4.4700e-01 -3.8620e-01  2.7840e-01  9.8010e-01\n",
       "      2.5000e-03  7.6700e-01  3.3110e-01 -1.4410e-01  1.9780e-01\n",
       "     -1.1335e+00  1.0260e+00  1.0908e+00 -1.2373e+00  2.7030e-01\n",
       "      3.0310e-01 -9.2100e-02 -1.2510e+00 -1.4881e+00 -7.0000e-02]]\n",
       "  \n",
       "   [[-2.1346e+00 -9.5190e-01 -1.1240e-01 -5.7710e-01  1.6034e+00\n",
       "      1.2005e+00 -7.7820e-01 -1.1885e+00 -6.8680e-01 -4.4390e-01\n",
       "      1.2584e+00  1.8700e-01 -1.7278e+00 -1.0174e+00  3.6650e-01\n",
       "      1.4224e+00 -8.4420e-01  2.1801e+00  5.5160e-01 -2.0152e+00\n",
       "      1.8423e+00 -1.2078e+00  1.5774e+00 -5.4400e-02 -2.2350e+00\n",
       "      4.4780e-01  5.0010e-01  5.1500e-02 -1.2297e+00  2.1460e-01]\n",
       "    [ 1.2736e+00  7.8890e-01 -1.5609e+00 -2.6960e-01  8.4570e-01\n",
       "      3.9450e-01 -2.4758e+00 -8.2360e-01 -1.2769e+00  1.0303e+00\n",
       "     -1.0883e+00  2.2187e+00 -9.0220e-01  5.6360e-01 -1.2649e+00\n",
       "      2.0491e+00 -7.6180e-01  5.7860e-01 -1.2315e+00 -1.1017e+00\n",
       "      1.9041e+00  1.1334e+00  6.6350e-01  4.4800e-02  1.6730e-01\n",
       "      1.4942e+00 -2.3280e+00  4.6880e-01  3.0800e-01 -1.7250e-01]]\n",
       "  \n",
       "   [[ 3.7230e-01 -9.1190e-01  1.0292e+00 -1.1220e-01  1.3897e+00\n",
       "      9.8040e-01 -1.3030e-01  2.4561e+00 -7.7200e-02  2.1954e+00\n",
       "     -1.2989e+00 -9.1450e-01 -1.1100e-01  1.2630e-01  1.0310e-01\n",
       "     -1.8608e+00  1.1753e+00 -6.6580e-01  1.1280e+00 -7.8600e-02\n",
       "      1.2628e+00  4.9300e-01 -5.0850e-01  1.1710e-01 -1.4025e+00\n",
       "     -1.1242e+00 -1.2399e+00  8.8770e-01  2.5040e-01 -2.2040e-01]\n",
       "    [ 6.3550e-01 -7.0000e-03  1.5405e+00 -6.3900e-02 -8.8490e-01\n",
       "      4.6300e-02  1.9591e+00 -5.9600e-01 -2.5408e+00  1.0300e+00\n",
       "      1.9236e+00  8.2000e-01 -6.3950e-01 -4.5630e-01 -1.6073e+00\n",
       "     -5.0130e-01  3.7530e-01  6.5210e-01 -5.2000e-03 -8.2840e-01\n",
       "     -8.5310e-01  5.7520e-01 -2.2440e-01  1.3243e+00 -9.8600e-02\n",
       "      1.8190e-01 -5.6900e-02 -3.5270e-01 -1.8270e-01  8.8670e-01]]\n",
       "  \n",
       "   [[-1.8614e+00 -4.2240e-01 -2.5710e-01  7.4590e-01  2.8820e-01\n",
       "      9.4020e-01  1.5883e+00  8.8070e-01  5.1080e-01 -7.3430e-01\n",
       "     -9.2820e-01 -8.8280e-01  6.2340e-01  1.6467e+00  3.2100e-01\n",
       "      3.7370e-01  2.9090e-01 -1.1201e+00 -2.2000e-01 -2.5020e-01\n",
       "     -2.3610e-01 -4.8060e-01  9.7620e-01 -1.5446e+00  5.3190e-01\n",
       "      3.8500e-02  3.5510e-01 -8.5350e-01 -9.6800e-02  9.4940e-01]\n",
       "    [ 4.3980e-01 -2.0366e+00  1.0579e+00  1.0765e+00 -5.5700e-01\n",
       "     -2.2718e+00 -4.2390e-01 -4.0460e-01 -2.4680e-01  9.8040e-01\n",
       "      5.2780e-01 -1.3402e+00  4.6490e-01  2.9580e-01 -2.7920e-01\n",
       "     -6.5640e-01 -1.0630e-01 -1.7900e-02  8.8790e-01  3.9900e-01\n",
       "      1.1248e+00 -6.2770e-01 -9.7830e-01  3.0672e+00  4.5000e-03\n",
       "     -1.6990e-01 -2.7460e-01  8.5550e-01 -2.9000e-01 -1.9140e-01]]\n",
       "  \n",
       "   [[ 2.0890e-01  1.1704e+00  1.4531e+00 -8.6200e-02 -1.4739e+00\n",
       "      1.0685e+00  1.0260e-01 -9.7400e-02 -1.3200e-02  1.1549e+00\n",
       "      2.3170e-01  3.3160e-01 -2.1640e-01 -9.4390e-01 -4.2570e-01\n",
       "      2.0142e+00 -2.0090e-01  2.8790e-01  5.7080e-01  3.4930e-01\n",
       "      3.2910e-01 -7.0810e-01  9.7410e-01  2.6920e-01 -1.0490e+00\n",
       "     -4.0650e-01 -9.0360e-01 -6.7700e-02  4.5550e-01 -4.8590e-01]\n",
       "    [-8.4890e-01  2.2286e+00  3.3770e-01 -3.5640e-01 -1.2340e-01\n",
       "     -1.1789e+00  2.1612e+00 -2.8810e-01 -9.2300e-02 -5.4100e-01\n",
       "      1.4058e+00 -7.0480e-01  2.7693e+00  1.5950e-01  9.6650e-01\n",
       "     -2.1623e+00  2.0180e-01 -7.5820e-01  1.1641e+00 -1.4971e+00\n",
       "      1.8786e+00 -4.9280e-01  1.7970e-01 -5.9300e-01  4.6460e-01\n",
       "     -3.2120e-01  2.4330e-01  3.3280e-01 -3.7070e-01 -1.6568e+00]]\n",
       "  \n",
       "   [[ 2.2310e-01 -2.1500e-02 -3.1390e-01 -4.6570e-01  6.7670e-01\n",
       "      2.2544e+00 -1.5913e+00  5.1100e-01 -7.5530e-01 -1.1589e+00\n",
       "      3.8050e-01 -3.0076e+00 -1.4753e+00  2.3500e-02  1.2733e+00\n",
       "      1.0164e+00 -4.6660e-01 -9.4600e-02  3.5830e-01  6.7620e-01\n",
       "     -2.6020e-01  1.6223e+00 -4.4850e-01  1.0237e+00  2.2250e-01\n",
       "     -1.8925e+00 -1.1991e+00  5.5300e-02 -1.4080e-01  7.1030e-01]\n",
       "    [ 1.6651e+00 -5.2900e-02 -8.9130e-01 -3.3320e-01  2.3650e-01\n",
       "      1.9940e-01  2.0270e-01  6.9910e-01 -1.6030e+00  6.4580e-01\n",
       "      2.3790e-01  5.9490e-01  6.8780e-01  9.9430e-01 -4.4870e-01\n",
       "      8.6870e-01 -1.0978e+00  2.1410e-01 -4.0020e-01  1.6441e+00\n",
       "      2.5780e+00  6.0790e-01 -1.0320e-01 -3.8960e-01 -1.1309e+00\n",
       "      9.5950e-01  1.4122e+00 -3.9450e-01 -7.0800e-02  7.1750e-01]]\n",
       "  \n",
       "   [[-6.5600e-01 -6.0230e-01 -1.7000e-02 -2.5280e-01 -4.0480e-01\n",
       "     -6.2650e-01  6.1840e-01  1.1778e+00 -9.2380e-01  1.1700e-02\n",
       "      1.6457e+00 -8.5360e-01  2.8720e-01  1.4870e-01 -3.8610e-01\n",
       "     -1.7279e+00 -7.9320e-01 -1.2116e+00 -9.0800e-01 -5.8950e-01\n",
       "     -9.0180e-01 -1.1482e+00 -1.1410e-01  8.5520e-01  8.9760e-01\n",
       "      1.0365e+00  6.0320e-01  1.7217e+00  2.6550e-01  5.7800e-01]\n",
       "    [-1.6390e-01  4.1930e-01 -9.3040e-01 -1.0540e-01 -1.0198e+00\n",
       "     -4.2310e-01 -5.6760e-01  1.4290e-01 -1.4513e+00 -9.6610e-01\n",
       "      5.8300e-02  1.5094e+00 -6.9060e-01 -4.4200e-02 -1.1366e+00\n",
       "     -4.3220e-01 -1.5480e-01  4.0330e-01  9.0920e-01 -1.1269e+00\n",
       "     -4.2060e-01 -7.4400e-01 -5.1620e-01  1.4570e+00 -1.6788e+00\n",
       "     -1.5250e-01 -7.5530e-01  3.7360e-01 -8.4400e-02 -8.2580e-01]]\n",
       "  \n",
       "   [[ 1.9070e+00 -4.3420e-01 -2.1665e+00  2.5982e+00  4.4760e-01\n",
       "      4.0730e-01  5.5060e-01 -4.8980e-01  5.9060e-01 -3.6480e-01\n",
       "     -1.4034e+00 -2.5380e-01  1.8552e+00 -7.9740e-01  3.6110e-01\n",
       "     -4.0600e-01 -7.3970e-01 -1.0464e+00  5.4800e-02 -6.6270e-01\n",
       "     -2.2696e+00 -9.0830e-01 -1.1317e+00 -3.4855e+00 -1.5822e+00\n",
       "      6.0000e-01 -1.7445e+00  7.6000e-03 -1.2245e+00  1.0287e+00]\n",
       "    [-1.2370e-01 -1.4980e-01 -1.3146e+00  8.8070e-01  5.6530e-01\n",
       "     -9.7100e-02 -2.6850e-01 -2.2650e-01  7.2420e-01  2.5000e-03\n",
       "      1.5025e+00 -4.4910e-01 -2.4000e-03  7.4280e-01  4.7950e-01\n",
       "     -7.3200e-02 -1.6539e+00 -1.6140e-01  2.1580e-01  9.6570e-01\n",
       "      1.4980e-01 -1.3661e+00  4.3530e-01  1.5165e+00  8.8100e-02\n",
       "      2.4890e-01  4.9330e-01 -2.3476e+00  6.6350e-01 -2.6860e-01]]\n",
       "  \n",
       "   [[ 7.6600e-01  1.2944e+00 -2.4330e-01  2.5610e-01  1.8751e+00\n",
       "      6.4280e-01  1.9780e-01 -7.4290e-01  1.1549e+00  4.3310e-01\n",
       "     -2.1390e-01 -2.4660e-01  3.5460e-01  1.1660e-01 -1.8678e+00\n",
       "     -1.4323e+00  6.1550e-01  1.8100e-01  2.2700e-02 -1.5867e+00\n",
       "     -1.3959e+00 -1.6073e+00  3.8050e-01 -5.0200e-02 -5.2390e-01\n",
       "      1.0628e+00 -6.0500e-01  5.5020e-01 -2.8710e-01 -1.0069e+00]\n",
       "    [ 6.6400e-02 -4.8380e-01 -1.8870e-01  1.6417e+00 -1.6320e-01\n",
       "      1.2198e+00  6.9200e-02  7.2090e-01 -7.9440e-01  2.5397e+00\n",
       "      1.3034e+00 -4.3350e-01  8.6220e-01  1.1659e+00  1.8459e+00\n",
       "     -3.6100e-01 -1.0925e+00 -4.0660e-01  2.0613e+00  4.7580e-01\n",
       "     -1.1295e+00 -5.6260e-01  1.2751e+00  1.5768e+00 -1.8678e+00\n",
       "      3.3400e-02 -9.1780e-01 -9.8350e-01 -4.3880e-01 -4.1000e-02]]])),\n",
       " (11.729079165566212,\n",
       "  Tensor([[[-4.9570e-01 -1.2251e+00  9.8840e-01  2.7400e-02  8.6630e-01\n",
       "     -5.2580e-01  1.0630e-01  2.7198e+00 -1.1489e+00 -3.6740e-01\n",
       "      4.4880e-01 -7.3370e-01 -2.0529e+00 -2.0540e-01 -1.1125e+00\n",
       "      6.1160e-01  6.1490e-01  1.3727e+00 -8.8930e-01  4.7830e-01\n",
       "     -1.0350e+00 -8.4850e-01 -2.5330e-01  7.3520e-01  1.5818e+00\n",
       "     -1.0196e+00  1.3350e-01 -7.7300e-02  1.1130e+00  1.5340e-01]\n",
       "    [ 1.2619e+00 -1.6560e-01  9.8200e-02  1.4630e+00 -3.2620e-01\n",
       "     -2.6550e-01 -1.1930e-01 -1.0223e+00 -5.6000e-03  1.0546e+00\n",
       "      9.7970e-01 -9.3050e-01 -5.0980e-01  1.0708e+00  2.3520e-01\n",
       "     -8.4260e-01  4.9080e-01  8.9230e-01 -2.8100e-02 -2.3800e-02\n",
       "     -5.5800e-02  7.1210e-01 -2.7930e-01 -1.0040e-01 -5.5520e-01\n",
       "      6.8150e-01  1.4822e+00 -4.3980e-01  2.0110e-01 -4.3610e-01]]\n",
       "  \n",
       "   [[ 2.6810e-01  3.8080e-01 -1.2532e+00  1.2541e+00 -2.3490e-01\n",
       "     -8.6770e-01  7.5060e-01 -1.9595e+00  8.5710e-01 -7.4740e-01\n",
       "      1.5610e-01  5.8700e-02  6.5660e-01 -6.8740e-01 -4.1030e-01\n",
       "      1.0423e+00  1.9370e+00  1.7559e+00  1.8491e+00 -1.0288e+00\n",
       "     -7.3380e-01  1.0210e-01  1.8350e-01 -2.4600e-02  1.6540e-01\n",
       "      1.2254e+00 -1.5680e+00 -2.8910e-01  5.5310e-01 -1.1989e+00]\n",
       "    [ 1.9326e+00  1.6654e+00  1.7300e-02  1.1582e+00 -9.4560e-01\n",
       "     -8.4600e-01 -1.0812e+00  1.1281e+00  5.7660e-01 -9.4780e-01\n",
       "      1.0602e+00 -8.2820e-01  1.0715e+00 -5.0930e-01  2.9840e-01\n",
       "     -1.9980e-01  1.4456e+00  1.0330e-01  1.1895e+00 -1.5410e+00\n",
       "     -1.2109e+00 -9.4320e-01  7.5420e-01  2.1950e-01  8.5650e-01\n",
       "     -2.7700e-02 -1.1373e+00  1.2440e-01 -1.1789e+00  2.8720e-01]]\n",
       "  \n",
       "   [[ 1.7030e-01 -4.1460e-01  4.9790e-01 -1.1103e+00  1.5740e-01\n",
       "      7.6700e-02  1.2117e+00 -9.4970e-01 -3.7430e-01  8.1360e-01\n",
       "      7.2800e-02 -1.3942e+00 -5.7000e-01  8.6830e-01 -1.6350e-01\n",
       "      8.3550e-01  8.8470e-01  7.5580e-01  3.4080e-01 -7.0170e-01\n",
       "      4.3930e-01 -1.4942e+00 -1.0374e+00 -7.0810e-01 -1.0585e+00\n",
       "     -6.4710e-01 -8.7160e-01  7.6490e-01 -1.5280e+00 -4.5210e-01]\n",
       "    [ 5.1630e-01 -1.0800e+00 -1.0421e+00  1.7998e+00  5.1490e-01\n",
       "     -1.3175e+00 -6.6620e-01 -1.2347e+00  1.0819e+00 -1.5629e+00\n",
       "      8.4280e-01  4.9470e-01  1.4201e+00  1.4188e+00 -1.9790e-01\n",
       "      9.0100e-01  5.4310e-01  1.8221e+00 -3.7740e-01  4.3880e-01\n",
       "     -4.2360e-01 -1.6484e+00  4.9210e-01 -6.8000e-03 -1.9000e+00\n",
       "     -1.8784e+00  3.8540e-01  9.4630e-01  1.0188e+00  8.2490e-01]]\n",
       "  \n",
       "   [[-3.1390e-01 -2.1900e-02  1.9500e-02  1.8810e-01  7.3350e-01\n",
       "     -1.2990e-01 -4.2150e-01  4.1030e-01  1.6973e+00  8.8630e-01\n",
       "     -1.5414e+00  3.4820e-01 -9.1480e-01  1.6800e-01  8.1900e-02\n",
       "      7.9600e-01 -2.4600e-02  1.9740e-01 -8.2870e-01  8.4810e-01\n",
       "      5.4110e-01  5.9080e-01 -1.5415e+00 -7.0060e-01 -5.6500e-01\n",
       "     -5.4260e-01 -6.4740e-01 -1.2311e+00 -5.7280e-01 -3.6600e-02]\n",
       "    [ 4.3620e-01 -2.3300e-02  1.1268e+00 -4.0070e-01 -5.2640e-01\n",
       "     -1.1444e+00  1.5110e-01 -7.6220e-01 -9.3740e-01  4.4870e-01\n",
       "      1.1643e+00 -4.4700e-01 -3.8620e-01  2.7840e-01  9.8010e-01\n",
       "      2.5000e-03  7.6700e-01  3.3110e-01 -1.4410e-01  1.9780e-01\n",
       "     -1.1335e+00  1.0260e+00  1.0908e+00 -1.2373e+00  2.7030e-01\n",
       "      3.0310e-01 -9.2100e-02 -1.2510e+00 -1.4881e+00 -7.0000e-02]]\n",
       "  \n",
       "   [[-2.1346e+00 -9.5190e-01 -1.1240e-01 -5.7710e-01  1.6034e+00\n",
       "      1.2005e+00 -7.7820e-01 -1.1885e+00 -6.8680e-01 -4.4390e-01\n",
       "      1.2584e+00  1.8700e-01 -1.7278e+00 -1.0174e+00  3.6650e-01\n",
       "      1.4224e+00 -8.4420e-01  2.1801e+00  5.5160e-01 -2.0152e+00\n",
       "      1.8423e+00 -1.2078e+00  1.5774e+00 -5.4400e-02 -2.2350e+00\n",
       "      4.4780e-01  5.0010e-01  5.1500e-02 -1.2297e+00  2.1460e-01]\n",
       "    [ 1.2736e+00  7.8890e-01 -1.5609e+00 -2.6960e-01  8.4570e-01\n",
       "      3.9450e-01 -2.4758e+00 -8.2360e-01 -1.2769e+00  1.0303e+00\n",
       "     -1.0883e+00  2.2187e+00 -9.0220e-01  5.6360e-01 -1.2649e+00\n",
       "      2.0491e+00 -7.6180e-01  5.7860e-01 -1.2315e+00 -1.1017e+00\n",
       "      1.9041e+00  1.1334e+00  6.6350e-01  4.4800e-02  1.6730e-01\n",
       "      1.4942e+00 -2.3280e+00  4.6880e-01  3.0800e-01 -1.7250e-01]]\n",
       "  \n",
       "   [[ 3.7230e-01 -9.1190e-01  1.0292e+00 -1.1220e-01  1.3897e+00\n",
       "      9.8040e-01 -1.3030e-01  2.4561e+00 -7.7200e-02  2.1954e+00\n",
       "     -1.2989e+00 -9.1450e-01 -1.1100e-01  1.2630e-01  1.0310e-01\n",
       "     -1.8608e+00  1.1753e+00 -6.6580e-01  1.1280e+00 -7.8600e-02\n",
       "      1.2628e+00  4.9300e-01 -5.0850e-01  1.1710e-01 -1.4025e+00\n",
       "     -1.1242e+00 -1.2399e+00  8.8770e-01  2.5040e-01 -2.2040e-01]\n",
       "    [ 6.3550e-01 -7.0000e-03  1.5405e+00 -6.3900e-02 -8.8490e-01\n",
       "      4.6300e-02  1.9591e+00 -5.9600e-01 -2.5408e+00  1.0300e+00\n",
       "      1.9236e+00  8.2000e-01 -6.3950e-01 -4.5630e-01 -1.6073e+00\n",
       "     -5.0130e-01  3.7530e-01  6.5210e-01 -5.2000e-03 -8.2840e-01\n",
       "     -8.5310e-01  5.7520e-01 -2.2440e-01  1.3243e+00 -9.8600e-02\n",
       "      1.8190e-01 -5.6900e-02 -3.5270e-01 -1.8270e-01  8.8670e-01]]\n",
       "  \n",
       "   [[-1.8614e+00 -4.2240e-01 -2.5710e-01  7.4590e-01  2.8820e-01\n",
       "      9.4020e-01  1.5883e+00  8.8070e-01  5.1080e-01 -7.3430e-01\n",
       "     -9.2820e-01 -8.8280e-01  6.2340e-01  1.6467e+00  3.2100e-01\n",
       "      3.7370e-01  2.9090e-01 -1.1201e+00 -2.2000e-01 -2.5020e-01\n",
       "     -2.3610e-01 -4.8060e-01  9.7620e-01 -1.5446e+00  5.3190e-01\n",
       "      3.8500e-02  3.5510e-01 -8.5350e-01 -9.6800e-02  9.4940e-01]\n",
       "    [ 4.3980e-01 -2.0366e+00  1.0579e+00  1.0765e+00 -5.5700e-01\n",
       "     -2.2718e+00 -4.2390e-01 -4.0460e-01 -2.4680e-01  9.8040e-01\n",
       "      5.2780e-01 -1.3402e+00  4.6490e-01  2.9580e-01 -2.7920e-01\n",
       "     -6.5640e-01 -1.0630e-01 -1.7900e-02  8.8790e-01  3.9900e-01\n",
       "      1.1248e+00 -6.2770e-01 -9.7830e-01  3.0672e+00  4.5000e-03\n",
       "     -1.6990e-01 -2.7460e-01  8.5550e-01 -2.9000e-01 -1.9140e-01]]\n",
       "  \n",
       "   [[ 2.0890e-01  1.1704e+00  1.4531e+00 -8.6200e-02 -1.4739e+00\n",
       "      1.0685e+00  1.0260e-01 -9.7400e-02 -1.3200e-02  1.1549e+00\n",
       "      2.3170e-01  3.3160e-01 -2.1640e-01 -9.4390e-01 -4.2570e-01\n",
       "      2.0142e+00 -2.0090e-01  2.8790e-01  5.7080e-01  3.4930e-01\n",
       "      3.2910e-01 -7.0810e-01  9.7410e-01  2.6920e-01 -1.0490e+00\n",
       "     -4.0650e-01 -9.0360e-01 -6.7700e-02  4.5550e-01 -4.8590e-01]\n",
       "    [-8.4890e-01  2.2286e+00  3.3770e-01 -3.5640e-01 -1.2340e-01\n",
       "     -1.1789e+00  2.1612e+00 -2.8810e-01 -9.2300e-02 -5.4100e-01\n",
       "      1.4058e+00 -7.0480e-01  2.7693e+00  1.5950e-01  9.6650e-01\n",
       "     -2.1623e+00  2.0180e-01 -7.5820e-01  1.1641e+00 -1.4971e+00\n",
       "      1.8786e+00 -4.9280e-01  1.7970e-01 -5.9300e-01  4.6460e-01\n",
       "     -3.2120e-01  2.4330e-01  3.3280e-01 -3.7070e-01 -1.6568e+00]]\n",
       "  \n",
       "   [[ 2.2310e-01 -2.1500e-02 -3.1390e-01 -4.6570e-01  6.7670e-01\n",
       "      2.2544e+00 -1.5913e+00  5.1100e-01 -7.5530e-01 -1.1589e+00\n",
       "      3.8050e-01 -3.0076e+00 -1.4753e+00  2.3500e-02  1.2733e+00\n",
       "      1.0164e+00 -4.6660e-01 -9.4600e-02  3.5830e-01  6.7620e-01\n",
       "     -2.6020e-01  1.6223e+00 -4.4850e-01  1.0237e+00  2.2250e-01\n",
       "     -1.8925e+00 -1.1991e+00  5.5300e-02 -1.4080e-01  7.1030e-01]\n",
       "    [ 1.6651e+00 -5.2900e-02 -8.9130e-01 -3.3320e-01  2.3650e-01\n",
       "      1.9940e-01  2.0270e-01  6.9910e-01 -1.6030e+00  6.4580e-01\n",
       "      2.3790e-01  5.9490e-01  6.8780e-01  9.9430e-01 -4.4870e-01\n",
       "      8.6870e-01 -1.0978e+00  2.1410e-01 -4.0020e-01  1.6441e+00\n",
       "      2.5780e+00  6.0790e-01 -1.0320e-01 -3.8960e-01 -1.1309e+00\n",
       "      9.5950e-01  1.4122e+00 -3.9450e-01 -7.0800e-02  7.1750e-01]]\n",
       "  \n",
       "   [[-6.5600e-01 -6.0230e-01 -1.7000e-02 -2.5280e-01 -4.0480e-01\n",
       "     -6.2650e-01  6.1840e-01  1.1778e+00 -9.2380e-01  1.1700e-02\n",
       "      1.6457e+00 -8.5360e-01  2.8720e-01  1.4870e-01 -3.8610e-01\n",
       "     -1.7279e+00 -7.9320e-01 -1.2116e+00 -9.0800e-01 -5.8950e-01\n",
       "     -9.0180e-01 -1.1482e+00 -1.1410e-01  8.5520e-01  8.9760e-01\n",
       "      1.0365e+00  6.0320e-01  1.7217e+00  2.6550e-01  5.7800e-01]\n",
       "    [-1.6390e-01  4.1930e-01 -9.3040e-01 -1.0540e-01 -1.0198e+00\n",
       "     -4.2310e-01 -5.6760e-01  1.4290e-01 -1.4513e+00 -9.6610e-01\n",
       "      5.8300e-02  1.5094e+00 -6.9060e-01 -4.4200e-02 -1.1366e+00\n",
       "     -4.3220e-01 -1.5480e-01  4.0330e-01  9.0920e-01 -1.1269e+00\n",
       "     -4.2060e-01 -7.4400e-01 -5.1620e-01  1.4570e+00 -1.6788e+00\n",
       "     -1.5250e-01 -7.5530e-01  3.7360e-01 -8.4400e-02 -8.2580e-01]]\n",
       "  \n",
       "   [[ 1.9070e+00 -4.3420e-01 -2.1665e+00  2.5982e+00  4.4760e-01\n",
       "      4.0730e-01  5.5060e-01 -4.8980e-01  5.9060e-01 -3.6480e-01\n",
       "     -1.4034e+00 -2.5380e-01  1.8552e+00 -7.9740e-01  3.6110e-01\n",
       "     -4.0600e-01 -7.3970e-01 -1.0464e+00  5.4800e-02 -6.6270e-01\n",
       "     -2.2696e+00 -9.0830e-01 -1.1317e+00 -3.4855e+00 -1.5822e+00\n",
       "      6.0000e-01 -1.7445e+00  7.6000e-03 -1.2245e+00  1.0287e+00]\n",
       "    [-1.2370e-01 -1.4980e-01 -1.3146e+00  8.8070e-01  5.6530e-01\n",
       "     -9.7100e-02 -2.6850e-01 -2.2650e-01  7.2420e-01  2.5000e-03\n",
       "      1.5025e+00 -4.4910e-01 -2.4000e-03  7.4280e-01  4.7950e-01\n",
       "     -7.3200e-02 -1.6539e+00 -1.6140e-01  2.1580e-01  9.6570e-01\n",
       "      1.4980e-01 -1.3661e+00  4.3530e-01  1.5165e+00  8.8100e-02\n",
       "      2.4890e-01  4.9330e-01 -2.3476e+00  6.6350e-01 -2.6860e-01]]\n",
       "  \n",
       "   [[ 7.6600e-01  1.2944e+00 -2.4330e-01  2.5610e-01  1.8751e+00\n",
       "      6.4280e-01  1.9780e-01 -7.4290e-01  1.1549e+00  4.3310e-01\n",
       "     -2.1390e-01 -2.4660e-01  3.5460e-01  1.1660e-01 -1.8678e+00\n",
       "     -1.4323e+00  6.1550e-01  1.8100e-01  2.2700e-02 -1.5867e+00\n",
       "     -1.3959e+00 -1.6073e+00  3.8050e-01 -5.0200e-02 -5.2390e-01\n",
       "      1.0628e+00 -6.0500e-01  5.5020e-01 -2.8710e-01 -1.0069e+00]\n",
       "    [ 6.6400e-02 -4.8380e-01 -1.8870e-01  1.6417e+00 -1.6320e-01\n",
       "      1.2198e+00  6.9200e-02  7.2090e-01 -7.9440e-01  2.5397e+00\n",
       "      1.3034e+00 -4.3350e-01  8.6220e-01  1.1659e+00  1.8459e+00\n",
       "     -3.6100e-01 -1.0925e+00 -4.0660e-01  2.0613e+00  4.7580e-01\n",
       "     -1.1295e+00 -5.6260e-01  1.2751e+00  1.5768e+00 -1.8678e+00\n",
       "      3.3400e-02 -9.1780e-01 -9.8350e-01 -4.3880e-01 -4.1000e-02]]]))]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sum_input_val(x, 2) for x in np.arange(1.4, 1.9, 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7462781154631912"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(11.546474090837279 - 11.37184627929096) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Gradients are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-416.0378316897645,\n",
       "  Tensor([[[-0.4957 -1.2251  0.9884 ... -0.0773  1.113   0.1534]\n",
       "    [ 1.2619 -0.1656  0.0982 ... -0.4398  0.2011 -0.4361]\n",
       "    [ 0.2681  0.3808 -1.2532 ... -0.2891  0.5531 -1.1989]\n",
       "    ...\n",
       "    [-2.1346 -0.9519 -0.1124 ...  0.0515 -1.2297  0.2146]\n",
       "    [ 1.2736  0.7889 -1.5609 ...  0.4688  0.308  -0.1725]\n",
       "    [ 0.3723 -0.9119  1.0292 ...  0.8877  0.2504 -0.2204]]\n",
       "  \n",
       "   [[ 0.6355 -0.007   1.5405 ... -0.3527 -0.1827  0.8867]\n",
       "    [-1.8614 -0.4224 -0.2571 ... -0.8535 -0.0968  0.9494]\n",
       "    [ 0.4398 -2.0366  1.0579 ...  0.8555 -0.29   -0.1914]\n",
       "    ...\n",
       "    [-0.1639  0.4193 -0.9304 ...  0.3736 -0.0844 -0.8258]\n",
       "    [ 1.907  -0.4342 -2.1665 ...  0.0076 -1.2245  1.0287]\n",
       "    [-0.1237 -0.1498 -1.3146 ... -2.3476  0.6635 -0.2686]]\n",
       "  \n",
       "   [[ 0.766   1.2944 -0.2433 ...  0.5502 -0.2871 -1.0069]\n",
       "    [ 0.0664 -0.4838 -0.1887 ... -0.9835 -0.4388 -0.041 ]\n",
       "    [ 2.2724 -0.1125  1.1493 ... -0.4063  1.4525 -0.0086]\n",
       "    ...\n",
       "    [ 2.0698  0.2094 -0.8956 ...  0.178  -0.2295  1.0218]\n",
       "    [ 1.2349  1.5619 -0.6159 ...  0.3288 -0.9915 -1.9259]\n",
       "    [-1.1922 -2.738   0.8921 ...  1.712  -0.8447 -0.6527]]\n",
       "  \n",
       "   ...\n",
       "  \n",
       "   [[-2.1072 -1.6568 -1.426  ... -1.0226  0.0194 -0.5191]\n",
       "    [-1.4977  0.1357  0.6554 ... -1.4219  1.5037  2.1611]\n",
       "    [-1.0692 -0.0527  0.1153 ... -0.3646 -1.2814 -1.0744]\n",
       "    ...\n",
       "    [ 0.2541  1.8191  0.2385 ... -1.2623 -0.8764 -0.7234]\n",
       "    [-0.1519 -1.6538 -2.6766 ...  0.7645  0.7993  1.9762]\n",
       "    [-1.6461 -0.154   0.0868 ... -0.3635 -0.2723 -0.7195]]\n",
       "  \n",
       "   [[ 0.1018  0.1027 -0.6239 ... -0.8581  0.0309  1.0657]\n",
       "    [-0.7503 -0.1999  0.6317 ...  0.0221  0.2424  0.0327]\n",
       "    [-0.5034  0.0227  0.127  ... -0.5855 -0.5453  0.0236]\n",
       "    ...\n",
       "    [ 0.6664 -0.2885  0.8406 ... -1.1517  1.3523  0.2509]\n",
       "    [ 0.9482 -0.4938  1.5693 ...  2.4418  1.487   0.2061]\n",
       "    [ 1.3931 -0.2557  1.3478 ... -1.1739 -2.0125 -0.0691]]\n",
       "  \n",
       "   [[-0.85    1.1558  0.5126 ...  2.0508  0.2903 -0.4431]\n",
       "    [-0.8631  0.5512  2.1175 ...  0.0687 -1.9906 -0.9638]\n",
       "    [-0.6273 -1.3715  0.5618 ...  0.1889  1.4613 -1.1799]\n",
       "    ...\n",
       "    [ 0.6244  0.2491  1.3714 ...  0.4692  0.7011 -0.6779]\n",
       "    [-0.0815 -1.3651  1.6821 ...  2.1561 -0.339   0.2521]\n",
       "    [-0.9982 -0.0261  0.548  ...  1.058  -0.245   2.1332]]])),\n",
       " (-416.6620200709412,\n",
       "  Tensor([[[-0.4957 -1.2251  0.9884 ... -0.0773  1.113   0.1534]\n",
       "    [ 1.2619 -0.1656  0.0982 ... -0.4398  0.2011 -0.4361]\n",
       "    [ 0.2681  0.3808 -1.2532 ... -0.2891  0.5531 -1.1989]\n",
       "    ...\n",
       "    [-2.1346 -0.9519 -0.1124 ...  0.0515 -1.2297  0.2146]\n",
       "    [ 1.2736  0.7889 -1.5609 ...  0.4688  0.308  -0.1725]\n",
       "    [ 0.3723 -0.9119  1.0292 ...  0.8877  0.2504 -0.2204]]\n",
       "  \n",
       "   [[ 0.6355 -0.007   1.5405 ... -0.3527 -0.1827  0.8867]\n",
       "    [-1.8614 -0.4224 -0.2571 ... -0.8535 -0.0968  0.9494]\n",
       "    [ 0.4398 -2.0366  1.0579 ...  0.8555 -0.29   -0.1914]\n",
       "    ...\n",
       "    [-0.1639  0.4193 -0.9304 ...  0.3736 -0.0844 -0.8258]\n",
       "    [ 1.907  -0.4342 -2.1665 ...  0.0076 -1.2245  1.0287]\n",
       "    [-0.1237 -0.1498 -1.3146 ... -2.3476  0.6635 -0.2686]]\n",
       "  \n",
       "   [[ 0.766   1.2944 -0.2433 ...  0.5502 -0.2871 -1.0069]\n",
       "    [ 0.0664 -0.4838 -0.1887 ... -0.9835 -0.4388 -0.041 ]\n",
       "    [ 2.2724 -0.1125  1.1493 ... -0.4063  1.4525 -0.0086]\n",
       "    ...\n",
       "    [ 2.0698  0.2094 -0.8956 ...  0.178  -0.2295  1.0218]\n",
       "    [ 1.2349  1.5619 -0.6159 ...  0.3288 -0.9915 -1.9259]\n",
       "    [-1.1922 -2.738   0.8921 ...  1.712  -0.8447 -0.6527]]\n",
       "  \n",
       "   ...\n",
       "  \n",
       "   [[-2.1072 -1.6568 -1.426  ... -1.0226  0.0194 -0.5191]\n",
       "    [-1.4977  0.1357  0.6554 ... -1.4219  1.5037  2.1611]\n",
       "    [-1.0692 -0.0527  0.1153 ... -0.3646 -1.2814 -1.0744]\n",
       "    ...\n",
       "    [ 0.2541  1.8191  0.2385 ... -1.2623 -0.8764 -0.7234]\n",
       "    [-0.1519 -1.6538 -2.6766 ...  0.7645  0.7993  1.9762]\n",
       "    [-1.6461 -0.154   0.0868 ... -0.3635 -0.2723 -0.7195]]\n",
       "  \n",
       "   [[ 0.1018  0.1027 -0.6239 ... -0.8581  0.0309  1.0657]\n",
       "    [-0.7503 -0.1999  0.6317 ...  0.0221  0.2424  0.0327]\n",
       "    [-0.5034  0.0227  0.127  ... -0.5855 -0.5453  0.0236]\n",
       "    ...\n",
       "    [ 0.6664 -0.2885  0.8406 ... -1.1517  1.3523  0.2509]\n",
       "    [ 0.9482 -0.4938  1.5693 ...  2.4418  1.487   0.2061]\n",
       "    [ 1.3931 -0.2557  1.3478 ... -1.1739 -2.0125 -0.0691]]\n",
       "  \n",
       "   [[-0.85    1.1558  0.5126 ...  2.0508  0.2903 -0.4431]\n",
       "    [-0.8631  0.5512  2.1175 ...  0.0687 -1.9906 -0.9638]\n",
       "    [-0.6273 -1.3715  0.5618 ...  0.1889  1.4613 -1.1799]\n",
       "    ...\n",
       "    [ 0.6244  0.2491  1.3714 ...  0.4692  0.7011 -0.6779]\n",
       "    [-0.0815 -1.3651  1.6821 ...  2.1561 -0.339   0.2521]\n",
       "    [-0.9982 -0.0261  0.548  ...  1.058  -0.245   2.1332]]])),\n",
       " (-417.2892446722648,\n",
       "  Tensor([[[-0.4957 -1.2251  0.9884 ... -0.0773  1.113   0.1534]\n",
       "    [ 1.2619 -0.1656  0.0982 ... -0.4398  0.2011 -0.4361]\n",
       "    [ 0.2681  0.3808 -1.2532 ... -0.2891  0.5531 -1.1989]\n",
       "    ...\n",
       "    [-2.1346 -0.9519 -0.1124 ...  0.0515 -1.2297  0.2146]\n",
       "    [ 1.2736  0.7889 -1.5609 ...  0.4688  0.308  -0.1725]\n",
       "    [ 0.3723 -0.9119  1.0292 ...  0.8877  0.2504 -0.2204]]\n",
       "  \n",
       "   [[ 0.6355 -0.007   1.5405 ... -0.3527 -0.1827  0.8867]\n",
       "    [-1.8614 -0.4224 -0.2571 ... -0.8535 -0.0968  0.9494]\n",
       "    [ 0.4398 -2.0366  1.0579 ...  0.8555 -0.29   -0.1914]\n",
       "    ...\n",
       "    [-0.1639  0.4193 -0.9304 ...  0.3736 -0.0844 -0.8258]\n",
       "    [ 1.907  -0.4342 -2.1665 ...  0.0076 -1.2245  1.0287]\n",
       "    [-0.1237 -0.1498 -1.3146 ... -2.3476  0.6635 -0.2686]]\n",
       "  \n",
       "   [[ 0.766   1.2944 -0.2433 ...  0.5502 -0.2871 -1.0069]\n",
       "    [ 0.0664 -0.4838 -0.1887 ... -0.9835 -0.4388 -0.041 ]\n",
       "    [ 2.2724 -0.1125  1.1493 ... -0.4063  1.4525 -0.0086]\n",
       "    ...\n",
       "    [ 2.0698  0.2094 -0.8956 ...  0.178  -0.2295  1.0218]\n",
       "    [ 1.2349  1.5619 -0.6159 ...  0.3288 -0.9915 -1.9259]\n",
       "    [-1.1922 -2.738   0.8921 ...  1.712  -0.8447 -0.6527]]\n",
       "  \n",
       "   ...\n",
       "  \n",
       "   [[-2.1072 -1.6568 -1.426  ... -1.0226  0.0194 -0.5191]\n",
       "    [-1.4977  0.1357  0.6554 ... -1.4219  1.5037  2.1611]\n",
       "    [-1.0692 -0.0527  0.1153 ... -0.3646 -1.2814 -1.0744]\n",
       "    ...\n",
       "    [ 0.2541  1.8191  0.2385 ... -1.2623 -0.8764 -0.7234]\n",
       "    [-0.1519 -1.6538 -2.6766 ...  0.7645  0.7993  1.9762]\n",
       "    [-1.6461 -0.154   0.0868 ... -0.3635 -0.2723 -0.7195]]\n",
       "  \n",
       "   [[ 0.1018  0.1027 -0.6239 ... -0.8581  0.0309  1.0657]\n",
       "    [-0.7503 -0.1999  0.6317 ...  0.0221  0.2424  0.0327]\n",
       "    [-0.5034  0.0227  0.127  ... -0.5855 -0.5453  0.0236]\n",
       "    ...\n",
       "    [ 0.6664 -0.2885  0.8406 ... -1.1517  1.3523  0.2509]\n",
       "    [ 0.9482 -0.4938  1.5693 ...  2.4418  1.487   0.2061]\n",
       "    [ 1.3931 -0.2557  1.3478 ... -1.1739 -2.0125 -0.0691]]\n",
       "  \n",
       "   [[-0.85    1.1558  0.5126 ...  2.0508  0.2903 -0.4431]\n",
       "    [-0.8631  0.5512  2.1175 ...  0.0687 -1.9906 -0.9638]\n",
       "    [-0.6273 -1.3715  0.5618 ...  0.1889  1.4613 -1.1799]\n",
       "    ...\n",
       "    [ 0.6244  0.2491  1.3714 ...  0.4692  0.7011 -0.6779]\n",
       "    [-0.0815 -1.3651  1.6821 ...  2.1561 -0.339   0.2521]\n",
       "    [-0.9982 -0.0261  0.548  ...  1.058  -0.245   2.1332]]])),\n",
       " (-417.923790705221,\n",
       "  Tensor([[[-0.4957 -1.2251  0.9884 ... -0.0773  1.113   0.1534]\n",
       "    [ 1.2619 -0.1656  0.0982 ... -0.4398  0.2011 -0.4361]\n",
       "    [ 0.2681  0.3808 -1.2532 ... -0.2891  0.5531 -1.1989]\n",
       "    ...\n",
       "    [-2.1346 -0.9519 -0.1124 ...  0.0515 -1.2297  0.2146]\n",
       "    [ 1.2736  0.7889 -1.5609 ...  0.4688  0.308  -0.1725]\n",
       "    [ 0.3723 -0.9119  1.0292 ...  0.8877  0.2504 -0.2204]]\n",
       "  \n",
       "   [[ 0.6355 -0.007   1.5405 ... -0.3527 -0.1827  0.8867]\n",
       "    [-1.8614 -0.4224 -0.2571 ... -0.8535 -0.0968  0.9494]\n",
       "    [ 0.4398 -2.0366  1.0579 ...  0.8555 -0.29   -0.1914]\n",
       "    ...\n",
       "    [-0.1639  0.4193 -0.9304 ...  0.3736 -0.0844 -0.8258]\n",
       "    [ 1.907  -0.4342 -2.1665 ...  0.0076 -1.2245  1.0287]\n",
       "    [-0.1237 -0.1498 -1.3146 ... -2.3476  0.6635 -0.2686]]\n",
       "  \n",
       "   [[ 0.766   1.2944 -0.2433 ...  0.5502 -0.2871 -1.0069]\n",
       "    [ 0.0664 -0.4838 -0.1887 ... -0.9835 -0.4388 -0.041 ]\n",
       "    [ 2.2724 -0.1125  1.1493 ... -0.4063  1.4525 -0.0086]\n",
       "    ...\n",
       "    [ 2.0698  0.2094 -0.8956 ...  0.178  -0.2295  1.0218]\n",
       "    [ 1.2349  1.5619 -0.6159 ...  0.3288 -0.9915 -1.9259]\n",
       "    [-1.1922 -2.738   0.8921 ...  1.712  -0.8447 -0.6527]]\n",
       "  \n",
       "   ...\n",
       "  \n",
       "   [[-2.1072 -1.6568 -1.426  ... -1.0226  0.0194 -0.5191]\n",
       "    [-1.4977  0.1357  0.6554 ... -1.4219  1.5037  2.1611]\n",
       "    [-1.0692 -0.0527  0.1153 ... -0.3646 -1.2814 -1.0744]\n",
       "    ...\n",
       "    [ 0.2541  1.8191  0.2385 ... -1.2623 -0.8764 -0.7234]\n",
       "    [-0.1519 -1.6538 -2.6766 ...  0.7645  0.7993  1.9762]\n",
       "    [-1.6461 -0.154   0.0868 ... -0.3635 -0.2723 -0.7195]]\n",
       "  \n",
       "   [[ 0.1018  0.1027 -0.6239 ... -0.8581  0.0309  1.0657]\n",
       "    [-0.7503 -0.1999  0.6317 ...  0.0221  0.2424  0.0327]\n",
       "    [-0.5034  0.0227  0.127  ... -0.5855 -0.5453  0.0236]\n",
       "    ...\n",
       "    [ 0.6664 -0.2885  0.8406 ... -1.1517  1.3523  0.2509]\n",
       "    [ 0.9482 -0.4938  1.5693 ...  2.4418  1.487   0.2061]\n",
       "    [ 1.3931 -0.2557  1.3478 ... -1.1739 -2.0125 -0.0691]]\n",
       "  \n",
       "   [[-0.85    1.1558  0.5126 ...  2.0508  0.2903 -0.4431]\n",
       "    [-0.8631  0.5512  2.1175 ...  0.0687 -1.9906 -0.9638]\n",
       "    [-0.6273 -1.3715  0.5618 ...  0.1889  1.4613 -1.1799]\n",
       "    ...\n",
       "    [ 0.6244  0.2491  1.3714 ...  0.4692  0.7011 -0.6779]\n",
       "    [-0.0815 -1.3651  1.6821 ...  2.1561 -0.339   0.2521]\n",
       "    [-0.9982 -0.0261  0.548  ...  1.058  -0.245   2.1332]]]))]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sum_input_val(x, 11) for x in np.arange(-1.2, -0.8, 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6548500265074608"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(188.85741144545887 - 188.79192644280812) * 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
