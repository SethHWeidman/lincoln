{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key thing I'll work towards here is a `Model` that has a dictionary of `params` and a `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import List, NamedTuple, Callable, Optional, Union, Iterator, Dict\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arrayable = Union[float, list, np.ndarray]\n",
    "\n",
    "Tensorable = Union['Tensor', float, np.ndarray]\n",
    "\n",
    "def ensure_tensor(tensorable: Tensorable) -> 'Tensor':\n",
    "    if isinstance(tensorable, Tensor):\n",
    "        return tensorable\n",
    "    else:\n",
    "        return Tensor(tensorable)\n",
    "    \n",
    "def ensure_array(arrayable: Arrayable) -> np.ndarray:\n",
    "    if isinstance(arrayable, np.ndarray):\n",
    "        return arrayable\n",
    "    else:\n",
    "        return np.array(arrayable)\n",
    "\n",
    "class Dependency(NamedTuple):\n",
    "    tensor: 'Tensor'\n",
    "    grad_fn: Callable[[np.ndarray], np.ndarray]\n",
    "        \n",
    "def collapse_sum(grad: np.ndarray,\n",
    "                 t: 'Tensor') -> np.ndarray:\n",
    "\n",
    "    # Sum out added dims\n",
    "    ndims_added = grad.ndim - t.data.ndim\n",
    "    for _ in range(ndims_added):\n",
    "        grad = grad.sum(axis=0)\n",
    "\n",
    "    # Sum across broadcasted (but non-added dims)\n",
    "    for i, dim in enumerate(t.shape):\n",
    "        if dim == 1:\n",
    "            grad = grad.sum(axis=i, keepdims=True)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data: np.ndarray,\n",
    "                 depends_on: List[Dependency] = None,\n",
    "                 no_grad: bool = False) -> None:\n",
    "        self.data = ensure_array(data)\n",
    "        self.depends_on = depends_on or []\n",
    "        self.no_grad = no_grad\n",
    "        self.shape = self.data.shape\n",
    "        self.grad: Optional['Tensor'] = None\n",
    "        if not self.no_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({np.round(self.data, 4)})\"\n",
    "            \n",
    "    def __add__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"gets called if I do t + other\"\"\"\n",
    "        return _add(self, ensure_tensor(other))\n",
    "\n",
    "    def __radd__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"gets called if I do other + t\"\"\"\n",
    "        return _add(ensure_tensor(other), self)\n",
    "\n",
    "    def __iadd__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"when we do t += other\"\"\"\n",
    "        self.data = self.data + ensure_tensor(other).data\n",
    "        return self    \n",
    "\n",
    "    def __isub__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"when we do t -= other\"\"\"\n",
    "        self.data = self.data - ensure_tensor(other).data\n",
    "        return self\n",
    "    \n",
    "    def __imul__(self, other: Tensorable) -> 'Tensor':\n",
    "        \"\"\"when we do t *= other\"\"\"\n",
    "        self.data = self.data * ensure_tensor(other).data\n",
    "        return self\n",
    "\n",
    "    def __mul__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _mul(self, ensure_tensor(other))\n",
    "\n",
    "    def __rmul__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _mul(ensure_tensor(other), self)\n",
    "\n",
    "    def __matmul__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _matmul(self, other)\n",
    "\n",
    "    def __neg__(self) -> 'Tensor':\n",
    "        return _neg(self)\n",
    "\n",
    "    def __sub__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _sub(self, ensure_tensor(other))\n",
    "\n",
    "    def __rsub__(self, other: Tensorable) -> 'Tensor':\n",
    "        return _sub(ensure_tensor(other), self)\n",
    "    \n",
    "    def __getitem__(self, idxs) -> 'Tensor':\n",
    "        return _slice(self, idxs)\n",
    "    \n",
    "    def concat(self, other: Tensorable) -> 'Tensor':\n",
    "        return _concat(self, ensure_tensor(other))\n",
    "    \n",
    "    def repeat(self, repeats: int) -> 'Tensor':\n",
    "        return _repeat(self, repeats)\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        self.grad = Tensor(np.zeros_like(self.data, dtype=np.float64),\n",
    "                           no_grad = True)\n",
    "    \n",
    "    def sum(self) -> 'Tensor':\n",
    "        return tensor_sum(self)\n",
    "    \n",
    "    def backward(self, grad: 'Tensor' = None) -> None:\n",
    "        # backward mostly going to be called on the loss after calling \"sum\"\n",
    "        if self.no_grad:\n",
    "            return\n",
    "        \n",
    "        if self.shape == ():\n",
    "            grad = Tensor(np.array(1.0))\n",
    "\n",
    "        \n",
    "        self.grad.data = self.grad.data + grad.data\n",
    "\n",
    "        for dependency in self.depends_on:\n",
    "            backward_grad = dependency.grad_fn(grad.data)\n",
    "            dependency.tensor.backward(Tensor(backward_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "\n",
    "    def _forward(t1: Tensor, t2: Tensor) -> np.ndarray:\n",
    "        return t1.data + t2.data\n",
    "\n",
    "    def t1_grad(grad: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        grad = collapse_sum(grad, t1)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def t2_grad(grad: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        grad = collapse_sum(grad, t2)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    data = _forward(t1, t2)\n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def _mul(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "\n",
    "    def _forward(t1: Tensor, t2: Tensor) -> np.ndarray:\n",
    "        return t1.data * t2.data\n",
    "\n",
    "    def t1_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        grad = grad * t2.data\n",
    "        grad = collapse_sum(grad, t1)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def t2_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        grad = grad * t1.data\n",
    "        grad = collapse_sum(grad, t2)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    data = _forward(t1, t2)\n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def _matmul(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "\n",
    "    assert t1.shape[1] == t2.shape[0]\n",
    "\n",
    "    def _forward(t1: Tensor, t2: Tensor) -> np.ndarray:\n",
    "        return t1.data @ t2.data\n",
    "\n",
    "    def t1_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        grad = grad @ t2.data.T\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def t2_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        grad = t1.data.T @ grad\n",
    "\n",
    "        return grad\n",
    "\n",
    "    data = _forward(t1, t2)\n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def _neg(t: Tensor) -> Tensor:\n",
    "\n",
    "    def _forward(t: Tensor) -> np.ndarray:\n",
    "        return -t.data\n",
    "\n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return -grad\n",
    "\n",
    "    data = _forward(t)\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad),\n",
    "    ]\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def _sub(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "    return t1 + -t2\n",
    "\n",
    "def _slice(t: Tensor, idxs: slice) -> Tensor:\n",
    "\n",
    "    def _forward(t: Tensor):\n",
    "        return t.data[idxs]\n",
    "\n",
    "    data = _forward(t)    \n",
    "    \n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        bigger_grad = np.zeros_like(data)\n",
    "        bigger_grad[idxs] = grad\n",
    "        return bigger_grad\n",
    "\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad),\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on)\n",
    "\n",
    "def tensor_sum(t: Tensor) -> Tensor:\n",
    "\n",
    "    def _forward(t: Tensor):\n",
    "        return t.data.sum()\n",
    "\n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad * np.ones_like(t.data)\n",
    "\n",
    "    data = _forward(t)\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad),\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on)        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _concat(t1: Tensor, t2: Tensor) -> Tensor:\n",
    "    \n",
    "    assert t1.shape[0] == t2.shape[0],\\\n",
    "    \"Concatenated Tensors must have the same shape along first dimension\"\n",
    "    \n",
    "    def _forward(t1: Tensor, t2: Tensor):\n",
    "        return np.concatenate([t1.data, t2.data], axis=1)    \n",
    "    \n",
    "    def t1_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad[:,:t1.shape[1]]\n",
    "    \n",
    "    def t2_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad[:,t1.shape[1]:]\n",
    "\n",
    "    data = _forward(t1, t2)\n",
    "    \n",
    "    depends_on = [\n",
    "        Dependency(t1, t1_grad),\n",
    "        Dependency(t2, t2_grad)\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _repeat(t: Tensor, repeats: int) -> Tensor:\n",
    "\n",
    "    assert t.shape[0] == 1,\\\n",
    "    \"Repeat operation should only be used on rows\"\n",
    "    \n",
    "    def _forward(t: Tensor, repeats: int) -> np.ndarray:\n",
    "        return np.repeat(t.data, repeats, axis=0)    \n",
    "    \n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad.sum(axis=0)\n",
    "\n",
    "    data = _forward(t, repeats)\n",
    "    \n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad)\n",
    "    ]\n",
    "\n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `concat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing concat forward\n",
    "\n",
    "a = np.random.randn(2, 3)\n",
    "b = np.random.randn(2, 5)\n",
    "\n",
    "c = np.concatenate([a, b], axis=1)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat grad calculations\n",
    "a_grad = c[:,:a.shape[1]]\n",
    "b_grad = c[:,a.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 5)\n"
     ]
    }
   ],
   "source": [
    "print(a_grad.shape)\n",
    "print(b_grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing concat forward\n",
    "\n",
    "aT = Tensor(a)\n",
    "bT = Tensor(b)\n",
    "c = aT.concat(bT)\n",
    "\n",
    "d = c.sum()\n",
    "\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[1. 1. 1.]\n",
      " [1. 1. 1.]])\n",
      "Tensor([[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(aT.grad)\n",
    "print(bT.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `repeat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8046, -1.7214,  0.6557],\n",
       "       [ 0.8046, -1.7214,  0.6557]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing repeat forward\n",
    "\n",
    "a = np.random.randn(1, 3)\n",
    "\n",
    "np.repeat(a, 2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.1888, -2.8004,  1.0108])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat backward\n",
    "\n",
    "a = np.random.randn(1, 3)\n",
    "\n",
    "a2 = np.repeat(a, 2, axis=0)\n",
    "\n",
    "a2.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat with tensors\n",
    "\n",
    "a = np.random.randn(1, 3)\n",
    "b = np.random.randn(3, 3)\n",
    "\n",
    "aT = Tensor(a)\n",
    "bT = Tensor(b)\n",
    "a2 = aT.repeat(3)\n",
    "c = (a2 + bT).sum()\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[3. 3. 3.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aT.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Parameter(Tensor):\n",
    "    def __init__(self, *shape) -> None:\n",
    "        data = np.random.randn(*shape)\n",
    "        super().__init__(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def parameters(self) -> Iterator[Parameter]:\n",
    "        for name, value in inspect.getmembers(self):\n",
    "            if isinstance(value, Parameter):\n",
    "                yield value\n",
    "            elif isinstance(value, Model):\n",
    "                yield from value.parameters()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for parameter in self.parameters():\n",
    "            parameter.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr: float = 0.01) -> None:\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, model: Model) -> None:\n",
    "        for parameter in model.parameters():\n",
    "            parameter -= parameter.grad * self.lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(t: Tensor) -> Tensor:\n",
    "    def _forward(t: Tensor):\n",
    "        return np.tanh(t.data)\n",
    "\n",
    "    data = _forward(t)\n",
    "    \n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad * (1 - data * data)\n",
    "\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad)\n",
    "    ]\n",
    "    \n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t: Tensor) -> Tensor:\n",
    "    \n",
    "    def _forward(t: Tensor):\n",
    "        return 1.0 / (1.0 + np.exp(-t.data))\n",
    "\n",
    "    data = _forward(t)\n",
    "    \n",
    "    def t_grad(grad: np.ndarray) -> np.ndarray:\n",
    "        return grad * data * (1.0 - data)\n",
    "\n",
    "    depends_on = [\n",
    "        Dependency(t, t_grad)\n",
    "    ]\n",
    "    \n",
    "    return Tensor(data, depends_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "y_train, y_test = y_train.reshape((-1,1)), y_test.reshape((-1,1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = (Tensor(X_train, no_grad=True),\n",
    "                                    Tensor(X_test, no_grad=True),\n",
    "                                    Tensor(y_train, no_grad=True),\n",
    "                                    Tensor(y_test, no_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonModel(Model):\n",
    "    def __init__(self, \n",
    "                 num_hidden: int = 13,\n",
    "                 seed: int = 1) -> None:\n",
    "        np.random.seed(seed)\n",
    "        self.w1 = Parameter(13, num_hidden)\n",
    "        self.b1 = Parameter(num_hidden)\n",
    "\n",
    "        self.w2 = Parameter(num_hidden, 1)\n",
    "        self.b2 = Parameter(1)\n",
    "\n",
    "    def predict(self, inputs: Tensor) -> Tensor:\n",
    "        # inputs will be (batch_size, 13)\n",
    "        x1 = inputs @ self.w1 + self.b1  # (batch_size, num_hidden)\n",
    "        x2 = tanh(x1)                    # (batch_size, num_hidden)\n",
    "        x3 = x2 @ self.w2 + self.b2      # (batch_size, 4)\n",
    "\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Tensor(10929.9298)\n",
      "1 Tensor(5931.3808)\n",
      "2 Tensor(5032.0439)\n",
      "3 Tensor(4765.6974)\n",
      "4 Tensor(4596.3881)\n",
      "5 Tensor(4585.5644)\n",
      "6 Tensor(4554.1706)\n",
      "7 Tensor(4486.7719)\n",
      "8 Tensor(4400.3648)\n",
      "9 Tensor(4251.6124)\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(lr=0.001)\n",
    "batch_size = 32\n",
    "model = BostonModel(seed=112418)\n",
    "train_size = X_train.shape[0]\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for start in range(0, train_size, batch_size):\n",
    "        end = start + batch_size\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        inputs = X_train[start:end]\n",
    "        inputs.no_grad = True\n",
    "\n",
    "        predicted = model.predict(inputs)\n",
    "        actual = y_train[start:end]\n",
    "        actual.no_grad = True\n",
    "\n",
    "        errors = predicted - actual\n",
    "        loss = (errors * errors).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step(model)\n",
    "        \n",
    "        \n",
    "        # test predictions\n",
    "    predicted = model.predict(X_test)\n",
    "    errors = predicted - y_test\n",
    "    loss = (errors * errors).sum()\n",
    "        \n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Tensor Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple forward with addition\n",
    "\n",
    "```\n",
    "a \n",
    " \\\n",
    "  c - s\n",
    " /\n",
    "b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0. 0.]\n",
      " [0. 0.]])\n",
      "Tensor([[1. 1.]\n",
      " [1. 1.]])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112518)\n",
    "a = Tensor(np.random.randn(2,2))\n",
    "b = Tensor(np.random.randn(2,2))\n",
    "c = a + b\n",
    "s = c.sum()\n",
    "\n",
    "print(a.grad)\n",
    "s.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple forward with branching\n",
    "\n",
    "```\n",
    "a \n",
    " \\\n",
    "  c1 \n",
    " /  \\\n",
    "b\n",
    "      s\n",
    "a \n",
    " \\   /\n",
    "  c2 \n",
    " /\n",
    "b \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0. 0.]\n",
      " [0. 0.]])\n",
      "Tensor([[0.6462 2.4838]\n",
      " [1.0027 1.1719]])\n",
      "\n",
      "a: Tensor([[ 2.1617 -0.4313]\n",
      " [ 0.3818  0.7105]])\n",
      "s: Tensor(2.8455)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112518)\n",
    "a = Tensor(np.random.randn(2,2))\n",
    "b = Tensor(np.random.randn(2,2))\n",
    "c1 = a + b\n",
    "c2 = a * b\n",
    "s = (c1 + c2).sum()\n",
    "\n",
    "print(a.grad)\n",
    "s.backward()\n",
    "print(a.grad)\n",
    "print()\n",
    "print(\"a:\", a)\n",
    "print(\"s:\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that increasing `a[0][0]` by 0.1 from `2.1617` to `2.2617` would increase `s` from `2.8455` to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.91012"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.8455 + 0.1 * 0.6462"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sum(a_first_val):\n",
    "    \n",
    "    np.random.seed(112518)\n",
    "    a = np.random.randn(2,2)\n",
    "    b = np.random.randn(2,2)\n",
    "    a2 = a.copy()\n",
    "    a2[0][0] = a_first_val\n",
    "    c1 = a2 + b\n",
    "    c2 = a2 * b\n",
    "    return (c1 + c2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8454767470862894\n",
      "2.910093928197016\n"
     ]
    }
   ],
   "source": [
    "print(check_sum(2.1617))\n",
    "print(check_sum(2.2617))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the same array multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: Tensor([[-0.0683  0.6959]\n",
      " [ 1.4791  0.5237]])\n",
      "s: Tensor(-1.2067)\n",
      "w.grad: Tensor([[-0.5692 -0.3029]\n",
      " [-0.0847 -1.7367]])\n"
     ]
    }
   ],
   "source": [
    "w = Tensor(np.random.randn(2,2))\n",
    "a = Tensor(np.random.randn(2,2))\n",
    "b = Tensor(np.random.randn(2,2))\n",
    "c = w + a\n",
    "s = (w * b).sum()\n",
    "\n",
    "s.backward()\n",
    "\n",
    "print(\"w:\", w)\n",
    "print(\"s:\", s)\n",
    "print(\"w.grad:\", w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031700000000000006"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.0683 + 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that increasing `w[0][0]` by 0.1 from `-0.0683` to `0.0317` would increase `s` from `-1.2067` to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2636200000000002"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1.2067 + 0.1 * -0.5692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sum(w_first_val):\n",
    "    \n",
    "    w2 = w.data.copy()\n",
    "    w2[0][0] = w_first_val\n",
    "    c.data = w2 + a.data\n",
    "    return (w2 * b.data).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.206689062693263\n",
      "-1.263613361478857\n"
     ]
    }
   ],
   "source": [
    "print(check_sum(-0.0683))\n",
    "print(check_sum(0.0317))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding bias multiple times\n",
    "\n",
    "Will mimic adding hidden state multiple times in an LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultimate goal:\n",
    "\n",
    "```python\n",
    "def lstm_node(inputs: Tensor, \n",
    "              hiddens: Tensor, \n",
    "              cells: Tensor, \n",
    "              params: Dict[str, Tensor]):\n",
    "    \n",
    "    assert input.shape[0] == hidden.shape[0] == cell.shape[0]\n",
    "    \n",
    "    Z = inputs.concat(hidden)\n",
    "    \n",
    "    forget = sigmoid(Z @ params['Wf'] + params['Bf'])\n",
    "\n",
    "    ingate = sigmoid(Z @ params['Wi'] + params['Bi'])\n",
    "    \n",
    "    outgate = sigmoid(Z @ params['Wo'] + params['Bo'])\n",
    "    \n",
    "    change = tanh(Z @ params['Wc'] + params['Bc'])\n",
    "    \n",
    "    cells = cells * forget + ingate * change\n",
    "    \n",
    "    hiddens = outgate * tanh(cells)\n",
    "    \n",
    "    outputs = hidden @ params['Wv'] + params['Bv']\n",
    "    \n",
    "    return outputs, hiddens, cells\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(batch_size: int,\n",
    "                state_size: int,\n",
    "                vocab_size: int) -> Dict[str, Parameter]:\n",
    "    \n",
    "    params = {}\n",
    "    params['Wf'] = Parameter(state_size + vocab_size, state_size)\n",
    "    params['Wi'] = Parameter(state_size + vocab_size, state_size)\n",
    "    params['Wo'] = Parameter(state_size + vocab_size, state_size)\n",
    "    params['Wc'] = Parameter(state_size + vocab_size, state_size)\n",
    "    params['Wv'] = Parameter(state_size, vocab_size)\n",
    "    \n",
    "    params['Bf'] = Parameter(state_size)\n",
    "    params['Bi'] = Parameter(state_size)\n",
    "    params['Bo'] = Parameter(state_size)\n",
    "    params['Bc'] = Parameter(state_size)\n",
    "    params['Bv'] = Parameter(vocab_size)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "batch_size = 12\n",
    "state_size = 20\n",
    "vocab_size = 30\n",
    "\n",
    "np.random.seed(112518)\n",
    "\n",
    "# initial data\n",
    "h_init = Tensor(np.random.randn(1, state_size))\n",
    "c_init = Tensor(np.random.randn(1, state_size))\n",
    "inputs = Tensor(np.random.randn(batch_size, vocab_size))\n",
    "\n",
    "# initialize params\n",
    "params = init_params(batch_size, state_size, vocab_size)\n",
    "\n",
    "# repeat cell and hidden\n",
    "hiddens = h_init.repeat(batch_size)\n",
    "cells = c_init.repeat(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_node(inputs: Tensor, \n",
    "              hiddens: Tensor, \n",
    "              cells: Tensor, \n",
    "              params: Dict[str, Parameter]):\n",
    "\n",
    "    assert inputs.shape[0] == hiddens.shape[0] == cells.shape[0]\n",
    "\n",
    "    Z = inputs.concat(hiddens)\n",
    "\n",
    "    forget = sigmoid(Z @ params['Wf'] + params['Bf'])\n",
    "\n",
    "    ingate = sigmoid(Z @ params['Wi'] + params['Bi'])\n",
    "\n",
    "    outgate = sigmoid(Z @ params['Wo'] + params['Bo'])\n",
    "\n",
    "    change = tanh(Z @ params['Wc'] + params['Bc'])\n",
    "\n",
    "    cells = cells * forget + ingate * change\n",
    "\n",
    "    hiddens = outgate * tanh(cells)\n",
    "\n",
    "    outputs = hiddens @ params['Wv'] + params['Bv']\n",
    "\n",
    "    return outputs, hiddens, cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_out, h_out, c_out = lstm_node(inputs, hiddens, cells, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 30)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_out.shape # (batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = o_out.sum()\n",
    "s.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(70.9797)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[ 2.1617 -0.4313  0.3818  0.7105 -0.3538  1.4838  0.0027  0.1719 -0.0683\n",
      "   0.6959  1.4791  0.5237  1.0117  0.0821 -0.2638 -1.4929 -0.5692 -0.3029\n",
      "  -0.0847 -1.7367]])\n",
      "Tensor([[  5.8457  -3.3987   3.0234   9.0145   7.7679  -2.7426   3.7362 -25.35\n",
      "   -7.5784 -19.3007  17.2762   8.8289 -21.3072 -16.8012 -21.413   19.6036\n",
      "    1.4624  10.1015   4.6027   2.511 ]])\n"
     ]
    }
   ],
   "source": [
    "print(h_init)\n",
    "print(h_init.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that if I increase `h_init[0]` from `2.1617` to `2.2617`, `s` will change to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.56427"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "70.9797 + 0.1 * 5.8457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_node_np(inputs: np.ndarray, \n",
    "              hiddens: np.ndarray, \n",
    "              cells: np.ndarray, \n",
    "              params: Dict[str, np.ndarray]):\n",
    "\n",
    "    assert inputs.shape[0] == hiddens.shape[0] == cells.shape[0]\n",
    "\n",
    "    def _sigmoid(arr: np.ndarray) -> np.ndarray:\n",
    "        return 1.0 / (1.0 + np.exp(-arr))\n",
    "        \n",
    "    Z = np.concatenate([inputs, hiddens], axis=1)\n",
    "\n",
    "    forget = _sigmoid(Z @ params['Wf'].data + params['Bf'].data)\n",
    "\n",
    "    ingate = _sigmoid(Z @ params['Wi'].data + params['Bi'].data)\n",
    "\n",
    "    outgate = _sigmoid(Z @ params['Wo'].data + params['Bo'].data)\n",
    "\n",
    "    change = np.tanh(Z @ params['Wc'].data + params['Bc'].data)\n",
    "\n",
    "    cells = cells * forget + ingate * change\n",
    "\n",
    "    hiddens = outgate * np.tanh(cells)\n",
    "\n",
    "    outputs = hiddens @ params['Wv'].data + params['Bv'].data\n",
    "\n",
    "    return outputs, hiddens, cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.1617 -0.4313  0.3818  0.7105 -0.3538  1.4838  0.0027  0.1719 -0.0683\n",
      "   0.6959  1.4791  0.5237  1.0117  0.0821 -0.2638 -1.4929 -0.5692 -0.3029\n",
      "  -0.0847 -1.7367]]\n",
      "[[ 2.2617 -0.4313  0.3818  0.7105 -0.3538  1.4838  0.0027  0.1719 -0.0683\n",
      "   0.6959  1.4791  0.5237  1.0117  0.0821 -0.2638 -1.4929 -0.5692 -0.3029\n",
      "  -0.0847 -1.7367]]\n"
     ]
    }
   ],
   "source": [
    "h_init_np = h_init.data\n",
    "print(h_init_np)\n",
    "h_init_np[0][0] += 0.1\n",
    "print(h_init_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 20)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens = np.repeat(h_init_np, batch_size, axis=0)\n",
    "hiddens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_np, h_np, c_np = lstm_node_np(inputs.data, \n",
    "                                  hiddens,\n",
    "                                  cells.data,\n",
    "                                  params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 30)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.57204981645923"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_np.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
