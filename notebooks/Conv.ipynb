{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Lincoln to system path\n",
    "import sys\n",
    "sys.path.append(\"/Users/seth/development/lincoln/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "import typing\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Old losses and layers code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lincoln as lnc\n",
    "from lincoln.layers import Dense, Conv2D, Layer\n",
    "from lincoln.losses import LogLoss, LogSigmoidLoss, MeanSquaredError\n",
    "from lincoln.optimizers import SGD\n",
    "from lincoln.operations import ReLU, Softmax\n",
    "from lincoln.activations import Sigmoid, LogSigmoid\n",
    "from lincoln.network import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "mnist_trainset = MNIST(root=\"./explanatory/data\", train=True, download=False, transform=None)\n",
    "mnist_testset = MNIST(root=\"./explanatory/data\", train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = mnist_trainset\n",
    "num_labels = len(data.train_labels)\n",
    "train_labels = torch.zeros(num_labels, 10)\n",
    "for i in range(num_labels):\n",
    "    train_labels[i][data.train_labels[i]] = 1\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = mnist_testset\n",
    "num_labels = len(data.test_labels)\n",
    "test_labels = torch.zeros(num_labels, 10)\n",
    "for i in range(num_labels):\n",
    "    test_labels[i][data.test_labels[i]] = 1\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_trainset.train_data.type(torch.float32).unsqueeze(3) / 255.0\n",
    "mnist_test = mnist_testset.test_data.type(torch.float32).unsqueeze(3) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist_train\n",
    "X_test = mnist_test\n",
    "y_train = train_labels\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cython = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv_transform(input_: Tensor) -> Tensor:\n",
    "\n",
    "    return input_.view(input_.shape[0],\n",
    "                       input_.shape[1] * input_.shape[2] * input_.shape[3])\n",
    "\n",
    "X_train_flat = _conv_transform(X_train)\n",
    "X_test_flat = _conv_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_nn_ll = NeuralNetwork(\n",
    "#     layers=[Dense(neurons=250, \n",
    "#                   activation=Sigmoid()),\n",
    "#             Dense(neurons=50, \n",
    "#                   activation=Sigmoid()),\n",
    "#             Dense(neurons=10, \n",
    "#                   activation=Sigmoid())],\n",
    "#             loss = LogLoss(),\n",
    "#             optimizer = SGD(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start = time.time()\n",
    "# conv_nn_ll.fit(X_train_flat, y_train, X_test_flat, y_test,  \n",
    "#         epochs=1,\n",
    "#         eval_every=1,\n",
    "#         batch_size=100,\n",
    "#         seed=91418);\n",
    "# print(\"Training took\", round(time.time()-start, 1), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nn(nn, X_test, y_test):\n",
    "    test_preds = nn.forward(X_test)\n",
    "    \n",
    "    test_preds_l = torch.argmax(test_preds, dim=1)\n",
    "    \n",
    "    test_vals_l = torch.argmax(y_test, dim=1)\n",
    "    \n",
    "    return torch.sum(test_preds_l == test_vals_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_nn(conv_nn_ll, X_test_flat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_perm = X_train.permute(0, 3, 1, 2)\n",
    "X_test_perm = X_test.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_nn_logloss = NeuralNetwork(\n",
    "    layers=[Conv2D(param_size=3,\n",
    "                   out_channels=24,\n",
    "                   activation=ReLU(),\n",
    "                   pytorch=True),\n",
    "            Conv2D(param_size=3,\n",
    "                   out_channels=30,\n",
    "                   activation=ReLU(),\n",
    "                   pytorch=True,\n",
    "                   flatten=True),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = LogLoss(),\n",
    "            optimizer = SGD(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_perm.requires_grad = True\n",
    "y_train.requires_grad = True\n",
    "X_test_perm.requires_grad = True\n",
    "y_test.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322.0651550292969\n",
      "322.9977111816406\n",
      "331.97906494140625\n",
      "321.9979248046875\n",
      "338.0932922363281\n",
      "327.8659362792969\n",
      "335.0276794433594\n",
      "327.6075439453125\n",
      "309.4156494140625\n",
      "327.34228515625\n",
      "314.3735656738281\n",
      "324.8614196777344\n",
      "309.9920349121094\n",
      "316.54376220703125\n",
      "308.6775817871094\n",
      "309.8320617675781\n",
      "300.82952880859375\n",
      "308.5901794433594\n",
      "308.6893615722656\n",
      "325.7525939941406\n",
      "306.55596923828125\n",
      "307.4443359375\n",
      "316.1963195800781\n",
      "303.03094482421875\n",
      "304.9211120605469\n",
      "293.7526550292969\n",
      "300.8333740234375\n",
      "300.2974853515625\n",
      "296.581298828125\n",
      "296.8754577636719\n",
      "293.6629638671875\n",
      "298.064208984375\n",
      "307.75177001953125\n",
      "296.323486328125\n",
      "293.2658386230469\n",
      "291.41876220703125\n",
      "289.95050048828125\n",
      "297.72930908203125\n",
      "285.41705322265625\n",
      "285.94940185546875\n",
      "289.4371032714844\n",
      "289.85186767578125\n",
      "295.3537292480469\n",
      "299.08087158203125\n",
      "286.6126403808594\n",
      "297.00506591796875\n",
      "280.3492736816406\n",
      "289.7687683105469\n",
      "291.3680114746094\n",
      "294.56494140625\n",
      "301.017578125\n",
      "294.62579345703125\n",
      "283.7514343261719\n",
      "286.19122314453125\n",
      "290.8516540527344\n",
      "282.0351867675781\n",
      "281.07867431640625\n",
      "293.0946960449219\n",
      "289.0044860839844\n",
      "279.0823059082031\n",
      "282.0174255371094\n",
      "284.02923583984375\n",
      "271.6164245605469\n",
      "289.34637451171875\n",
      "277.73016357421875\n",
      "282.89678955078125\n",
      "277.74688720703125\n",
      "290.59661865234375\n",
      "281.8839111328125\n",
      "295.7094421386719\n",
      "282.547607421875\n",
      "274.6737976074219\n",
      "279.735107421875\n",
      "289.5640869140625\n",
      "287.2677307128906\n",
      "285.0365905761719\n",
      "290.1185607910156\n",
      "278.2400207519531\n",
      "280.43505859375\n",
      "285.7885437011719\n",
      "272.0062561035156\n",
      "283.9659118652344\n",
      "273.97894287109375\n",
      "286.265380859375\n",
      "272.4624328613281\n",
      "280.7447509765625\n",
      "281.9978332519531\n",
      "271.1723937988281\n",
      "272.22576904296875\n",
      "279.5349426269531\n",
      "268.9488220214844\n",
      "276.7969665527344\n",
      "280.7274169921875\n",
      "274.2162780761719\n",
      "276.0733947753906\n",
      "271.0444030761719\n",
      "268.2301025390625\n",
      "286.8862609863281\n",
      "277.755615234375\n",
      "264.2189636230469\n",
      "273.21856689453125\n",
      "271.6969909667969\n",
      "266.8641662597656\n",
      "269.1578063964844\n",
      "272.4129943847656\n",
      "271.4093933105469\n",
      "276.9041442871094\n",
      "270.8182373046875\n",
      "277.1156921386719\n",
      "267.8245544433594\n",
      "274.0890197753906\n",
      "274.513671875\n",
      "269.2829895019531\n",
      "269.5415344238281\n",
      "274.8565368652344\n",
      "267.2582092285156\n",
      "265.0041198730469\n",
      "268.0546875\n",
      "273.791259765625\n",
      "271.2295837402344\n",
      "274.7960205078125\n",
      "264.8020935058594\n",
      "263.4597473144531\n",
      "265.66900634765625\n",
      "272.97393798828125\n",
      "259.73870849609375\n",
      "273.92724609375\n",
      "266.7729797363281\n",
      "284.00982666015625\n",
      "268.934326171875\n",
      "269.1163024902344\n",
      "261.2696533203125\n",
      "265.35626220703125\n",
      "269.848876953125\n",
      "277.44384765625\n",
      "266.8171691894531\n",
      "258.2616882324219\n",
      "259.5482482910156\n",
      "263.0431213378906\n",
      "261.72186279296875\n",
      "279.9800109863281\n",
      "262.890625\n",
      "271.52996826171875\n",
      "275.1101379394531\n",
      "264.9140319824219\n",
      "264.48406982421875\n",
      "264.8102111816406\n",
      "263.5755615234375\n",
      "261.4354248046875\n",
      "261.11590576171875\n",
      "257.5506286621094\n",
      "262.6966552734375\n",
      "264.1621398925781\n",
      "262.8918762207031\n",
      "256.185791015625\n",
      "261.83642578125\n",
      "268.35028076171875\n",
      "258.7859802246094\n",
      "261.2035827636719\n",
      "268.79486083984375\n",
      "264.7107849121094\n",
      "271.49298095703125\n",
      "265.7814636230469\n",
      "266.8419494628906\n",
      "261.71343994140625\n",
      "257.93634033203125\n",
      "260.7603759765625\n",
      "276.6614685058594\n",
      "261.55609130859375\n",
      "266.6615905761719\n",
      "259.9866027832031\n",
      "259.399658203125\n",
      "268.4817199707031\n",
      "254.01303100585938\n",
      "276.8758850097656\n",
      "264.7737121582031\n",
      "268.36798095703125\n",
      "264.0113830566406\n",
      "263.5140686035156\n",
      "250.3592987060547\n",
      "252.51083374023438\n",
      "270.26971435546875\n",
      "262.176513671875\n",
      "255.8472900390625\n",
      "262.2190856933594\n",
      "267.1542053222656\n",
      "264.7727355957031\n",
      "262.39520263671875\n",
      "258.24420166015625\n",
      "259.28460693359375\n",
      "259.54364013671875\n",
      "259.14349365234375\n",
      "264.6406555175781\n",
      "257.3922424316406\n",
      "262.9033203125\n",
      "267.0802917480469\n",
      "268.6954650878906\n",
      "262.5783996582031\n",
      "262.3573913574219\n",
      "259.9624328613281\n",
      "256.43182373046875\n",
      "260.5758361816406\n",
      "260.8377990722656\n",
      "255.84561157226562\n",
      "262.87890625\n",
      "256.509521484375\n",
      "264.7319641113281\n",
      "253.9615936279297\n",
      "253.90415954589844\n",
      "263.72705078125\n",
      "269.57781982421875\n",
      "259.1310119628906\n",
      "256.6033935546875\n",
      "258.1765441894531\n",
      "261.1800537109375\n",
      "264.0354309082031\n",
      "259.2097473144531\n",
      "252.46128845214844\n",
      "254.64100646972656\n",
      "259.97601318359375\n",
      "262.34014892578125\n",
      "256.16717529296875\n",
      "263.18890380859375\n",
      "260.9386291503906\n",
      "258.20257568359375\n",
      "261.5002746582031\n",
      "250.81776428222656\n",
      "260.90338134765625\n",
      "261.392822265625\n",
      "271.2614440917969\n",
      "255.279296875\n",
      "258.13262939453125\n",
      "264.5360107421875\n",
      "264.90155029296875\n",
      "261.8641052246094\n",
      "263.0054626464844\n",
      "252.87619018554688\n",
      "257.82611083984375\n",
      "260.44757080078125\n",
      "253.24432373046875\n",
      "267.2544250488281\n",
      "259.6077880859375\n",
      "257.87030029296875\n",
      "262.137939453125\n",
      "254.616943359375\n",
      "262.9797058105469\n",
      "264.3955993652344\n",
      "253.26486206054688\n",
      "259.4895935058594\n",
      "256.42620849609375\n",
      "252.7479248046875\n",
      "248.91156005859375\n",
      "263.3775939941406\n",
      "256.7642822265625\n",
      "277.9770812988281\n",
      "252.08311462402344\n",
      "255.3041534423828\n",
      "255.2555694580078\n",
      "258.8065185546875\n",
      "276.9661865234375\n",
      "255.65606689453125\n",
      "262.1490783691406\n",
      "264.6341552734375\n",
      "255.80868530273438\n",
      "259.42596435546875\n",
      "251.8523712158203\n",
      "255.60986328125\n",
      "256.6427917480469\n",
      "255.2700653076172\n",
      "257.97021484375\n",
      "256.92132568359375\n",
      "252.267578125\n",
      "265.0578918457031\n",
      "249.26235961914062\n",
      "255.82400512695312\n",
      "253.88372802734375\n",
      "249.76756286621094\n",
      "252.12203979492188\n",
      "255.71893310546875\n",
      "259.6867980957031\n",
      "261.310791015625\n",
      "255.3745574951172\n",
      "255.65708923339844\n",
      "267.17333984375\n",
      "259.3456726074219\n",
      "261.8643798828125\n",
      "261.9179382324219\n",
      "255.87940979003906\n",
      "250.64845275878906\n",
      "254.46044921875\n",
      "257.39959716796875\n",
      "255.70330810546875\n",
      "259.3773498535156\n",
      "256.4192199707031\n",
      "266.18695068359375\n",
      "257.17449951171875\n",
      "262.1563720703125\n",
      "254.88153076171875\n",
      "256.5128173828125\n",
      "257.2481689453125\n",
      "261.6103210449219\n",
      "255.9215850830078\n",
      "253.93284606933594\n",
      "263.98486328125\n",
      "253.66827392578125\n",
      "255.86512756347656\n",
      "270.28057861328125\n",
      "254.31561279296875\n",
      "249.9598388671875\n",
      "253.4653778076172\n",
      "251.77398681640625\n",
      "256.8440246582031\n",
      "274.525634765625\n",
      "257.09466552734375\n",
      "257.6570739746094\n",
      "257.9608459472656\n",
      "252.4244842529297\n",
      "251.3670654296875\n",
      "263.858642578125\n",
      "260.12200927734375\n",
      "254.02606201171875\n",
      "253.89645385742188\n",
      "261.06280517578125\n",
      "253.12319946289062\n",
      "257.7884216308594\n",
      "256.06707763671875\n",
      "255.48599243164062\n",
      "266.12548828125\n",
      "255.0441436767578\n",
      "254.69046020507812\n",
      "252.5542755126953\n",
      "254.83009338378906\n",
      "252.47398376464844\n",
      "248.67848205566406\n",
      "256.6961669921875\n",
      "258.44244384765625\n",
      "260.7059631347656\n",
      "253.04885864257812\n",
      "256.72198486328125\n",
      "269.208251953125\n",
      "258.7564392089844\n",
      "254.67636108398438\n",
      "253.99874877929688\n",
      "257.2230529785156\n",
      "261.76837158203125\n",
      "260.23236083984375\n",
      "255.77793884277344\n",
      "250.96908569335938\n",
      "248.6490478515625\n",
      "255.2527618408203\n",
      "261.2488098144531\n",
      "252.56671142578125\n",
      "256.86871337890625\n",
      "252.0792999267578\n",
      "254.247802734375\n",
      "253.75674438476562\n",
      "256.39111328125\n",
      "255.09042358398438\n",
      "259.44671630859375\n",
      "255.33277893066406\n",
      "253.473876953125\n",
      "255.22271728515625\n",
      "252.85870361328125\n",
      "257.3849792480469\n",
      "257.3291931152344\n",
      "255.4622039794922\n",
      "249.99537658691406\n",
      "251.16366577148438\n",
      "255.09146118164062\n",
      "250.4085235595703\n",
      "249.3187255859375\n",
      "253.69009399414062\n",
      "251.67478942871094\n",
      "253.1871795654297\n",
      "252.7020721435547\n",
      "252.73587036132812\n",
      "253.07278442382812\n",
      "250.3832550048828\n",
      "250.53366088867188\n",
      "258.32220458984375\n",
      "249.80703735351562\n",
      "257.47723388671875\n",
      "259.9822998046875\n",
      "260.74139404296875\n",
      "248.2850341796875\n",
      "257.3581848144531\n",
      "252.34335327148438\n",
      "249.557373046875\n",
      "247.71775817871094\n",
      "250.9293975830078\n",
      "254.29873657226562\n",
      "258.81072998046875\n",
      "254.2232666015625\n",
      "254.9774169921875\n",
      "254.49624633789062\n",
      "255.41387939453125\n",
      "252.90667724609375\n",
      "253.23593139648438\n",
      "247.90975952148438\n",
      "266.10858154296875\n",
      "251.147705078125\n",
      "252.2644500732422\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "conv_nn_logloss.fit(X_train_perm, y_train, X_test_perm, y_test,  \n",
    "        epochs=1,\n",
    "        eval_every=1,\n",
    "        batch_size=100,\n",
    "        seed=91418);\n",
    "print(\"Training took\", round(time.time()-start, 1), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7553)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_nn(conv_nn_logloss, X_test_perm, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
