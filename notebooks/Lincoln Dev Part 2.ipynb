{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm developing a few more layers and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lincoln package is in parent directory so lets add it to our path\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "from lincoln.layers import Layer\n",
    "from lincoln.losses import Loss\n",
    "import lincoln as lnc\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off we're going to create a version of the `Sigmoid` layer that returns the log probability. Using the log probability is much more stable computationally and the math gets much nicer. You should have noticed that we had to add a small vaue to the `LogLoss` forward method such that we never compute `torch.log(0)`. Mathematically, $p$ should never go exactly to zero and $\\log{p}$ should never explode. However, due to inaccuracies in floating point precision, you can get $p=0$ or $p=1$ which is undefined for $\\log(p)$. The `torch.log` function returns `-inf` and all the computations go haywire.\n",
    "\n",
    "So, to avoid this, we'll return $\\log{p}$ instead of $p$ itself. Then we can write a loss class that expects $\\log{p}$ is the input. The gradient is much simpler to compute using $\\log{p}$ as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `LogSigmoid` Layer\n",
    "\n",
    "The sigmoid function is defined as \n",
    "\n",
    "$$\n",
    "p = \\frac{e^a}{e^a + 1}\n",
    "$$\n",
    "\n",
    "If we take the log of this, \n",
    "\n",
    "$$\n",
    "z = \\log{p} = a - \\log{\\left(e^a + 1\\right)}\n",
    "$$\n",
    "\n",
    "From this `LogSigmoid` layer, we'll want to return `z = a - torch.log(torch.exp(a) + 1)`.\n",
    "\n",
    "For the backward pass, we'll need to return the gradient of this with respect to $a$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial a} &= 1 - \\frac{e^a}{e^a+1} \\\\\n",
    "&= 1 - p \\\\\n",
    "&= 1 - e^z\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then, the gradient is just `backward_grad = (1 - torch.exp(z))*in_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \n",
    "        if input.dim() != 2:\n",
    "            raise DimensionError(f\"Tensor should have dimension 2, instead it has dimension {input.dim()}\")\n",
    "        \n",
    "        self.last_input = input\n",
    "        self.output = input - torch.log(torch.exp(input) + 1)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, in_grad: Tensor) -> Tensor:\n",
    "        \n",
    "        if not hasattr(self, 'output'):\n",
    "            message = \"The forward method must be run before the backward method\"\n",
    "            raise lnc.exc.BackwardError(message)  \n",
    "        elif self.output.shape != in_grad.shape:\n",
    "            message = (f\"Two tensors should have the same shape; instead, first Tensor's shape \"\n",
    "                       f\"is {in_grad.shape} and second Tensor's shape is {self.output.shape}.\")\n",
    "            raise MatchError(message)\n",
    "        \n",
    "        backward_grad = (1 - torch.exp(self.output))*in_grad\n",
    "        \n",
    "        return backward_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `LogSigmoidLoss` loss\n",
    "\n",
    "Now I'm going to create a loss for this.\n",
    "\n",
    "From before\n",
    "\n",
    "$$\n",
    "L = \\begin{cases}\n",
    "    -\\log{p}     & \\text{if } y = 1 \\\\\n",
    "    -\\log{(1-p)}  & \\text{if } y = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The input to this loss is $z = \\log{p}$ though, so converting\n",
    "\n",
    "$$\n",
    "L = \\begin{cases}\n",
    "    -z     & \\text{if } y = 1\\\\\n",
    "    -\\log{(1-e^z)}  & \\text{if } y = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the gradient\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\begin{cases}\n",
    "    -1     & \\text{if } y = 1\\\\\n",
    "    \\frac{e^z}{1-e^z}  & \\text{if } y = 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSigmoidLoss(Loss):\n",
    "    def __init__(self, network):\n",
    "        super().__init__(network)\n",
    "        \n",
    "    def forward(self, input: Tensor, labels: Tensor) -> float:\n",
    "        \n",
    "        # Here we're assuming z is a log-probability\n",
    "        self.last_input = z = self.network(input)\n",
    "        self.labels = y = labels\n",
    "        \n",
    "        loss = torch.sum(-y*z - (1-y)*torch.log(1-torch.exp(z)))\n",
    "        return loss.item()\n",
    "    \n",
    "    def gradient(self) -> Tensor:\n",
    "        y, z = self.labels, self.last_input\n",
    "        n = y.shape[0]\n",
    "        exp_z = torch.exp(z)\n",
    "        \n",
    "        grad = torch.sum(-y + (1-y)*exp_z/(1 - exp_z), dim=1).view(n, -1)\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()\n",
    "features = breast_cancer.data\n",
    "labels = breast_cancer.target\n",
    "feature_names = breast_cancer.feature_names\n",
    "\n",
    "features = lnc.utils.standardize(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20.. Train loss: 0.8244..  Accuracy: 96.485%\n",
      "Epoch 40.. Train loss: 0.4849..  Accuracy: 98.243%\n",
      "Epoch 60.. Train loss: 0.4018..  Accuracy: 98.594%\n",
      "Epoch 80.. Train loss: 0.3626..  Accuracy: 98.594%\n",
      "Epoch 100.. Train loss: 0.3388..  Accuracy: 98.594%\n",
      "Epoch 120.. Train loss: 0.3226..  Accuracy: 98.594%\n",
      "Epoch 140.. Train loss: 0.3106..  Accuracy: 98.594%\n",
      "Epoch 160.. Train loss: 0.3014..  Accuracy: 98.594%\n",
      "Epoch 180.. Train loss: 0.2940..  Accuracy: 98.594%\n",
      "Epoch 200.. Train loss: 0.2880..  Accuracy: 98.770%\n",
      "Epoch 220.. Train loss: 0.2828..  Accuracy: 98.770%\n",
      "Epoch 240.. Train loss: 0.2784..  Accuracy: 98.770%\n",
      "Epoch 260.. Train loss: 0.2744..  Accuracy: 98.770%\n",
      "Epoch 280.. Train loss: 0.2709..  Accuracy: 98.770%\n",
      "Epoch 300.. Train loss: 0.2676..  Accuracy: 98.770%\n",
      "Epoch 320.. Train loss: 0.2646..  Accuracy: 98.770%\n",
      "Epoch 340.. Train loss: 0.2618..  Accuracy: 98.770%\n",
      "Epoch 360.. Train loss: 0.2592..  Accuracy: 98.770%\n",
      "Epoch 380.. Train loss: 0.2566..  Accuracy: 98.770%\n",
      "Epoch 400.. Train loss: 0.2542..  Accuracy: 98.770%\n",
      "Epoch 420.. Train loss: 0.2519..  Accuracy: 98.946%\n",
      "Epoch 440.. Train loss: 0.2496..  Accuracy: 98.946%\n",
      "Epoch 460.. Train loss: 0.2475..  Accuracy: 98.946%\n",
      "Epoch 480.. Train loss: 0.2453..  Accuracy: 98.946%\n",
      "Epoch 500.. Train loss: 0.2432..  Accuracy: 98.946%\n"
     ]
    }
   ],
   "source": [
    "network = lnc.Sequential(\n",
    "            lnc.layers.Dense(10),\n",
    "            lnc.layers.Dense(1, activation=LogSigmoid()),\n",
    "            )\n",
    "model = lnc.models.Logistic(network,\n",
    "                            loss=LogSigmoidLoss(network))\n",
    "model.fit(features, labels, batch_size=128, log_ps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Softmax` Layer\n",
    "\n",
    "$$\n",
    "p_j = \\frac{e^{a_j}}{\\sum_n^N e^{a_n}}\n",
    "$$\n",
    "\n",
    "and the gradient:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathrm{with} \\; \\Sigma = \\sum_n^N e^{a_n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial p_j}{\\partial a_k} &= \\frac{\\partial}{\\partial a_k} \\left[ e^{a_j} \\right] \\Sigma^{-1} + e^{a_j}\\frac{\\partial}{\\partial a_k}\\Sigma^{-1} \\\\\n",
    "&= \\delta_{jk}e^{a_j}\\Sigma^{-1} - e^{a_j} e^{a_k}\\Sigma^{-2} \\\\\n",
    "&= \\frac{e^{a_j}}{\\Sigma}\\left(\\delta_{jk} - \\frac{e^{a_k}}{\\Sigma}\\right) \\\\\n",
    "&= p_j (\\delta_{jk} - p_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "or should I write it out like this?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial p_j}{\\partial a_k} &= \\frac{\\partial}{\\partial a_k} \\left[ e^{a_j} \\right] \\left(\\sum_n^N e^{a_n}\\right)^{-1} + e^{a_j}\\frac{\\partial}{\\partial a_k}\\left(\\sum_n^N e^{a_n}\\right)^{-1} \\\\\n",
    "&= \\delta_{jk}e^{a_j}\\left(\\sum_n^N e^{a_n}\\right)^{-1} - e^{a_j} e^{a_k}\\left(\\sum_n^N e^{a_n}\\right)^{-2} \\\\\n",
    "&= \\frac{e^{a_j}}{\\sum_n^N e^{a_n}}\\left(\\delta_{jk} - \\frac{e^{a_k}}{\\sum_n^N e^{a_n}}\\right) \\\\\n",
    "&= p_j (\\delta_{jk} - p_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we use the Kronecker delta: $\\delta_{jk} = 1$ if $j=k$, otherwise $\\delta_{jk} = 0$\n",
    "\n",
    "This ends up as a Jacobian matrix, should have some text here around that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \n",
    "        if input.dim() != 2:\n",
    "            raise DimensionError(f\"Tensor should have dimension 2, instead it has dimension {input.dim()}\")\n",
    "        \n",
    "        self.last_input = input\n",
    "        n = input.shape[0]\n",
    "        self.output = torch.exp(input) / torch.sum(torch.exp(input), dim=1).view(n, 1)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, in_grad: Tensor) -> Tensor:\n",
    "        \n",
    "        if not hasattr(self, 'output'):\n",
    "            message = \"The forward method must be run before the backward method\"\n",
    "            raise lnc.exc.BackwardError(message)  \n",
    "        elif self.output.shape != in_grad.shape:\n",
    "            message = (f\"Two tensors should have the same shape; instead, first Tensor's shape \"\n",
    "                       f\"is {in_grad.shape} and second Tensor's shape is {self.output.shape}.\")\n",
    "            raise lnc.exc.MatchError(message)\n",
    "        \n",
    "        ps = self.output\n",
    "        N, M = ps.shape[0], ps.shape[1]\n",
    "        batch_jacobian = torch.zeros((N, M, M))\n",
    "        \n",
    "        for ii, p in enumerate(ps):\n",
    "            batch_jacobian[ii,:,:] = torch.diag(p) - torch.ger(p, p)\n",
    "        \n",
    "        backward_grad = torch.bmm(in_grad.view(N, 1, -1), batch_jacobian)\n",
    "        backward_grad.squeeze_()\n",
    "        \n",
    "        # Key assertion\n",
    "        if self.last_input.shape != backward_grad.shape:\n",
    "            message = (f\"Two tensors should have the same shape; instead, first Tensor's shape \"\n",
    "                       f\"is {self.last_input.shape} and second Tensor's shape is {backward_grad.shape}.\")\n",
    "            raise lnc.exc.MatchError(message)\n",
    "        \n",
    "        return backward_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `CrossEntropyLoss`\n",
    "\n",
    "Here we'll build a class for the cross-entropy loss.\n",
    "\n",
    "For this loss, we get class probabilities $p$ from the softmax function. For one example $i$, the cross-entropy loss is defined as:\n",
    "\n",
    "$$\n",
    "L_i = -\\log{p_c}\\quad \\mathrm{where}\\;(c = y_i)\n",
    "$$\n",
    "\n",
    "The total loss is the sum over all examples $L = \\sum_i L_i$\n",
    "\n",
    "Then, the gradient is for one example,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_i}{\\partial p_j} = -\\frac{\\delta_{jc}}{p_c} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Loss):\n",
    "    def __init__(self, network: typing.Type[Layer]):\n",
    "        super().__init__(network)\n",
    "        \n",
    "    def forward(self, input: Tensor, labels: Tensor) -> float:\n",
    "        \n",
    "        self.last_input = ps = self.network(input)\n",
    "        self.labels = ys = labels\n",
    "        self.output = -torch.log(torch.gather(ps, 1, ys))\n",
    "        return torch.sum(self.output).item()\n",
    "    \n",
    "    def gradient(self) -> Tensor:\n",
    "        ps = self.last_input\n",
    "        ys = self.labels\n",
    "        \n",
    "        # Create a mask for our correct labels, with 1s for the true labels, 0 elsewhere\n",
    "        mask = torch.zeros_like(ps)\n",
    "        mask.scatter_(1, ys, 1)\n",
    "        \n",
    "        # Picking out particular elements denoted by the correct labels\n",
    "        grads = mask * -1/ps\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train on MNIST, classifying handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
    "mnist = datasets.MNIST('./MNIST_data', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(mnist, batch_size=128, shuffle=True)\n",
    "\n",
    "test_mnist = datasets.MNIST('./MNIST_data', transform=transform, train=False)\n",
    "testloader = torch.utils.data.DataLoader(test_mnist, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a few metrics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    \n",
    "    def __init__(self, logprob=False):\n",
    "        self.logprob = logprob\n",
    "    \n",
    "    def metric(self, ps: Tensor, labels: Tensor) -> float:\n",
    "        if self.logprob:\n",
    "            _, predictions = torch.exp(ps).topk(1)\n",
    "        else:\n",
    "            _, predictions = ps.topk(1)\n",
    "        \n",
    "        equals = predictions.squeeze() == labels.squeeze()\n",
    "        accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "        return accuracy.item()\n",
    "\n",
    "    def __call__(self, ps: Tensor, labels: Tensor) -> float:\n",
    "        return self.metric(ps, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKError:\n",
    "    \n",
    "    def __init__(self, k=5, logprob=False):\n",
    "        self.k = k\n",
    "        self.logprob = logprob\n",
    "    \n",
    "    def metric(self, ps: Tensor, labels: Tensor) -> float:\n",
    "        if self.logprob:\n",
    "            ps = torch.exp(ps)\n",
    "            \n",
    "        p, cls = ps.topk(self.k)\n",
    "        labels = labels.view(-1, 1).repeat((1, self.k))\n",
    "        equals = (cls == labels).type(torch.FloatTensor)\n",
    "        error_rate = 1 - equals.sum(dim=1).mean()\n",
    "        return error_rate.item()\n",
    "\n",
    "    def __call__(self, ps: Tensor, labels: Tensor) -> float:\n",
    "        return self.metric(ps, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatGenerator():\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        \n",
    "    def __iter__(self):\n",
    "        yield from ((x.view(x.shape[0], -1), y.view(-1, 1)) for x, y in self.dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Should clean up how this `FlatGenerator` and batch generators work in general from an API/UI viewpoint. It's a little clunky for my tastes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.. Train loss: 136.3370..  Metric: 0.889\n",
      "Epoch 1.. Train loss: 48.0073..  Metric: 0.908\n",
      "Epoch 1.. Train loss: 40.7395..  Metric: 0.916\n",
      "Epoch 1.. Train loss: 36.0489..  Metric: 0.925\n"
     ]
    }
   ],
   "source": [
    "network = lnc.Sequential(lnc.layers.Dense(500), \n",
    "                         lnc.layers.Dense(10, activation=Softmax()))\n",
    "model = lnc.models.Classifier(network,\n",
    "                              loss=CrossEntropy(network),\n",
    "                              batch_gen=FlatGenerator(trainloader),\n",
    "                              valid_gen=FlatGenerator(testloader),\n",
    "                              metric=Accuracy())\n",
    "model.fit(epochs=1, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `LogSoftmax`\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_j &= \\frac{e^{a_j}}{\\sum_n^N e^{a_n}} \\\\\n",
    "z_j &= \\log{p_j} = a_j - \\log{\\sum_n e^{a_n}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_j}{\\partial a_k} = \\delta_{jk} - p_k = \\delta_{jk} - e^{z_k}\n",
    "$$\n",
    "\n",
    "The Jacobian looks like\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{a}}\\mathbf{z} = \\begin{bmatrix}\n",
    "1 - p_1 & -p_2 & -p_3 & \\dots  & -p_K \\\\\n",
    "-p_1 & 1 - p_2 & -p_3 & \\dots  & -p_K \\\\\n",
    "-p_1 & -p_2 & 1 - p_3 & \\dots  & -p_K \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-p_1 & -p_2 & -p_3 & \\dots  & 1 - p_K\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSoftmax(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        if input.dim() != 2:\n",
    "            raise DimensionError(f\"Tensor should have dimension 2, instead it has dimension {input.dim()}\")\n",
    "        \n",
    "        self.last_input = input\n",
    "        self.output = input - torch.log(torch.exp(input).sum(dim=1).view(-1, 1))\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, in_grad: Tensor) -> Tensor:\n",
    "        \n",
    "        if not hasattr(self, 'output'):\n",
    "            message = f\"The forward method of {self} must be run before the backward method\"\n",
    "            raise lnc.exc.BackwardError(message)  \n",
    "        elif self.output.shape != in_grad.shape:\n",
    "            message = (f\"Two tensors should have the same shape; instead, first Tensor's shape \"\n",
    "                       f\"is {in_grad.shape} and second Tensor's shape is {self.output.shape}.\")\n",
    "            raise lnc.exc.MatchError(message)\n",
    "        \n",
    "        ps = torch.exp(self.output)\n",
    "        N, M = ps.shape[0], ps.shape[1]\n",
    "        batch_jacobian = torch.zeros((N, M, M))\n",
    "         \n",
    "        # Create an identity matrix\n",
    "        ones = torch.diagflat(torch.ones(M))\n",
    "        \n",
    "        for ii, p in enumerate(ps):\n",
    "            # Repeat the p values across columns to get p_k\n",
    "            p_k = p.repeat((M, 1))\n",
    "            batch_jacobian[ii,:,:] = ones - p_k\n",
    "        \n",
    "        backward_grad = torch.bmm(in_grad.view(N, 1, -1), batch_jacobian)\n",
    "        backward_grad.squeeze_()\n",
    "        \n",
    "        # Key assertion\n",
    "        if self.last_input.shape != backward_grad.shape:\n",
    "            message = (f\"Two tensors should have the same shape; instead, first Tensor's shape \"\n",
    "                       f\"is {self.last_input.shape} and second Tensor's shape is {backward_grad.shape}.\")\n",
    "            raise lnc.exc.MatchError(message)\n",
    "        \n",
    "        return backward_grad\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LogSoftmax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log-Likelihood Loss: `NLLLoss`\n",
    "\n",
    "Now that we have our `LogSigmoid` layer, we need a loss that expects to receive log-probabilities.\n",
    "\n",
    "\n",
    "#### Forward pass\n",
    "Here, with $y_i$ as the correct class for example $i$\n",
    "\n",
    "$$\n",
    "L_i = -z_{y_i}\n",
    "$$\n",
    "\n",
    "and the total loss\n",
    "\n",
    "$$\n",
    "L = \\sum_i L_i\n",
    "$$\n",
    "\n",
    "We can build this loss using the `scatter` method. This takes a tensor of indices and places values from another tensor at those indices. So what we can do is a create a tensor of all zeros, then place $z_{y_i}$ at the correct element.\n",
    "\n",
    "If `zs` is the input and `ys` are the correct classes, \n",
    "\n",
    "```python\n",
    ">> zeros = torch.zeros_like(zs)\n",
    ">> mask = zeros.scatter(1, ys, 1)\n",
    ">> print(mask)\n",
    "tensor([[ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
    "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
    "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
    "        [ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
    ">> loss = mask * -zs\n",
    "```\n",
    "\n",
    "#### Backward pass\n",
    "Now we want to calculate the gradient, fairly straightforward here:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_i}{\\partial z_j} = -\\delta_{jc} \\quad \\mathrm{where}\\;(c = y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLoss(Loss):\n",
    "    def __init__(self, network):\n",
    "        super().__init__(network)\n",
    "        \n",
    "    def forward(self, input: Tensor, labels: Tensor) -> float:\n",
    "        \n",
    "        self.last_input = zs = self.network(input)\n",
    "        self.labels = labels\n",
    "        \n",
    "        zeros = torch.zeros_like(zs)\n",
    "        mask = zeros.scatter(1, labels, 1)\n",
    "        L = mask * -zs\n",
    "        \n",
    "        self.output = L.sum().item()\n",
    "        return self.output\n",
    "    \n",
    "    def gradient(self) -> Tensor:\n",
    "        \n",
    "        if not hasattr(self, 'output'):\n",
    "            message = f\"The forward method of {self} must be run before the backward method\"\n",
    "            raise lnc.exc.BackwardError(message)\n",
    "        \n",
    "        zeros = torch.zeros_like(self.last_input)\n",
    "        backward_grad = zeros.scatter(1, self.labels, -1)\n",
    "        \n",
    "        # Key assertion\n",
    "        if self.last_input.shape != backward_grad.shape:\n",
    "            message = (f\"Two tensors should have the same shape; instead, first Tensor's shape \"\n",
    "                       f\"is {self.last_input.shape} and second Tensor's shape is {backward_grad.shape}.\")\n",
    "            raise lnc.exc.MatchError(message)\n",
    "        \n",
    "        return backward_grad\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"NLLLoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.. Train loss: 144.6850..  Metric: 0.853\n",
      "Epoch 1.. Train loss: 49.0181..  Metric: 0.911\n",
      "Epoch 1.. Train loss: 38.9346..  Metric: 0.907\n",
      "Epoch 1.. Train loss: 34.7589..  Metric: 0.928\n",
      "Epoch 2.. Train loss: 9.6997..  Metric: 0.933\n",
      "Epoch 2.. Train loss: 29.0309..  Metric: 0.937\n",
      "Epoch 2.. Train loss: 27.5376..  Metric: 0.941\n",
      "Epoch 2.. Train loss: 24.5093..  Metric: 0.947\n",
      "Epoch 2.. Train loss: 24.5538..  Metric: 0.949\n",
      "Epoch 3.. Train loss: 13.5384..  Metric: 0.954\n",
      "Epoch 3.. Train loss: 19.9678..  Metric: 0.953\n",
      "Epoch 3.. Train loss: 20.4131..  Metric: 0.956\n",
      "Epoch 3.. Train loss: 18.1194..  Metric: 0.958\n",
      "Epoch 3.. Train loss: 17.5657..  Metric: 0.958\n",
      "Epoch 4.. Train loss: 14.8582..  Metric: 0.963\n",
      "Epoch 4.. Train loss: 16.5139..  Metric: 0.963\n",
      "Epoch 4.. Train loss: 14.7026..  Metric: 0.963\n",
      "Epoch 4.. Train loss: 14.2133..  Metric: 0.966\n",
      "Epoch 5.. Train loss: 3.2286..  Metric: 0.967\n",
      "Epoch 5.. Train loss: 13.0312..  Metric: 0.968\n",
      "Epoch 5.. Train loss: 11.9734..  Metric: 0.968\n",
      "Epoch 5.. Train loss: 11.8046..  Metric: 0.968\n",
      "Epoch 5.. Train loss: 12.3526..  Metric: 0.970\n"
     ]
    }
   ],
   "source": [
    "network = lnc.Sequential(lnc.layers.Dense(500), \n",
    "                         lnc.layers.Dense(10, activation=LogSoftmax()))\n",
    "model = lnc.models.Classifier(network,\n",
    "                              loss=NLLLoss(network),\n",
    "                              batch_gen=FlatGenerator(trainloader),\n",
    "                              valid_gen=FlatGenerator(testloader),\n",
    "                              metric=Accuracy(logprob=True))\n",
    "model.fit(epochs=5, print_every=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
