{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History\n",
    "\n",
    "This was an attempt to do LSTMs using the `PyTorchOperation` approach that I tried in convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Lincoln to system path\n",
    "import sys\n",
    "sys.path.append(\"/Users/seth/development/lincoln/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "from typing import Tuple, Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operation with multiple inputs, gradients\n",
    "class Operation(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, *inputs) -> Tuple[Tensor]:\n",
    "\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "\n",
    "    def backward(self, *output_grads) -> Tuple[Tensor]:\n",
    "\n",
    "        assert_same_shapes(self.outputs, output_grads)\n",
    "\n",
    "        self._compute_grads(output_grads)\n",
    "\n",
    "        assert_same_shapes(self.inputs, self.input_grads)\n",
    "        return self.input_grads\n",
    "\n",
    "\n",
    "    def _compute_grads(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "        \n",
    "        self.input_grads = self._input_grads(output_grads)\n",
    "\n",
    "        return self.input_grads\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grads(self, *output_grads) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply class\n",
    "class Multiply(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise multiplication\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        return self.inputs[0] * self.inputs[1]\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        return self.inputs[1] * output_grads[0],\\\n",
    "               self.inputs[0] * output_grads[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert_same_shapes function\n",
    "def assert_same_shapes(tensors: Tuple[Tensor],\n",
    "                       tensor_grads: Tuple[Tensor]):\n",
    "    if len(tensors) == 1:\n",
    "        tensors = tensors[0]\n",
    "    if len(tensor_grads) == 1:\n",
    "        tensor_grads = tensor_grads[0]        \n",
    "\n",
    "    for tensor, tensor_grad in zip(tensors, tensor_grads):\n",
    "        assert tensor.shape == tensor_grad.shape, \\\n",
    "        '''\n",
    "        Two tensors should have the same shape; instead, first Tensor's shape is {0}\n",
    "        and second Tensor's shape is {1}.\n",
    "        '''.format(tuple(tensor_grad.shape), tuple(tensor.shape))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inputs to Multiply Operation\n",
    "m = Multiply()\n",
    "torch.manual_seed(102218)\n",
    "m1, m2 = torch.rand(2,2), torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8841, 0.2608],\n",
       "        [0.1206, 0.4546]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Multiply (Operation with multiple inputs) forward\n",
    "m.forward(m1, m2) # tuple of length 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9344, 0.4609],\n",
       "         [0.4948, 0.5119]]), tensor([[0.9462, 0.5658],\n",
       "         [0.2438, 0.8881]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Multiply (Operation with multiple inputs) backward\n",
    "m_grad = torch.ones_like(m1)\n",
    "m.backward(m_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch `Multiply`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9462, 0.5658],\n",
      "        [0.2438, 0.8881]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# m1 initially\n",
    "print(m1)\n",
    "print(m1.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detach to copy values\n",
    "m1_g = m1.detach()\n",
    "m2_g = m2.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require gradients\n",
    "m1_g.requires_grad = True\n",
    "m2_g.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform operation\n",
    "out = m1_g * m2_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define grad\n",
    "mul_grad = torch.ones_like(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send gradient backward\n",
    "out.backward(gradient=mul_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9344, 0.4609],\n",
      "        [0.4948, 0.5119]])\n",
      "tensor([[0.9462, 0.5658],\n",
      "        [0.2438, 0.8881]])\n"
     ]
    }
   ],
   "source": [
    "# same gradients as above!\n",
    "print(m1_g.grad)\n",
    "print(m2_g.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PyTorchOperation` base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Operation class\n",
    "class PyTorchOperation(Operation):\n",
    "\n",
    "    def __init__(self) -> Tensor:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, *inputs) -> Tuple[Tensor]:\n",
    "\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.inputs_with_grad = self._inputs_autograd()\n",
    "        \n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "        \n",
    "        \n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "    def _inputs_autograd(self) -> Tuple[Tensor]:\n",
    "        inputs_with_grad = tuple(inp.detach() for inp in self.inputs)\n",
    "        for inp in inputs_with_grad:\n",
    "            inp.requires_grad = True\n",
    "        return inputs_with_grad\n",
    "        \n",
    "\n",
    "    def _input_grads(self, output_grads: Tensor) -> Tensor:\n",
    "\n",
    "        for out, grad in zip(self.outputs, output_grads):\n",
    "            out.backward(gradient=grad)\n",
    "\n",
    "        input_grads = tuple()\n",
    "        for inp in self.inputs_with_grad:\n",
    "            input_grads = input_grads + (inp.grad,)\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply_PyTorch class\n",
    "class Multiply_PyTorch(PyTorchOperation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        \n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        return self.inputs_with_grad[0] * self.inputs_with_grad[1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Multiply stuff\n",
    "m = Multiply_PyTorch()\n",
    "mp1, mp2 = torch.rand(2,2), torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3988, 0.1799],\n",
      "        [0.6886, 0.0705]]) tensor([[0.3258, 0.4976],\n",
      "        [0.6935, 0.2254]])\n"
     ]
    }
   ],
   "source": [
    "# Print initial tensors\n",
    "print(mp1, mp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1299, 0.0895],\n",
       "         [0.4775, 0.0159]], grad_fn=<ThMulBackward>),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward\n",
    "m.forward(mp1, mp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3258, 0.4976],\n",
       "         [0.6935, 0.2254]]), tensor([[0.3988, 0.1799],\n",
       "         [0.6886, 0.0705]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize grad and backward\n",
    "m.backward(torch.ones_like(mp1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complicated example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Divide two inputs into:\n",
    "    * `A`, `B`\n",
    "    * `A` gets copied into `A1` and `A2`\n",
    "    * `A1` and `A2` each get multiplied, bias added, activation, to create `C1` and `C2`.\n",
    "    * These get added together to create `D`.\n",
    "    * `B` and `D` get multiplied to create `E`. \n",
    "    \n",
    "Then the question is: what is the gradient of `E` with respect to `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lincoln.operations.operations import WeightMultiply\n",
    "from lincoln.operations.operations import BiasAdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define base operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy class - 1 to many\n",
    "class Copy(Operation):\n",
    "   \n",
    "    def __init__(self, num=2):\n",
    "        self.num = num\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise multiplication\n",
    "        '''\n",
    "        output = tuple()\n",
    "        for i in range(self.num):\n",
    "            output = output + self.inputs\n",
    "            \n",
    "        return output\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "        input_grad = torch.zeros_like(output_grads[0])\n",
    "        for grad in output_grads:\n",
    "             input_grad = input_grad + grad\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add2 Operation\n",
    "class Add2(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise addition\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        return self.inputs[0] + self.inputs[1]\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        return output_grads[0], output_grads[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat operation - two to one\n",
    "class Concat2(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tensor:\n",
    "        '''\n",
    "        Element-wise multiplication\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        self.input_shapes = [inp.shape[1] for inp in self.inputs]\n",
    "        \n",
    "        return torch.cat(list(self.inputs), dim=1)\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "        return torch.split(output_grads[0], \n",
    "                           self.input_shapes,\n",
    "                           dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# printing \"a\" and \"b\" tensors\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 1.]]), tensor([[1., 1.],\n",
       "         [1., 1.]]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Concat2()\n",
    "out = c.forward(a,b)\n",
    "print(out)\n",
    "c.backward(torch.ones_like(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: incorporate these into Lincoln somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tensors\n",
    "torch.manual_seed(102218)\n",
    "a, b = torch.rand(2,2), torch.rand(2,2)\n",
    "w1, w2 = torch.rand(2,2), torch.rand(2,2)\n",
    "b1, b2 = torch.rand(1,2), torch.rand(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set requires_grad = True for all tensors\n",
    "for t in [a, b, w1, w2, b1, b2]:\n",
    "    t.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define operations\n",
    "c = Copy()\n",
    "add2 = Add2()\n",
    "mul = Multiply()\n",
    "wm1 = WeightMultiply(w1)\n",
    "ba1 = BiasAdd(b1)\n",
    "wm2 = WeightMultiply(w2)\n",
    "ba2 = BiasAdd(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define forward path\n",
    "a1, a2 = c.forward(a)\n",
    "c1 = ba1.forward(wm1.forward(a1))\n",
    "c2 = ba2.forward(wm2.forward(a2))\n",
    "d = add2.forward(c1, c2)\n",
    "e = mul.forward(b, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4230, 1.1577],\n",
       "        [1.2516, 1.0909]], grad_fn=<ThMulBackward>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9232, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(e) # sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing gradient backwards\n",
    "e.backward(gradient=torch.ones_like(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9893, 1.4278],\n",
      "        [0.7053, 0.8353]])\n",
      "tensor([[2.5932, 2.5116],\n",
      "        [2.5298, 2.1312]])\n",
      "tensor([[1.0048, 0.5609],\n",
      "        [0.9681, 0.7154]])\n",
      "tensor([[1.4291, 0.9728]])\n"
     ]
    }
   ],
   "source": [
    "# all grads work\n",
    "print(a.grad)\n",
    "\n",
    "print(b.grad)\n",
    "\n",
    "print(w1.grad)\n",
    "\n",
    "print(b1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that increasing `a[0][0]` by `0.1` will increase the sum from `5.9232` to `5.9232 + 0.1 * 0.9893`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.02213"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define what sum with \"a_new\" should be: \n",
    "5.9232 + 0.1 * 0.9893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper function\n",
    "def sum_with_a(a):\n",
    "    a1, a2 = c.forward(a)\n",
    "    c1 = ba1.forward(wm1.forward(a1))\n",
    "    c2 = ba2.forward(wm2.forward(a2))\n",
    "    d = add2.forward(c1, c2)\n",
    "    e = mul.forward(b, d)\n",
    "    return torch.sum(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0462, 0.5658],\n",
      "        [0.2438, 0.8881]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# define new version of a with one value incremented\n",
    "a_new = a.clone()\n",
    "a_new[0][0] += 0.1\n",
    "print(a_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9232, grad_fn=<SumBackward0>)\n",
      "tensor(6.0222, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test that sum equals expected i.e. that gradients are correct\n",
    "print(sum_with_a(a))\n",
    "print(sum_with_a(a_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning this into a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Layer` class:\n",
    "\n",
    "* Has forward and backward methods. These simply loop through the operations, passing things forwards or backards.\n",
    "* On setup, requires a `num_in`\n",
    "\n",
    "`AutogradBlock` class:\n",
    "\n",
    "* Has forward method, which calls the `_output` method and `_setup` if it is the first iteration.\n",
    "* On setup, requires we define all ops in a `Dict[Operation]`. In addition, we define weights and give them a gradient.\n",
    "* On `_output`, we actually compute the output from the inputs.\n",
    "* We get the `param_grads` in a similar way we get them from `Layer`s - looping through a dictionary now instead of through a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouble is: this solution begs the question: why not write an autograd library and do everything that way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for autograd from scratch:\n",
    "\n",
    "* Could invest a week and unpack the example at `autodiff` in README.\n",
    "   * He makes heavy use of `einsum`, which is its own overhead.\n",
    "* Just use PyTorch (bad - why the operations? Might be ok to illustrate LSTM). \n",
    "\n",
    "Point is to illustrate how things work. Period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lincoln operations\n",
    "from lincoln.operations.operations import WeightMultiply\n",
    "from lincoln.operations.operations import BiasAdd\n",
    "from lincoln.operations.base import ParamOperation\n",
    "from lincoln.operations.activations import Sigmoid, Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutogradBlock: block of operations with multiple inputs and outputs\n",
    "class AutogradBlock(object):\n",
    "\n",
    "    def __init__(self) -> Tensor:\n",
    "        super().__init__()\n",
    "        self.params: Dict[Tensor] = {}\n",
    "        self.param_grads: List[Tensor] = []\n",
    "        self.ops: Dict[Operation] = {}\n",
    "        self.first: bool = True\n",
    "\n",
    "            \n",
    "    def _setup_block(self) -> Tuple[Tensor]:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def forward(self, *inputs) -> Tuple[Tensor]:\n",
    "\n",
    "        if self.first:\n",
    "            self._setup_block()\n",
    "            self.first = False\n",
    "            \n",
    "        self.inputs = inputs\n",
    "        \n",
    "        self.inputs_with_grad = self._inputs_autograd()\n",
    "        \n",
    "        self.params_with_grad = self._params_autograd()\n",
    "        self._gradify_operations()\n",
    "\n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "    def _inputs_autograd(self) -> Tuple[Tensor]:\n",
    "        inputs_with_grad = tuple(inp.detach() for inp in self.inputs)\n",
    "        for inp in inputs_with_grad:\n",
    "            inp.requires_grad = True\n",
    "        return inputs_with_grad\n",
    "\n",
    "    \n",
    "    def _params_autograd(self) -> Tuple[Tensor]:\n",
    "        params_with_grad = tuple(param.detach() for param in self.params.values())\n",
    "        for param in params_with_grad:\n",
    "            param.requires_grad = True\n",
    "        return params_with_grad\n",
    "    \n",
    "\n",
    "    def _gradify_operations(self) -> Tuple[Tensor]:\n",
    "        for op, tensor in zip([op for op in self.ops.values() \n",
    "                               if issubclass(op.__class__, ParamOperation)],\n",
    "                              self.params_with_grad):\n",
    "            setattr(op, \"param\", tensor)\n",
    "    \n",
    "    \n",
    "    def backward(self, *output_grads) -> Tuple[Tensor]:\n",
    "\n",
    "        assert_same_shapes(self.outputs, output_grads)\n",
    "\n",
    "        self.input_grads = self._input_grads(output_grads)\n",
    "        \n",
    "        if self.params:\n",
    "            self.param_grads = self._param_grads()\n",
    "    \n",
    "        assert_same_shapes(self.inputs, self.input_grads)\n",
    "        return self.input_grads\n",
    "\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "        \n",
    "        if len(output_grads) == 1:\n",
    "            self.outputs.backward(output_grads)\n",
    "        else: \n",
    "            for out, grad in zip(self.outputs, output_grads):\n",
    "                out.backward(gradient=grad, retain_graph=True)\n",
    "\n",
    "        input_grads = tuple()\n",
    "        for inp in self.inputs_with_grad:\n",
    "            input_grads = input_grads + (inp.grad,)\n",
    "        \n",
    "        return input_grads\n",
    "\n",
    "    \n",
    "    def _param_grads(self) -> List[Tensor]:\n",
    "        return tuple(param.grad for param in self.params_with_grad)\n",
    "\n",
    "    \n",
    "    def _params(self) -> None:\n",
    "        return tuple(param.data for param in self.params_with_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToyExample class, inheriting from AutogradBlock. \n",
    "# Analogous to a custom Layer\n",
    "class ToyExample(AutogradBlock):\n",
    "   \n",
    "    def __init__(self, \n",
    "                 seed=12345):\n",
    "        super().__init__()\n",
    "        self.seed = seed\n",
    "    \n",
    "        \n",
    "    def _setup_block(self) -> Tuple[Tensor]:\n",
    "\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        self.ops['cp'] = Copy()\n",
    "        self.ops['add2'] = Add2()\n",
    "        self.ops['mul'] = Multiply()\n",
    "        \n",
    "        self.params['w1'] = torch.rand(2,2)\n",
    "        self.ops['wm1'] = WeightMultiply(self.params['w1'])\n",
    "        \n",
    "        self.params['w2'] = torch.rand(2,2)\n",
    "        self.ops['wm2'] = WeightMultiply(self.params['w2'])\n",
    "        \n",
    "        self.params['b1'] = torch.rand(1,2)\n",
    "        self.ops['ba1'] = BiasAdd(self.params['b1']) \n",
    "        \n",
    "        self.params['b2'] = torch.rand(1,2)\n",
    "        self.ops['ba2'] = BiasAdd(self.params['b2'])\n",
    "\n",
    "        \n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        \n",
    "        a, b = self.inputs_with_grad \n",
    "        a1, a2 = self.ops['cp'].forward(a) \n",
    "        c1 = self.ops['ba1'].forward(self.ops['wm1'].forward(a1))\n",
    "        c2 = self.ops['ba2'].forward(self.ops['wm2'].forward(a2))\n",
    "        d = self.ops['add2'].forward(c1, c2)\n",
    "        \n",
    "        return self.ops['mul'].forward(b, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate\n",
    "t = ToyExample(seed=102218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new input tensors\n",
    "torch.manual_seed(102218)\n",
    "a = torch.rand(2,2)\n",
    "b = torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.0692, 0.9283],\n",
      "        [1.0894, 0.8928]], grad_fn=<ThMulBackward>)\n",
      "tensor(5.9798, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "out = t.forward(a, b)\n",
    "print(out)\n",
    "print(torch.sum(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad # no grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.2304, 1.3354],\n",
       "         [1.4560, 1.0821]]), tensor([[3.2847, 2.0140],\n",
       "         [2.2020, 1.7442]]))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sending gradient backwards\n",
    "t.backward(torch.ones_like(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad # still no grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0462, 0.5658],\n",
      "        [0.2438, 0.8881]])\n"
     ]
    }
   ],
   "source": [
    "# new version of a\n",
    "a_new = a.clone()\n",
    "a_new[0][0] += 0.1\n",
    "print(a_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.2449, 0.9756],\n",
      "        [1.0894, 0.8928]], grad_fn=<ThMulBackward>)\n",
      "tensor(6.2028, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# printing what output should be\n",
    "out_new = t.forward(a_new, b)\n",
    "print(out_new)\n",
    "print(torch.sum(out_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.20284"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted value\n",
    "# old_sum + 0.1 * gradient\n",
    "5.9798 + 0.1 * 2.2304"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing `param_grads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0048, 0.5609],\n",
       "         [0.9681, 0.7154]]), tensor([[1.0048, 0.5609],\n",
       "         [0.9681, 0.7154]]), tensor([[1.4291, 0.9728]]), tensor([[1.4291, 0.9728]]))"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have grads\n",
    "t.param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9462, 0.5658],\n",
      "        [0.2438, 0.8881]])\n",
      "tensor([[0.9344, 0.4609],\n",
      "        [0.4948, 0.5119]])\n",
      "tensor([[0.3988, 0.1799]])\n",
      "tensor([[0.6886, 0.0705]])\n"
     ]
    }
   ],
   "source": [
    "# we have grads\n",
    "for val in t.params.values():\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs and beyond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Node will be an `AutogradBlock` with three inputs and three outputs. \n",
    "\n",
    "LSTM Layer, on the other hand..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM using `AutogradBlock`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `LSTMLayer`\n",
    "* Will have params.\n",
    "* Instead of `operations` will have `AutogradBlocks`\n",
    "\n",
    "#### `LSTMNode`\n",
    "* `AutogradBlock`\n",
    "* `forward` method will take in `params` as an argument\n",
    "* `backward` method will produce `input_grads`, and also need to increment the overall `param_grads`\n",
    "    * Latter should be ok as long as we have `requires_grad = True` for the params passed into the `Node`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMNode series of operations\n",
    "class LSTMNode(AutogradBlock):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden_size: int,\n",
    "                 vocab_size: int,\n",
    "                 seed: int=12345):\n",
    "\n",
    "        super().__init__()\n",
    "        self.seed = seed\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        \n",
    "    def forward(self, \n",
    "                lstm_params: Dict[str, Tensor],\n",
    "                *inputs) -> Tuple[Tensor]:\n",
    "\n",
    "        if self.first:\n",
    "            self._setup_block(lstm_params)\n",
    "            self.first = False\n",
    "            \n",
    "        self.inputs = inputs\n",
    "        \n",
    "        self.inputs_with_grad = self._inputs_autograd()\n",
    "        \n",
    "        self.params_with_grad = self._params_autograd()\n",
    "        self._gradify_operations()\n",
    "\n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "    \n",
    "    def backward(self, \n",
    "                 lstm_params: Dict[str, Tensor],\n",
    "                 *output_grads) -> Tuple[Tensor]:\n",
    "\n",
    "        assert_same_shapes(self.outputs, output_grads)\n",
    "\n",
    "        self.input_grads = self._input_grads(output_grads)\n",
    "\n",
    "        if self.params:\n",
    "            self.param_grads = self._param_grads()\n",
    "\n",
    "        assert_same_shapes(self.inputs, self.input_grads)\n",
    "        return self.input_grads\n",
    "\n",
    "    \n",
    "    def _setup_block(self,\n",
    "                     lstm_params: Dict[str, Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        self.ops['con'] = Concat2()\n",
    "        self.ops['copy'] = Copy(4)\n",
    "        self.ops['sig1'] = Sigmoid()\n",
    "        self.ops['sig2'] = Sigmoid()\n",
    "        self.ops['sig3'] = Sigmoid()\n",
    "        self.ops['tan1'] = Tanh()\n",
    "        self.ops['tan2'] = Tanh()\n",
    "        self.ops['mul1'] = Multiply()\n",
    "        self.ops['mul2'] = Multiply()\n",
    "        self.ops['mul3'] = Multiply()\n",
    "        self.ops['add1'] = Add2()\n",
    "        self.ops['add2'] = Add2()\n",
    "#         import pdb; pdb.set_trace()\n",
    "        self.ops['Wf'] = WeightMultiply(lstm_params['Wf'])\n",
    "        self.ops['Bf'] = BiasAdd(lstm_params['Bf'])\n",
    "        \n",
    "        self.ops['Wi'] = WeightMultiply(lstm_params['Wi'])\n",
    "        self.ops['Bi'] = BiasAdd(lstm_params['Bi'])\n",
    "        \n",
    "        self.ops['Wc'] = WeightMultiply(lstm_params['Wc'])\n",
    "        self.ops['Bc'] = BiasAdd(lstm_params['Bc'])\n",
    "\n",
    "        self.ops['Wo'] = WeightMultiply(lstm_params['Wo'])\n",
    "        self.ops['Bo'] = BiasAdd(lstm_params['Bo'])        \n",
    "\n",
    "        self.ops['Wv'] = WeightMultiply(lstm_params['Wv'])\n",
    "        self.ops['Bv'] = BiasAdd(lstm_params['Bv'])  \n",
    "        \n",
    "        \n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        \n",
    "        X_in, H_in, C_in = self.inputs_with_grad \n",
    "        Z = self.ops['con'].forward(X_in, H_in)\n",
    "        z1, z2, z3, z4 = self.ops['copy'].forward(Z) \n",
    "#         import pdb; pdb.set_trace()\n",
    "        F = self.ops['Wf'].forward(z1)\n",
    "        F = self.ops['Bf'].forward(F)\n",
    "        F_out = self.ops['sig1'].forward(F)\n",
    "\n",
    "        I = self.ops['Wi'].forward(z2)\n",
    "        I = self.ops['Bi'].forward(I)\n",
    "        I_out = self.ops['sig2'].forward(I)\n",
    "        \n",
    "        C = self.ops['Wc'].forward(z3)\n",
    "        C = self.ops['Bc'].forward(C)\n",
    "        C_bar = self.ops['tan1'].forward(C)\n",
    "        \n",
    "        c1 = self.ops['mul1'].forward(F_out, C_in)\n",
    "        c2 = self.ops['mul2'].forward(I_out, C_bar)\n",
    "        \n",
    "        C_out = self.ops['add1'].forward(c1, c2)\n",
    "\n",
    "        O = self.ops['Wo'].forward(z4)\n",
    "        O = self.ops['Bo'].forward(O)\n",
    "        O_out = self.ops['sig3'].forward(O)\n",
    "        \n",
    "        C_tan = self.ops['tan2'].forward(c_new)\n",
    "        H_out = self.ops['mul3'].forward(O_out, C_tan)\n",
    "\n",
    "        X = self.ops['Wv'].forward(H_out)\n",
    "        X_out = self.ops['Bv'].forward(X)\n",
    "        print(H_out[:5, :5])\n",
    "        return X_out, H_out, C_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMLayer class - series of operations\n",
    "class LSTMLayer(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_len: int,\n",
    "                 vocab_size: int,\n",
    "                 hidden_size: int = 100):\n",
    "        super().__init__()\n",
    "        self.nodes = [LSTMNode(hidden_size, vocab_size) for _ in range(max_len)]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.first: bool = True\n",
    "        self.start_H: Tensor = None\n",
    "        self.start_C: Tensor = None\n",
    "        self.params: Dict[Tensor] = {}\n",
    "        \n",
    "    \n",
    "    def _init_params(self, input_: Tensor) -> Tensor:\n",
    "        '''\n",
    "        First dimension of input_ will be batch size\n",
    "        '''\n",
    "        self.start_H = torch.zeros(input_.shape[0], self.hidden_size)\n",
    "        self.start_C = torch.zeros(input_.shape[0], self.hidden_size)\n",
    "        self.params['Wf'] = torch.rand(self.hidden_size + self.vocab_size, \n",
    "                                       self.hidden_size)\n",
    "        self.params['Bf'] = torch.rand(1, self.hidden_size)        \n",
    "\n",
    "        self.params['Wi'] = torch.rand(self.hidden_size + self.vocab_size, \n",
    "                                       self.hidden_size)\n",
    "        self.params['Bi'] = torch.rand(1, self.hidden_size)\n",
    "        \n",
    "        self.params['Wc'] = torch.rand(self.hidden_size + self.vocab_size, \n",
    "                                       self.hidden_size)\n",
    "        self.params['Bc'] = torch.rand(1, self.hidden_size)\n",
    "    \n",
    "        self.params['Wo'] = torch.rand(self.hidden_size + self.vocab_size, \n",
    "                                       self.hidden_size)\n",
    "        self.params['Bo'] = torch.rand(1, self.hidden_size)\n",
    "\n",
    "        self.params['Wv'] = torch.rand(self.hidden_size, \n",
    "                                       self.vocab_size)\n",
    "        self.params['Bv'] = torch.rand(1, self.vocab_size)\n",
    "\n",
    "        for param in self.params.values():\n",
    "            param.requires_grad = True        \n",
    "\n",
    "            \n",
    "    def _zero_param_grads(self) -> None:\n",
    "        for param in self.params.values():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.zero_() \n",
    "\n",
    "    \n",
    "    def forward(self, input_: Tensor) -> Tensor:\n",
    "        if self.first:\n",
    "            self._init_params(input_)\n",
    "            self.first = False        \n",
    "    \n",
    "        # shape: batch size by sequence length by vocab_size\n",
    "        self.input_ = input_\n",
    "        \n",
    "        H_in = torch.clone(self.start_H)\n",
    "        C_in = torch.clone(self.start_C)\n",
    "\n",
    "        self.output = torch.zeros_like(self.input_)\n",
    "        \n",
    "        seq_len = self.input_.shape[1]\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            \n",
    "            # pass info forward through the nodes \n",
    "            elem_out, H_in, C_in = self.nodes[i].forward(self.params, self.input_[:, i, :], \n",
    "                                                         H_in, C_in)\n",
    "            \n",
    "            # define output to be \n",
    "            self.output[:, i, :] = elem_out\n",
    "            \n",
    "        self.start_H = H_in\n",
    "        self.start_C = C_in\n",
    "            \n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, output_grad: Tensor) -> Tensor:\n",
    "        \n",
    "#         self._zero_param_grads()\n",
    "        \n",
    "        dH_next = torch.zeros_like(self.start_H)\n",
    "        dC_next = torch.zeros_like(self.start_C)\n",
    "\n",
    "        self.input_grad = torch.zeros_like(self.input_)\n",
    "\n",
    "        for i in reversed(range(self.input_.shape[1])):\n",
    "\n",
    "            # pass info forward through the nodes \n",
    "            grad_out, dH_next, dC_next = \\\n",
    "                self.nodes[i].backward(self.params, output_grad[:, i, :], \n",
    "                                       dH_next, dC_next)\n",
    "            \n",
    "            # define output to be \n",
    "            self.input_grad[:, i, :] = grad_out\n",
    "            \n",
    "        return self.input_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 62])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define input, shape: (batch_size, sequence_length, vocab_size)\n",
    "batch_size = 10\n",
    "sequence_length = 20\n",
    "vocab_size = 62\n",
    "lstm_in = torch.rand(batch_size, sequence_length, vocab_size)\n",
    "lstm_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 62])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_in[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate objects\n",
    "lay = LSTMLayer(sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7616, 0.7616, 0.7616, 0.7616, 0.7616],\n",
      "        [0.7616, 0.7616, 0.7616, 0.7616, 0.7616],\n",
      "        [0.7616, 0.7616, 0.7616, 0.7616, 0.7616],\n",
      "        [0.7616, 0.7616, 0.7616, 0.7616, 0.7616],\n",
      "        [0.7616, 0.7616, 0.7616, 0.7616, 0.7616]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9427, 0.9427, 0.9427, 0.9427, 0.9427],\n",
      "        [0.9427, 0.9427, 0.9427, 0.9427, 0.9427],\n",
      "        [0.9427, 0.9427, 0.9427, 0.9427, 0.9427],\n",
      "        [0.9427, 0.9427, 0.9427, 0.9427, 0.9427],\n",
      "        [0.9427, 0.9427, 0.9427, 0.9427, 0.9427]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9597, 0.9597, 0.9597, 0.9597, 0.9597],\n",
      "        [0.9597, 0.9597, 0.9597, 0.9597, 0.9597],\n",
      "        [0.9597, 0.9597, 0.9597, 0.9597, 0.9597],\n",
      "        [0.9597, 0.9597, 0.9597, 0.9597, 0.9597],\n",
      "        [0.9597, 0.9597, 0.9597, 0.9597, 0.9597]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9611, 0.9611, 0.9611, 0.9611, 0.9611],\n",
      "        [0.9611, 0.9611, 0.9611, 0.9611, 0.9611],\n",
      "        [0.9611, 0.9611, 0.9611, 0.9611, 0.9611],\n",
      "        [0.9611, 0.9611, 0.9611, 0.9611, 0.9611],\n",
      "        [0.9611, 0.9611, 0.9611, 0.9611, 0.9611]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612],\n",
      "        [0.9612, 0.9612, 0.9612, 0.9612, 0.9612]], grad_fn=<SliceBackward>)\n",
      "torch.Size([10, 20, 62])\n"
     ]
    }
   ],
   "source": [
    "# test passing forward\n",
    "lstm_out = lay.forward(lstm_in)\n",
    "print(lstm_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# testing backward \"retain_graph = True\"\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x1 = x + 2\n",
    "x2 = x + 3\n",
    "y = x1 + 2\n",
    "z = x2 + 2\n",
    "y.backward(torch.ones(2, 2))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# does retain_graph=True change anything?\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x1 = x + 2\n",
    "x2 = x + 3\n",
    "y = x1 + 2\n",
    "z = x2 + 2\n",
    "y.backward(torch.ones(2, 2), retain_graph=True)\n",
    "z.backward(torch.ones(2, 2), retain_graph=True) \n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7.7725e-04, 5.7210e-04, 7.5713e-04,  ..., 7.7355e-04,\n",
       "          7.4075e-04, 7.8117e-04],\n",
       "         [5.5032e-20, 4.8205e-20, 5.1460e-20,  ..., 6.4384e-20,\n",
       "          5.4052e-20, 4.8819e-20],\n",
       "         [4.1686e-23, 4.0675e-23, 5.0620e-23,  ..., 5.5260e-23,\n",
       "          3.9072e-23, 4.3622e-23],\n",
       "         ...,\n",
       "         [1.8779e-23, 1.1537e-23, 1.7565e-23,  ..., 1.9291e-23,\n",
       "          1.2771e-23, 1.5281e-23],\n",
       "         [4.1651e-24, 3.5412e-24, 2.5523e-24,  ..., 4.6605e-24,\n",
       "          2.9198e-24, 3.0040e-24],\n",
       "         [9.2731e-24, 1.0888e-23, 1.2496e-23,  ..., 1.3382e-23,\n",
       "          1.1926e-23, 1.0415e-23]],\n",
       "\n",
       "        [[1.2259e-04, 9.2816e-05, 1.0968e-04,  ..., 1.0683e-04,\n",
       "          9.6706e-05, 1.1418e-04],\n",
       "         [6.7575e-19, 6.9795e-19, 8.9514e-19,  ..., 9.2751e-19,\n",
       "          7.5530e-19, 8.5921e-19],\n",
       "         [1.8754e-23, 1.8340e-23, 1.7686e-23,  ..., 2.3172e-23,\n",
       "          1.7370e-23, 1.6557e-23],\n",
       "         ...,\n",
       "         [4.5157e-23, 3.5073e-23, 3.0150e-23,  ..., 4.8005e-23,\n",
       "          2.9974e-23, 3.2274e-23],\n",
       "         [4.1623e-24, 4.2501e-24, 4.7952e-24,  ..., 5.7103e-24,\n",
       "          4.9030e-24, 5.1335e-24],\n",
       "         [1.5713e-23, 1.5355e-23, 1.4715e-23,  ..., 1.9850e-23,\n",
       "          1.3555e-23, 1.3363e-23]],\n",
       "\n",
       "        [[1.6789e-03, 1.3100e-03, 1.7144e-03,  ..., 1.6099e-03,\n",
       "          1.4487e-03, 1.5569e-03],\n",
       "         [1.0527e-20, 1.3105e-20, 1.3372e-20,  ..., 1.5780e-20,\n",
       "          1.3682e-20, 1.3334e-20],\n",
       "         [2.7161e-24, 3.1848e-24, 3.2208e-24,  ..., 3.6510e-24,\n",
       "          3.6592e-24, 3.4930e-24],\n",
       "         ...,\n",
       "         [3.0747e-24, 2.8357e-24, 3.5522e-24,  ..., 3.8580e-24,\n",
       "          2.8297e-24, 3.1510e-24],\n",
       "         [3.2039e-23, 2.4723e-23, 3.6424e-23,  ..., 3.7070e-23,\n",
       "          2.6583e-23, 3.1771e-23],\n",
       "         [1.0648e-22, 8.4372e-23, 7.4909e-23,  ..., 1.2115e-22,\n",
       "          7.3750e-23, 8.1943e-23]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[5.1251e-04, 3.3702e-04, 5.2882e-04,  ..., 4.5152e-04,\n",
       "          3.9778e-04, 4.7368e-04],\n",
       "         [5.4410e-21, 5.0804e-21, 5.0824e-21,  ..., 6.7623e-21,\n",
       "          5.4115e-21, 4.9014e-21],\n",
       "         [4.2663e-23, 3.3126e-23, 2.5827e-23,  ..., 4.3550e-23,\n",
       "          2.3629e-23, 2.8663e-23],\n",
       "         ...,\n",
       "         [2.8020e-24, 2.5469e-24, 2.5289e-24,  ..., 3.2078e-24,\n",
       "          2.1074e-24, 2.7416e-24],\n",
       "         [1.3355e-23, 1.6796e-23, 1.5996e-23,  ..., 2.0735e-23,\n",
       "          1.7619e-23, 1.6544e-23],\n",
       "         [9.9664e-24, 9.8014e-24, 1.0515e-23,  ..., 1.2513e-23,\n",
       "          9.8333e-24, 9.6499e-24]],\n",
       "\n",
       "        [[1.9178e-03, 1.6871e-03, 1.8783e-03,  ..., 1.9675e-03,\n",
       "          1.6951e-03, 1.9978e-03],\n",
       "         [5.2869e-19, 5.5000e-19, 5.2402e-19,  ..., 5.3942e-19,\n",
       "          3.9562e-19, 5.2977e-19],\n",
       "         [5.1188e-23, 4.4946e-23, 6.4396e-23,  ..., 5.8967e-23,\n",
       "          5.1384e-23, 5.4730e-23],\n",
       "         ...,\n",
       "         [1.6883e-23, 1.5446e-23, 1.6384e-23,  ..., 2.0816e-23,\n",
       "          1.4943e-23, 1.5178e-23],\n",
       "         [3.1971e-24, 3.2209e-24, 3.4539e-24,  ..., 3.9429e-24,\n",
       "          3.7120e-24, 3.3021e-24],\n",
       "         [1.1460e-23, 1.1876e-23, 1.1157e-23,  ..., 1.3868e-23,\n",
       "          9.8836e-24, 1.1061e-23]],\n",
       "\n",
       "        [[2.6721e-04, 1.7619e-04, 2.7318e-04,  ..., 2.3288e-04,\n",
       "          1.8856e-04, 2.3879e-04],\n",
       "         [2.7925e-20, 3.0012e-20, 2.8901e-20,  ..., 4.0664e-20,\n",
       "          3.2846e-20, 2.9603e-20],\n",
       "         [6.6880e-23, 7.4346e-23, 8.3056e-23,  ..., 9.1774e-23,\n",
       "          6.6187e-23, 7.3813e-23],\n",
       "         ...,\n",
       "         [2.1587e-24, 2.2698e-24, 2.1016e-24,  ..., 2.4514e-24,\n",
       "          1.5748e-24, 2.1588e-24],\n",
       "         [3.3903e-24, 3.1860e-24, 3.2785e-24,  ..., 4.2712e-24,\n",
       "          2.9619e-24, 3.4516e-24],\n",
       "         [4.5470e-24, 5.6408e-24, 4.8782e-24,  ..., 6.5619e-24,\n",
       "          5.5378e-24, 4.8563e-24]]])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test backward\n",
    "lstm_grad = torch.ones_like(lstm_out)\n",
    "lay.backward(lstm_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([162, 100])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay.params['Wf'].grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LSTM Layer` \n",
    "\n",
    "Input: series of word embeddings, cell state.\n",
    "\n",
    "Output: series of embeddings of the same size as the word, hidden state.\n",
    "\n",
    "Cell state will be an attribute of the layer.\n",
    "\n",
    "LSTM layer will have an additional function \"reset state\" that resets the hidden state to zero.\n",
    "\n",
    "## `LSTM Node` \n",
    "\n",
    "Each LSTM layer will have a series of a special kind of operation, `LSTM Node`. These nodes will have a series of special operations: they will take in as input an embedding and a hidden state and pass out an embedding and an updated hidden state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations of LSTM Node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Receives as input:\n",
    "\n",
    "* X (batch size x embedding dim)\n",
    "* H_prev (batch size x hidden dim)\n",
    "* C_prev (batch size x hidden dim)\n",
    "\n",
    "Diff components:\n",
    "\n",
    "Create \"Z\"\n",
    "Copy \"Z\"\n",
    "\n",
    "\n",
    "1. Z = Concat(X, H)\n",
    "1. Z1, Z2, Z3, Z4 = Copy(Z)\n",
    "1. F = WeightMultiply(Z1, W_f)\n",
    "1. F = BiasAdd(F, B_f)\n",
    "1. F_out = Sigmoid(F)\n",
    "1. I = WeightMultiply(Z2, W_i)\n",
    "1. I = BiasAdd(I, B_i)\n",
    "1. I_out = Sigmoid(I)\n",
    "1. C = WeightMultiply(Z3, W_c)\n",
    "1. C = BiasAdd(C, B_c)\n",
    "1. C_bar = Tanh(C, B_c)\n",
    "1. C1 = Multiply(F_out, C_prev)\n",
    "1. C2 = Multiply(I_out, C_bar)\n",
    "1. C_new = Add(C1, C2)\n",
    "1. O = WeightMutiply(Z4, W_o)\n",
    "1. O = Add(O, B_o)\n",
    "1. O_out = Sigmoid(O)\n",
    "1. C_tan = Tanh(C_new)\n",
    "1. H_new = Multiply(O_out, C_tan)\n",
    "1. H_out = WeightMultiply(H_new, W_v)\n",
    "1. X_out = BiasAdd(H_out, B_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues:\n",
    "\n",
    "* Concat operations needs to produce four distinct outputs. \n",
    "    * Potential solution: `Copy` operation that sums gradients.\n",
    "\n",
    "* Need a way to handle branching:\n",
    "* After `Copy`, there will be four `forward` operations happening.\n",
    "* Can't write `for operation in self.operations: operation.forward(X)`\n",
    "    * Maybe I can\n",
    "\n",
    "That's it!        \n",
    "1. Parameters are initialized at the `LSTMLayer` level. \n",
    "2. `LSTMNode` has a series of `ParamOperation`s.\n",
    "3. At the beginning of each operation's `forward` and `backward` methods, the nodes get parameters from the `LSTMLayer`.\n",
    "4. On the backward pass, they:\n",
    "    * Provide `param_grad`s for each `ParamOperation`.\n",
    "    * `LSTMNode` will need a function to return all of its `param_grad`s.\n",
    "5. The `LSTMLayer` can get the `param_grad`s from all of the `LSTMNode`s, aggregate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNode(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, \n",
    "                X_input: Tensor, \n",
    "                H_in: Tensor,\n",
    "                C_in: Tensor, \n",
    "                params: Dict[Tensor]):\n",
    "        \n",
    "        operations_1 = [\n",
    "            Concat(X_input, H_in),\n",
    "            Copy(4), \n",
    "        ] \n",
    "        \n",
    "        operations_2 = [\n",
    "            WeightMultiply(params['W_f']),\n",
    "            BiasAdd(params['B_f']),\n",
    "            Sigmoid()\n",
    "        ] # outputs F_out\n",
    "        \n",
    "        operations_3 = [\n",
    "            WeightMultiply(params['W_i']),\n",
    "            BiasAdd(params['B_i']),\n",
    "            Sigmoid()\n",
    "        ] # outputs I_out\n",
    "        \n",
    "        operations_4 = [\n",
    "            WeightMultiply(params['W_c']),\n",
    "            BiasAdd(params['B_c']),\n",
    "            Tanh()\n",
    "        ] # outputs C_bar\n",
    "        \n",
    "        operations_5 = [\n",
    "            Multiply(), # outputs C1\n",
    "            Multiply()\n",
    "        ]\n",
    "\n",
    "        operation_blocks = [\n",
    "            [operations_1, \n",
    "             [operations_2, operations_3]\n",
    "             , operations_4, oper]\n",
    "            \n",
    "        ]\n",
    "        self.input = input\n",
    "\n",
    "        self.output = self._output()\n",
    "\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, output_grad: Tensor) -> Tensor:\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self._compute_grads(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "\n",
    "    def _compute_grads(self, output_grad: Tensor) -> Tensor:\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> Tensor:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self, output_grad: Tensor) -> Tensor:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Z, Z = Concat(X, H_in) # Concat takes in two inputs produces one output, will have X_grad and H_in_grad\n",
    "\n",
    "Z_1, Z_2, Z_3 = Copy(Z, 3)\n",
    "\n",
    "F = WeightMultiply(Z_1, W_f) \n",
    "F = BiasAdd(F, B_f)\n",
    "F_out = Sigmoid(F)\n",
    "\n",
    "I = WeightMultiply(Z_2, W_i) \n",
    "I = BiasAdd(I, B_i)\n",
    "I_out = Sigmoid(I)\n",
    "\n",
    "C = WeightMultiply(Z_3, W_c)\n",
    "C = BiasAdd(C, B_c)\n",
    "C_bar = Tanh(C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations are the smallest units that have inputs and outputs defined by having a input and output.\n",
    "\n",
    "Going to need to build operations that:\n",
    "\n",
    "* Can take in one input and produce multiple outputs\n",
    "* If it produces multiple outputs, _gradients will need to accumulate_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Divide two inputs into:\n",
    "    * `A`, `B`\n",
    "    * `A` gets copied into `A1` and `A2`\n",
    "    * `A1` and `A2` each get multiplied, bias added, activation, to create `C1` and `C2`.\n",
    "    * These get added together to create `D`.\n",
    "    * `B` and `D` get multiplied to create `E`. \n",
    "    \n",
    "Then the question is: what is the gradient of `E` with respect to `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult(*args):\n",
    "    num = 1\n",
    "    print(type(args))\n",
    "    for arg in args:\n",
    "        num *= arg\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shapes(tensors: Tuple[Tensor],\n",
    "                       tensor_grads: Tuple[Tensor]):\n",
    "    for tensor, tensor_grad in zip(tensors, tensor_grads):\n",
    "        assert tensor.shape == tensor_grad.shape, \\\n",
    "        '''\n",
    "        Two tensors should have the same shape; instead, first Tensor's shape is {0}\n",
    "        and second Tensor's shape is {1}.\n",
    "        '''.format(tuple(tensor_grad.shape), tuple(tensor.shape))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New `Operation` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, *inputs) -> Tuple[Tensor]:\n",
    "\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "\n",
    "    def backward(self, *output_grads) -> Tuple[Tensor]:\n",
    "#         import pdb; pdb.set_trace()\n",
    "        assert_same_shapes(self.outputs, output_grads)\n",
    "\n",
    "        self._compute_grads(output_grads)\n",
    "\n",
    "        assert_same_shapes(self.inputs, self.input_grads)\n",
    "        return self.input_grads\n",
    "\n",
    "\n",
    "    def _compute_grads(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "        \n",
    "        self.input_grads = self._input_grads(output_grads)\n",
    "\n",
    "        return self.input_grads\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grads(self, *output_grads) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([1,2])\n",
    "b = Tensor([3,4])\n",
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Multiply`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiply(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise multiplication\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        return (self.inputs[0] * self.inputs[1],)\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        return self.inputs[1] * output_grads[0],\\\n",
    "               self.inputs[0] * output_grads[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Multiply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(m.forward(a,b))\n",
    "print(m.backward(Tensor([1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Add2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add2(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise addition\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        return (self.inputs[0] + self.inputs[1],)\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        return output_grads[0], output_grads[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add = Add2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add.forward(a,b))\n",
    "print(add.backward(torch.ones_like(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Concat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([[1,2],[3,4]])\n",
    "b = Tensor([[5,6],[7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.cat([a,b], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.split(c, [2, 2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(inp.shape[1] for inp in (a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concat2(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise multiplication\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        self.input_shapes = [inp.shape[1] for inp in self.inputs]\n",
    "        \n",
    "        return (torch.cat(list(self.inputs), dim=1), )\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "        return torch.split(output_grads[0], \n",
    "                           self.input_shapes,\n",
    "                           dim=1)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = Copy()\n",
    "out = cp.forward(a)\n",
    "cp.backward(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Divide two inputs into:\n",
    "    * `A`, `B`\n",
    "    * `A` gets copied into `A1` and `A2`\n",
    "    * `A1` and `A2` each get multiplied, bias added, activation, to create `C1` and `C2`.\n",
    "    * These get added together to create `D`.\n",
    "    * `B` and `D` get multiplied to create `E`. \n",
    "    \n",
    "Then the question is: what is the gradient of `E` with respect to `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderedDict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_1 = [Copy()]\n",
    "ops_2 = [[[WeightMultiply(w1), BiasAdd(b1), Sigmoid()],\n",
    "          [WeightMultiply(w1), BiasAdd(b1), Sigmoid()]],\n",
    "         Add2()]\n",
    "ops_3 = [Multiply()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_op(ops_1, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three cases:\n",
    "\n",
    "* If `op` is an operation, simply call forward.\n",
    "* If `op` is a list and `op[0]` is an operation, call `forward_op` on all the operations in `op`\n",
    "* If `op` is a list and `op[0]` is a list, call `forward_op` on `op` and `inputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_op(ops, *inputs):\n",
    "    out = inputs\n",
    "    for op in ops:\n",
    "        if isinstance(op, list):\n",
    "            if isinstance(op[0], list):\n",
    "                out = tuple(forward_op(operation, inp) for operation, inp in zip(op, out))\n",
    "            else:\n",
    "                \n",
    "\n",
    "            \n",
    "        if len(out) == 1:\n",
    "            out = op.forward(out[0])\n",
    "        else:\n",
    "            out = op.forward(out)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If type of op is list, that means we want to apply the first element of the list to the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_op(ops_2, forward_op(ops_1, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "e = forward_op(ops_3, b, forward_op(ops_2, forward_op(ops_1, a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult(start, *args):\n",
    "    res = start\n",
    "    for num in args:\n",
    "        res *= num\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now how to do the backward pass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "\n",
    "    def __init__(self, param: Tensor) -> Tensor:\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "\n",
    "    def _compute_grads(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "        self.input_grad = self._input_grad(output_grads)\n",
    "        self.param_grad = self._param_grad(output_grads)\n",
    "\n",
    "\n",
    "    def _param_grad(self, output_grad: Tensor) -> Tensor:\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "\n",
    "    def __init__(self, W: Tensor):\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _outputs(self) -> Tensor:\n",
    "        return (torch.mm(self.inputs[0], self.param), )\n",
    "\n",
    "    def _input_grads(self, output_grad: Tensor) -> Tensor:\n",
    "        return (torch.mm(output_grad[0], self.param.transpose(0, 1)), )\n",
    "\n",
    "    def _param_grad(self, output_grad: Tensor) -> Tensor:\n",
    "        return torch.mm(self.inputs[0].transpose(0, 1), output_grad[0])\n",
    "\n",
    "\n",
    "class BiasAdd(ParamOperation):\n",
    "\n",
    "    def __init__(self,\n",
    "                 B: Tensor):\n",
    "        super().__init__(B)\n",
    "\n",
    "\n",
    "    def _outputs(self) -> Tensor:\n",
    "        return (torch.add(self.inputs[0], self.param), )\n",
    "\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "        return (torch.ones_like(self.inputs[0]) * output_grad[0], )\n",
    "\n",
    "\n",
    "    def _param_grad(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "        param_grad = torch.ones_like(self.param) * output_grad[0]\n",
    "        return torch.sum(param_grad, dim=0).reshape(1, param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    '''\n",
    "    Sigmoid activation function\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        return (1.0/(1.0+torch.exp(-1.0 * self.input)), )\n",
    "\n",
    "\n",
    "    def _input_grad(self, output_grad: Tensor) -> Tensor:\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return (input_grad, )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Sigmoid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: copy, and then weight multiply each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101218)\n",
    "op1 = Copy()\n",
    "w1, w2 = torch.rand(2,2), torch.rand(2,2)\n",
    "b1, b2 = torch.rand(1,2), torch.rand(1,2)\n",
    "op2, op3 = WeightMultiply(w1), WeightMultiply(w2)\n",
    "ops2 = [op2, op3]\n",
    "stages = [[op1], ops2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_in = (a,)\n",
    "for stage in stages:\n",
    "    stage_out = tuple()\n",
    "    for i, op in enumerate(stage):\n",
    "        stage_out = stage_out + op.forward(stage_in[i])\n",
    "    stage_in = stage_out\n",
    "final_out = stage_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ones_like_multiple(output: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "    return tuple(torch.ones_like(tensor) for tensor in output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops = [[[WeightMultiply(w1), BiasAdd(b1), Sigmoid()], \n",
    "             [WeightMultiply(w1), BiasAdd(b1), Sigmoid()]], Add2()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: full_ops[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a list of lists. It has length 2. \n",
    "\n",
    "That means ultimately, we want the first input to go through the first argument, and the second input to go through the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a non-nested list. This signals: take the first argument, and feed it through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have another non-nested list. We must similarly: take the second argument, and feed it through. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll have two outputs, at the end of `full_ops[0]`. We move on the `full_ops[1]`, which indeed is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nested_list(nested_list):\n",
    "    for el in nested_list:\n",
    "        if isinstance(el, list):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual walkthrough: `full_ops_advanced`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops_advanced = [Copy(), \n",
    "  [[WeightMultiply(w1), BiasAdd(b1), Sigmoid()], \n",
    "   [WeightMultiply(w1), BiasAdd(b1), Sigmoid()]],\n",
    "  Add2()],\n",
    " Multiply()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops_advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a list of length 2. This means: pass the first element to the first branch, and the second element to the second branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops_advanced[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still a nested list. However, the first element is an operation. So, we want to start by feeding this element through the `Operation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops_advanced[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outputs two things, and indeed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops_advanced[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is of length two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have:\n",
    "\n",
    "* If the first element in the list is an operation, feed the input through it, and keep doing so until you reach an element in the list that is not an `Operation`.\n",
    "* At some point, you'll have outputs from the `Operation`s, and another (possibly nested) list of operations to deal with. At this point, you should call the main function again, with the `output`s from the first part, and the new operations. This function should return a number of outputs equal to the length of the list.\n",
    "* At this point, we'll have two outputs, and we'll move on to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops_advanced[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take the outputs from the prior section and feed them through this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we reach another function. Since this function is next in the list, our code will have to assume that it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This nested list structure cannot support branching. Need to try another approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ops_advanced[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# New approach: using PyTorch's `autograd` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
