{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History\n",
    "\n",
    "This was an attempt to build an LSTM using the `ParamOperation` material from the first three chapters of `lincoln`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Lincoln to system path\n",
    "import sys\n",
    "sys.path.append(\"/Users/seth/development/lincoln/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "from typing import Tuple, Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operation with multiple inputs, gradients\n",
    "class Operation(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, *inputs) -> Tuple[Tensor]:\n",
    "\n",
    "        if len(inputs) == 1:\n",
    "        \n",
    "            self.inputs = inputs[0]\n",
    "        \n",
    "\n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "\n",
    "    def backward(self, *output_grads) -> Tuple[Tensor]:\n",
    "\n",
    "        assert_same_shapes(self.outputs, output_grads)\n",
    "\n",
    "        self._compute_grads(output_grads)\n",
    "\n",
    "        assert_same_shapes(self.inputs, self.input_grads)\n",
    "        return self.input_grads\n",
    "\n",
    "\n",
    "    def _compute_grads(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "\n",
    "\n",
    "        \n",
    "        self.input_grads = self._input_grads(output_grads)\n",
    "\n",
    "        return self.input_grads\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grads(self, *output_grads) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing old Operation definition with new, \"multiple input\" definition\n",
    "class ReLU(Operation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _outputs(self) -> Tensor:\n",
    "        self.output = torch.clamp(self.inputs, 0, 1e5)\n",
    "        return self.output\n",
    "\n",
    "    def _input_grads(self, output_grad: Tensor) -> Tensor:\n",
    "        relu_backward = (self.output > 0).type(self.output.dtype)\n",
    "        return relu_backward * output_grad\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = torch.rand(2,2)\n",
    "r = ReLU()\n",
    "r.forward(m1)\n",
    "r.backward(torch.ones_like(m1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply class\n",
    "class Multiply2(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise multiplication\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        return self.inputs[0] * self.inputs[1]\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        return self.inputs[1] * output_grads[0],\\\n",
    "               self.inputs[0] * output_grads[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert_same_shapes function\n",
    "def assert_same_shapes(tensors: Tuple[Tensor],\n",
    "                       tensor_grads: Tuple[Tensor]):\n",
    "\n",
    "    assert len(tensors) == len(tensor_grads)\n",
    "    \n",
    "    if len(tensors) == 1:\n",
    "        tensors = tensors[0]\n",
    "    if len(tensor_grads) == 1:\n",
    "        tensor_grads = tensor_grads[0]        \n",
    "\n",
    "    for tensor, tensor_grad in zip(tensors, tensor_grads):\n",
    "        assert tensor.shape == tensor_grad.shape, \\\n",
    "        '''\n",
    "        Two tensors should have the same shape; instead, first Tensor's shape is {0}\n",
    "        and second Tensor's shape is {1}.\n",
    "        '''.format(tuple(tensor_grad.shape), tuple(tensor.shape))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inputs to Multiply Operation\n",
    "m = Multiply()\n",
    "torch.manual_seed(102218)\n",
    ", torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8841, 0.2608],\n",
       "        [0.1206, 0.4546]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Multiply (Operation with multiple inputs) forward\n",
    "m.forward(m1, m2) # tuple of length 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9344, 0.4609],\n",
       "         [0.4948, 0.5119]]), tensor([[0.9462, 0.5658],\n",
       "         [0.2438, 0.8881]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test Multiply (Operation with multiple inputs) backward\n",
    "m_grad = torch.ones_like(m1)\n",
    "m.backward(m_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch `Multiply`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9462, 0.5658],\n",
      "        [0.2438, 0.8881]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# m1 initially\n",
    "print(m1)\n",
    "print(m1.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detach to copy values\n",
    "m1_g = m1.detach()\n",
    "m2_g = m2.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require gradients\n",
    "m1_g.requires_grad = True\n",
    "m2_g.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform operation\n",
    "out = m1_g * m2_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define grad\n",
    "mul_grad = torch.ones_like(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send gradient backward\n",
    "out.backward(gradient=mul_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9344, 0.4609],\n",
      "        [0.4948, 0.5119]])\n",
      "tensor([[0.9462, 0.5658],\n",
      "        [0.2438, 0.8881]])\n"
     ]
    }
   ],
   "source": [
    "# same gradients as above!\n",
    "print(m1_g.grad)\n",
    "print(m2_g.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PyTorchOperation` base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Operation class\n",
    "class PyTorchOperation(Operation):\n",
    "\n",
    "    def __init__(self) -> Tensor:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, *inputs) -> Tuple[Tensor]:\n",
    "\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.inputs_with_grad = self._inputs_autograd()\n",
    "        \n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "        \n",
    "        \n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "    def _inputs_autograd(self) -> Tuple[Tensor]:\n",
    "        inputs_with_grad = tuple(inp.detach() for inp in self.inputs)\n",
    "        for inp in inputs_with_grad:\n",
    "            inp.requires_grad = True\n",
    "        return inputs_with_grad\n",
    "        \n",
    "\n",
    "    def _input_grads(self, output_grads: Tensor) -> Tensor:\n",
    "\n",
    "        for out, grad in zip(self.outputs, output_grads):\n",
    "            out.backward(gradient=grad)\n",
    "\n",
    "        input_grads = tuple()\n",
    "        for inp in self.inputs_with_grad:\n",
    "            input_grads = input_grads + (inp.grad,)\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply_PyTorch class\n",
    "class Multiply_PyTorch(PyTorchOperation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        \n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        return self.inputs_with_grad[0] * self.inputs_with_grad[1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Multiply stuff\n",
    "m = Multiply_PyTorch()\n",
    "mp1, mp2 = torch.rand(2,2), torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3988, 0.1799],\n",
      "        [0.6886, 0.0705]]) tensor([[0.3258, 0.4976],\n",
      "        [0.6935, 0.2254]])\n"
     ]
    }
   ],
   "source": [
    "# Print initial tensors\n",
    "print(mp1, mp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1299, 0.0895],\n",
       "         [0.4775, 0.0159]], grad_fn=<ThMulBackward>),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward\n",
    "m.forward(mp1, mp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3258, 0.4976],\n",
       "         [0.6935, 0.2254]]), tensor([[0.3988, 0.1799],\n",
       "         [0.6886, 0.0705]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize grad and backward\n",
    "m.backward(torch.ones_like(mp1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complicated example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Divide two inputs into:\n",
    "    * `A`, `B`\n",
    "    * `A` gets copied into `A1` and `A2`\n",
    "    * `A1` and `A2` each get multiplied, bias added, activation, to create `C1` and `C2`.\n",
    "    * These get added together to create `D`.\n",
    "    * `B` and `D` get multiplied to create `E`. \n",
    "    \n",
    "Then the question is: what is the gradient of `E` with respect to `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lincoln.operations.operations import WeightMultiply\n",
    "from lincoln.operations.operations import BiasAdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define base operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy class - 1 to many\n",
    "class Copy(Operation):\n",
    "   \n",
    "    def __init__(self, num=2):\n",
    "        self.num = num\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise multiplication\n",
    "        '''\n",
    "        output = tuple()\n",
    "        for i in range(self.num):\n",
    "            output = output + self.inputs\n",
    "            \n",
    "        return output\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tensor:\n",
    "        input_grad = torch.zeros_like(output_grads[0])\n",
    "        for grad in output_grads:\n",
    "             input_grad = input_grad + grad\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add2 Operation\n",
    "class Add2(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        '''\n",
    "        Element-wise addition\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        return self.inputs[0] + self.inputs[1]\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        return output_grads[0], output_grads[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat operation - two to one\n",
    "class Concat2(Operation):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _outputs(self) -> Tensor:\n",
    "        '''\n",
    "        Element-wise multiplication\n",
    "        '''\n",
    "        assert len(self.inputs) == 2\n",
    "        \n",
    "        self.input_shapes = [inp.shape[1] for inp in self.inputs]\n",
    "        \n",
    "        return torch.cat(list(self.inputs), dim=1)\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "        return torch.split(output_grads[0], \n",
    "                           self.input_shapes,\n",
    "                           dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# printing \"a\" and \"b\" tensors\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 1.]]), tensor([[1., 1.],\n",
       "         [1., 1.]]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Concat2()\n",
    "out = c.forward(a,b)\n",
    "print(out)\n",
    "c.backward(torch.ones_like(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: incorporate these into Lincoln somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tensors\n",
    "torch.manual_seed(102218)\n",
    "a, b = torch.rand(2,2), torch.rand(2,2)\n",
    "w1, w2 = torch.rand(2,2), torch.rand(2,2)\n",
    "b1, b2 = torch.rand(1,2), torch.rand(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set requires_grad = True for all tensors\n",
    "for t in [a, b, w1, w2, b1, b2]:\n",
    "    t.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define operations\n",
    "c = Copy()\n",
    "add2 = Add2()\n",
    "mul = Multiply()\n",
    "wm1 = WeightMultiply(w1)\n",
    "ba1 = BiasAdd(b1)\n",
    "wm2 = WeightMultiply(w2)\n",
    "ba2 = BiasAdd(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define forward path\n",
    "a1, a2 = c.forward(a)\n",
    "c1 = ba1.forward(wm1.forward(a1))\n",
    "c2 = ba2.forward(wm2.forward(a2))\n",
    "d = add2.forward(c1, c2)\n",
    "e = mul.forward(b, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4230, 1.1577],\n",
       "        [1.2516, 1.0909]], grad_fn=<ThMulBackward>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9232, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(e) # sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing gradient backwards\n",
    "e.backward(gradient=torch.ones_like(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9893, 1.4278],\n",
      "        [0.7053, 0.8353]])\n",
      "tensor([[2.5932, 2.5116],\n",
      "        [2.5298, 2.1312]])\n",
      "tensor([[1.0048, 0.5609],\n",
      "        [0.9681, 0.7154]])\n",
      "tensor([[1.4291, 0.9728]])\n"
     ]
    }
   ],
   "source": [
    "# all grads work\n",
    "print(a.grad)\n",
    "\n",
    "print(b.grad)\n",
    "\n",
    "print(w1.grad)\n",
    "\n",
    "print(b1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that increasing `a[0][0]` by `0.1` will increase the sum from `5.9232` to `5.9232 + 0.1 * 0.9893`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.02213"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define what sum with \"a_new\" should be: \n",
    "5.9232 + 0.1 * 0.9893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper function\n",
    "def sum_with_a(a):\n",
    "    a1, a2 = c.forward(a)\n",
    "    c1 = ba1.forward(wm1.forward(a1))\n",
    "    c2 = ba2.forward(wm2.forward(a2))\n",
    "    d = add2.forward(c1, c2)\n",
    "    e = mul.forward(b, d)\n",
    "    return torch.sum(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0462, 0.5658],\n",
      "        [0.2438, 0.8881]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# define new version of a with one value incremented\n",
    "a_new = a.clone()\n",
    "a_new[0][0] += 0.1\n",
    "print(a_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9232, grad_fn=<SumBackward0>)\n",
      "tensor(6.0222, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test that sum equals expected i.e. that gradients are correct\n",
    "print(sum_with_a(a))\n",
    "print(sum_with_a(a_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning this into a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Layer` class:\n",
    "\n",
    "* Has forward and backward methods. These simply loop through the operations, passing things forwards or backards.\n",
    "* On setup, requires a `num_in`\n",
    "\n",
    "`AutogradBlock` class:\n",
    "\n",
    "* Has forward method, which calls the `_output` method and `_setup` if it is the first iteration.\n",
    "* On setup, requires we define all ops in a `Dict[Operation]`. In addition, we define weights and give them a gradient.\n",
    "* On `_output`, we actually compute the output from the inputs.\n",
    "* We get the `param_grads` in a similar way we get them from `Layer`s - looping through a dictionary now instead of through a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouble is: this solution begs the question: why not write an autograd library and do everything that way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for autograd from scratch:\n",
    "\n",
    "* Could invest a week and unpack the example at `autodiff` in README.\n",
    "   * He makes heavy use of `einsum`, which is its own overhead.\n",
    "* Just use PyTorch (bad - why the operations? Might be ok to illustrate LSTM). \n",
    "\n",
    "Point is to illustrate how things work. Period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lincoln operations\n",
    "from lincoln.operations.operations import WeightMultiply\n",
    "from lincoln.operations.operations import BiasAdd\n",
    "from lincoln.operations.base import ParamOperation\n",
    "from lincoln.operations.activations import Sigmoid, Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutogradBlock: block of operations with multiple inputs and outputs\n",
    "class AutogradBlock(object):\n",
    "\n",
    "    def __init__(self) -> Tensor:\n",
    "        super().__init__()\n",
    "        self.params: Dict[Tensor] = {}\n",
    "        self.param_grads: List[Tensor] = []\n",
    "        self.ops: Dict[Operation] = {}\n",
    "        self.first: bool = True\n",
    "\n",
    "            \n",
    "    def _setup_block(self) -> Tuple[Tensor]:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def forward(self, *inputs) -> Tuple[Tensor]:\n",
    "\n",
    "        if self.first:\n",
    "            self._setup_block()\n",
    "            self.first = False\n",
    "            \n",
    "        self.inputs = inputs\n",
    "        \n",
    "        self.inputs_with_grad = self._inputs_autograd()\n",
    "        \n",
    "        self.params_with_grad = self._params_autograd()\n",
    "        self._gradify_operations()\n",
    "\n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "    def _inputs_autograd(self) -> Tuple[Tensor]:\n",
    "        inputs_with_grad = tuple(inp.detach() for inp in self.inputs)\n",
    "        for inp in inputs_with_grad:\n",
    "            inp.requires_grad = True\n",
    "        return inputs_with_grad\n",
    "\n",
    "    \n",
    "    def _params_autograd(self) -> Tuple[Tensor]:\n",
    "        params_with_grad = tuple(param.detach() for param in self.params.values())\n",
    "        for param in params_with_grad:\n",
    "            param.requires_grad = True\n",
    "        return params_with_grad\n",
    "    \n",
    "\n",
    "    def _gradify_operations(self) -> Tuple[Tensor]:\n",
    "        for op, tensor in zip([op for op in self.ops.values() \n",
    "                               if issubclass(op.__class__, ParamOperation)],\n",
    "                              self.params_with_grad):\n",
    "            setattr(op, \"param\", tensor)\n",
    "    \n",
    "    \n",
    "    def backward(self, *output_grads) -> Tuple[Tensor]:\n",
    "\n",
    "        assert_same_shapes(self.outputs, output_grads)\n",
    "\n",
    "        self.input_grads = self._input_grads(output_grads)\n",
    "        \n",
    "        if self.params:\n",
    "            self.param_grads = self._param_grads()\n",
    "    \n",
    "        assert_same_shapes(self.inputs, self.input_grads)\n",
    "        return self.input_grads\n",
    "\n",
    "\n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _input_grads(self, output_grads: Tuple[Tensor]) -> Tuple[Tensor]:\n",
    "        \n",
    "        if len(output_grads) == 1:\n",
    "            self.outputs.backward(output_grads)\n",
    "        else: \n",
    "            for out, grad in zip(self.outputs, output_grads):\n",
    "                out.backward(gradient=grad, retain_graph=True)\n",
    "\n",
    "        input_grads = tuple()\n",
    "        for inp in self.inputs_with_grad:\n",
    "            input_grads = input_grads + (inp.grad,)\n",
    "        \n",
    "        return input_grads\n",
    "\n",
    "    \n",
    "    def _param_grads(self) -> List[Tensor]:\n",
    "        return tuple(param.grad for param in self.params_with_grad)\n",
    "\n",
    "    \n",
    "    def _params(self) -> None:\n",
    "        return tuple(param.data for param in self.params_with_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToyExample class, inheriting from AutogradBlock. \n",
    "# Analogous to a custom Layer\n",
    "class ToyExample(AutogradBlock):\n",
    "   \n",
    "    def __init__(self, \n",
    "                 seed=12345):\n",
    "        super().__init__()\n",
    "        self.seed = seed\n",
    "    \n",
    "        \n",
    "    def _setup_block(self) -> Tuple[Tensor]:\n",
    "\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        self.ops['cp'] = Copy()\n",
    "        self.ops['add2'] = Add2()\n",
    "        self.ops['mul'] = Multiply()\n",
    "        \n",
    "        self.params['w1'] = torch.rand(2,2)\n",
    "        self.ops['wm1'] = WeightMultiply(self.params['w1'])\n",
    "        \n",
    "        self.params['w2'] = torch.rand(2,2)\n",
    "        self.ops['wm2'] = WeightMultiply(self.params['w2'])\n",
    "        \n",
    "        self.params['b1'] = torch.rand(1,2)\n",
    "        self.ops['ba1'] = BiasAdd(self.params['b1']) \n",
    "        \n",
    "        self.params['b2'] = torch.rand(1,2)\n",
    "        self.ops['ba2'] = BiasAdd(self.params['b2'])\n",
    "\n",
    "        \n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        \n",
    "        a, b = self.inputs_with_grad \n",
    "        a1, a2 = self.ops['cp'].forward(a) \n",
    "        c1 = self.ops['ba1'].forward(self.ops['wm1'].forward(a1))\n",
    "        c2 = self.ops['ba2'].forward(self.ops['wm2'].forward(a2))\n",
    "        d = self.ops['add2'].forward(c1, c2)\n",
    "        \n",
    "        return self.ops['mul'].forward(b, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate\n",
    "t = ToyExample(seed=102218)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new input tensors\n",
    "torch.manual_seed(102218)\n",
    "a = torch.rand(2,2)\n",
    "b = torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.0692, 0.9283],\n",
      "        [1.0894, 0.8928]], grad_fn=<ThMulBackward>)\n",
      "tensor(5.9798, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "out = t.forward(a, b)\n",
    "print(out)\n",
    "print(torch.sum(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad # no grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.2304, 1.3354],\n",
       "         [1.4560, 1.0821]]), tensor([[3.2847, 2.0140],\n",
       "         [2.2020, 1.7442]]))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sending gradient backwards\n",
    "t.backward(torch.ones_like(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad # still no grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0462, 0.5658],\n",
      "        [0.2438, 0.8881]])\n"
     ]
    }
   ],
   "source": [
    "# new version of a\n",
    "a_new = a.clone()\n",
    "a_new[0][0] += 0.1\n",
    "print(a_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.2449, 0.9756],\n",
      "        [1.0894, 0.8928]], grad_fn=<ThMulBackward>)\n",
      "tensor(6.2028, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# printing what output should be\n",
    "out_new = t.forward(a_new, b)\n",
    "print(out_new)\n",
    "print(torch.sum(out_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.20284"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted value\n",
    "# old_sum + 0.1 * gradient\n",
    "5.9798 + 0.1 * 2.2304"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing `param_grads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0048, 0.5609],\n",
       "         [0.9681, 0.7154]]), tensor([[1.0048, 0.5609],\n",
       "         [0.9681, 0.7154]]), tensor([[1.4291, 0.9728]]), tensor([[1.4291, 0.9728]]))"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have grads\n",
    "t.param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9462, 0.5658],\n",
      "        [0.2438, 0.8881]])\n",
      "tensor([[0.9344, 0.4609],\n",
      "        [0.4948, 0.5119]])\n",
      "tensor([[0.3988, 0.1799]])\n",
      "tensor([[0.6886, 0.0705]])\n"
     ]
    }
   ],
   "source": [
    "# we have grads\n",
    "for val in t.params.values():\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs and beyond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Node will be an `AutogradBlock` with three inputs and three outputs. \n",
    "\n",
    "LSTM Layer, on the other hand..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM using `AutogradBlock`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `LSTMLayer`\n",
    "* Will have params.\n",
    "* Instead of `operations` will have `AutogradBlocks`\n",
    "\n",
    "#### `LSTMNode`\n",
    "* `AutogradBlock`\n",
    "* `forward` method will take in `params` as an argument\n",
    "* `backward` method will produce `input_grads`, and also need to increment the overall `param_grads`\n",
    "    * Latter should be ok as long as we have `requires_grad = True` for the params passed into the `Node`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMNode series of operations\n",
    "class LSTMNode(AutogradBlock):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden_size: int,\n",
    "                 vocab_size: int,\n",
    "                 seed: int=12345):\n",
    "\n",
    "        super().__init__()\n",
    "        self.seed = seed\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        \n",
    "    def forward(self, \n",
    "                lstm_params: Dict[str, Tensor],\n",
    "                *inputs) -> Tuple[Tensor]:\n",
    "\n",
    "        if self.first:\n",
    "            self._setup_block(lstm_params)\n",
    "            self.first = False\n",
    "            \n",
    "        self.inputs = inputs\n",
    "        \n",
    "        self.inputs_with_grad = self._inputs_autograd()\n",
    "        \n",
    "        self.params_with_grad = self._params_autograd()\n",
    "        self._gradify_operations()\n",
    "\n",
    "        self.outputs = self._outputs()\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "    \n",
    "    def backward(self, \n",
    "                 lstm_params: Dict[str, Tensor],\n",
    "                 *output_grads) -> Tuple[Tensor]:\n",
    "\n",
    "        assert_same_shapes(self.outputs, output_grads)\n",
    "\n",
    "        self.input_grads = self._input_grads(output_grads)\n",
    "\n",
    "        if self.params:\n",
    "            self.param_grads = self._param_grads()\n",
    "\n",
    "        assert_same_shapes(self.inputs, self.input_grads)\n",
    "        return self.input_grads\n",
    "\n",
    "    \n",
    "    def _setup_block(self,\n",
    "                     lstm_params: Dict[str, Tensor]) -> Tuple[Tensor]:\n",
    "\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        self.ops['con'] = Concat2()\n",
    "        self.ops['copy'] = Copy(4)\n",
    "        self.ops['sig1'] = Sigmoid()\n",
    "        self.ops['sig2'] = Sigmoid()\n",
    "        self.ops['sig3'] = Sigmoid()\n",
    "        self.ops['tan1'] = Tanh()\n",
    "        self.ops['tan2'] = Tanh()\n",
    "        self.ops['mul1'] = Multiply()\n",
    "        self.ops['mul2'] = Multiply()\n",
    "        self.ops['mul3'] = Multiply()\n",
    "        self.ops['add1'] = Add2()\n",
    "        self.ops['add2'] = Add2()\n",
    "#         import pdb; pdb.set_trace()\n",
    "        self.ops['Wf'] = WeightMultiply(lstm_params['Wf'])\n",
    "        self.ops['Bf'] = BiasAdd(lstm_params['Bf'])\n",
    "        \n",
    "        self.ops['Wi'] = WeightMultiply(lstm_params['Wi'])\n",
    "        self.ops['Bi'] = BiasAdd(lstm_params['Bi'])\n",
    "        \n",
    "        self.ops['Wc'] = WeightMultiply(lstm_params['Wc'])\n",
    "        self.ops['Bc'] = BiasAdd(lstm_params['Bc'])\n",
    "\n",
    "        self.ops['Wo'] = WeightMultiply(lstm_params['Wo'])\n",
    "        self.ops['Bo'] = BiasAdd(lstm_params['Bo'])        \n",
    "\n",
    "        self.ops['Wv'] = WeightMultiply(lstm_params['Wv'])\n",
    "        self.ops['Bv'] = BiasAdd(lstm_params['Bv'])  \n",
    "        \n",
    "        \n",
    "    def _outputs(self) -> Tuple[Tensor]:\n",
    "        \n",
    "        X_in, H_in, C_in = self.inputs_with_grad \n",
    "        Z = self.ops['con'].forward(X_in, H_in)\n",
    "        z1, z2, z3, z4 = self.ops['copy'].forward(Z) \n",
    "#         import pdb; pdb.set_trace()\n",
    "        F = self.ops['Wf'].forward(z1)\n",
    "        F = self.ops['Bf'].forward(F)\n",
    "        F_out = self.ops['sig1'].forward(F)\n",
    "\n",
    "        I = self.ops['Wi'].forward(z2)\n",
    "        I = self.ops['Bi'].forward(I)\n",
    "        I_out = self.ops['sig2'].forward(I)\n",
    "        \n",
    "        C = self.ops['Wc'].forward(z3)\n",
    "        C = self.ops['Bc'].forward(C)\n",
    "        C_bar = self.ops['tan1'].forward(C)\n",
    "        \n",
    "        c1 = self.ops['mul1'].forward(F_out, C_in)\n",
    "        c2 = self.ops['mul2'].forward(I_out, C_bar)\n",
    "        \n",
    "        c_new = self.ops['add1'].forward(c1, c2)\n",
    "\n",
    "        O = self.ops['Wo'].forward(z4)\n",
    "        O = self.ops['Bo'].forward(O)\n",
    "        O_out = self.ops['sig3'].forward(O)\n",
    "        \n",
    "        C_out = self.ops['tan2'].forward(c_new)\n",
    "        \n",
    "        H_out = self.ops['mul3'].forward(O_out, C_out)\n",
    "\n",
    "        X = self.ops['Wv'].forward(H_out)\n",
    "        X_out = self.ops['Bv'].forward(X)\n",
    "\n",
    "        return X_out, H_out, C_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMLayer class - series of operations\n",
    "class LSTMLayer(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_len: int,\n",
    "                 vocab_size: int,\n",
    "                 hidden_size: int = 100):\n",
    "        super().__init__()\n",
    "        self.nodes = [LSTMNode(hidden_size, vocab_size) for _ in range(max_len)]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.first: bool = True\n",
    "        self.start_H: Tensor = None\n",
    "        self.start_C: Tensor = None\n",
    "        self.params: Dict[Tensor] = {}\n",
    "        \n",
    "    \n",
    "    def _init_params(self, input_: Tensor) -> Tensor:\n",
    "        '''\n",
    "        First dimension of input_ will be batch size\n",
    "        '''\n",
    "        self.start_H = torch.zeros(input_.shape[0], self.hidden_size)\n",
    "        self.start_C = torch.zeros(input_.shape[0], self.hidden_size)\n",
    "        self.params['Wf'] = torch.rand(self.hidden_size + self.vocab_size, \n",
    "                                       self.hidden_size)\n",
    "        self.params['Bf'] = torch.rand(1, self.hidden_size)        \n",
    "\n",
    "        self.params['Wi'] = torch.rand(self.hidden_size + self.vocab_size, \n",
    "                                       self.hidden_size)\n",
    "        self.params['Bi'] = torch.rand(1, self.hidden_size)\n",
    "        \n",
    "        self.params['Wc'] = torch.rand(self.hidden_size + self.vocab_size, \n",
    "                                       self.hidden_size)\n",
    "        self.params['Bc'] = torch.rand(1, self.hidden_size)\n",
    "    \n",
    "        self.params['Wo'] = torch.rand(self.hidden_size + self.vocab_size, \n",
    "                                       self.hidden_size)\n",
    "        self.params['Bo'] = torch.rand(1, self.hidden_size)\n",
    "\n",
    "        self.params['Wv'] = torch.rand(self.hidden_size, \n",
    "                                       self.vocab_size)\n",
    "        self.params['Bv'] = torch.rand(1, self.vocab_size)\n",
    "\n",
    "        for param in self.params.values():\n",
    "            param.requires_grad = True        \n",
    "\n",
    "            \n",
    "    def _zero_param_grads(self) -> None:\n",
    "        for param in self.params.values():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.zero_() \n",
    "\n",
    "                \n",
    "    def _params(self) -> Tuple[Tensor]:\n",
    "        return tuple(self.params.values())\n",
    "\n",
    "    \n",
    "    def _param_grads(self) -> Tuple[Tensor]:\n",
    "        return tuple(param.grad for param in self.params.values())    \n",
    "\n",
    "    \n",
    "    def forward(self, input_: Tensor) -> Tensor:\n",
    "        if self.first:\n",
    "            self._init_params(input_)\n",
    "            self.first = False        \n",
    "    \n",
    "        # shape: batch size by sequence length by vocab_size\n",
    "        self.input_ = input_\n",
    "        \n",
    "        H_in = torch.clone(self.start_H)\n",
    "        C_in = torch.clone(self.start_C)\n",
    "\n",
    "        self.output = torch.zeros_like(self.input_)\n",
    "        \n",
    "        seq_len = self.input_.shape[1]\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            \n",
    "            # pass info forward through the nodes \n",
    "            elem_out, H_in, C_in = self.nodes[i].forward(self.params, self.input_[:, i, :], \n",
    "                                                         H_in, C_in)\n",
    "            \n",
    "            self.output[:, i, :] = elem_out\n",
    "            \n",
    "        self.start_H = H_in\n",
    "        self.start_C = C_in\n",
    "\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, output_grad: Tensor) -> Tensor:\n",
    "        \n",
    "#         self._zero_param_grads()\n",
    "        \n",
    "        dH_next = torch.zeros_like(self.start_H)\n",
    "        dC_next = torch.zeros_like(self.start_C)\n",
    "\n",
    "        self.input_grad = torch.zeros_like(self.input_)\n",
    "\n",
    "        for i in reversed(range(self.input_.shape[1])):\n",
    "\n",
    "            # pass info forward through the nodes \n",
    "            grad_out, dH_next, dC_next = \\\n",
    "                self.nodes[i].backward(self.params, output_grad[:, i, :], \n",
    "                                       dH_next, dC_next)\n",
    "\n",
    "            self.input_grad[:, i, :] = grad_out\n",
    "            \n",
    "        return self.input_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 62])"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define input, shape: (batch_size, sequence_length, vocab_size)\n",
    "batch_size = 10\n",
    "sequence_length = 20\n",
    "vocab_size = 62\n",
    "lstm_in = torch.rand(batch_size, sequence_length, vocab_size)\n",
    "lstm_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 62])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_in[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate objects\n",
    "lay = LSTMLayer(sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 62])\n"
     ]
    }
   ],
   "source": [
    "# test passing forward\n",
    "lstm_out = lay.forward(lstm_in)\n",
    "print(lstm_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# testing backward \"retain_graph = True\"\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x1 = x + 2\n",
    "x2 = x + 3\n",
    "y = x1 + 2\n",
    "z = x2 + 2\n",
    "y.backward(torch.ones(2, 2))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test backward\n",
    "lstm_grad = torch.ones_like(lstm_out)\n",
    "lstm_in_grad = lay.backward(lstm_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# params and param grads\n",
    "lp = lay._params()\n",
    "lg = lay._param_grads()\n",
    "\n",
    "print(len(lp))\n",
    "print(len(lg))\n",
    "for p, g in zip(lp, lg):\n",
    "    print(p.shape == g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "\n",
    "* Put this into Lincoln\n",
    "* Make a demo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
