{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frameworks:\n",
    "\n",
    "* PyTorch first pass\n",
    "    * `Model` with `forward` method.\n",
    "    * Manual training loop\n",
    "* PyTorch second pass\n",
    "    * `Model` with `forward` method.\n",
    "    * `Trainer` class that takes in:\n",
    "        * `Model`\n",
    "        * `Optimizer`\n",
    "        * `_Loss`\n",
    "\n",
    "Models:\n",
    "\n",
    "* Boston dataset (used for testing)\n",
    "* Fashion MNIST\n",
    "* AE\n",
    "* GAN\n",
    "* Transformer\n",
    "* NTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from lincoln.utils import permute_data, assert_dim\n",
    "\n",
    "from lincoln.pytorch.model import PyTorchModel\n",
    "from lincoln.pytorch.train import PyTorchTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "X_train, X_test, y_train, y_test = Tensor(X_train), Tensor(X_test), Tensor(y_train), Tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Boston model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BostonModel(\n",
      "  (fc1): Linear(in_features=13, out_features=13, bias=True)\n",
      "  (fc2): Linear(in_features=13, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BostonModel(PyTorchModel):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(13, 13)\n",
    "        self.fc2 = nn.Linear(13, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        assert_dim(x, 2)\n",
    "        \n",
    "        assert x.shape[1] == 13\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = BostonModel()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer, loss\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "epochs = 20\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(X: Tensor,\n",
    "                      y: Tensor,\n",
    "                      size: int = 32) -> Tuple[Tensor]:\n",
    "\n",
    "    N = X.shape[0]\n",
    "\n",
    "    for ii in range(0, N, size):\n",
    "        X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
    "\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 524.85205078125\n",
      "1 478.5111389160156\n",
      "2 420.73828125\n",
      "3 353.9407958984375\n",
      "4 289.292724609375\n",
      "5 233.33729553222656\n",
      "6 189.20748901367188\n",
      "7 154.6373748779297\n",
      "8 127.28923034667969\n",
      "9 112.42050170898438\n",
      "10 103.9015884399414\n",
      "11 97.83414459228516\n",
      "12 94.86502838134766\n",
      "13 92.8217544555664\n",
      "14 92.03717803955078\n",
      "15 91.60162353515625\n",
      "16 91.26419830322266\n",
      "17 91.21491241455078\n",
      "18 91.09342956542969\n",
      "19 91.04370880126953\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "for e in range(epochs):\n",
    "    X_train, y_train = permute_data(X_train, y_train)  \n",
    "\n",
    "    batch_generator = generate_batches(X_train, y_train,\n",
    "                                       batch_size)\n",
    "\n",
    "    for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "\n",
    "        optimizer.zero_grad()   \n",
    "        output = net(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "    \n",
    "    optimizer.zero_grad() \n",
    "    output = net(X_test)\n",
    "    loss = criterion(output, y_test)\n",
    "    print(e, loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Trainer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "X_train, X_test, y_train, y_test = Tensor(X_train), Tensor(X_test), Tensor(y_train), Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(559.4208, grad_fn=<MseLossBackward>)\n",
      "1 tensor(526.6153, grad_fn=<MseLossBackward>)\n",
      "2 tensor(489.2494, grad_fn=<MseLossBackward>)\n",
      "3 tensor(441.0061, grad_fn=<MseLossBackward>)\n",
      "4 tensor(378.1341, grad_fn=<MseLossBackward>)\n",
      "5 tensor(309.1067, grad_fn=<MseLossBackward>)\n",
      "6 tensor(247.6548, grad_fn=<MseLossBackward>)\n",
      "7 tensor(204.3143, grad_fn=<MseLossBackward>)\n",
      "8 tensor(154.8956, grad_fn=<MseLossBackward>)\n",
      "9 tensor(129.8301, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = BostonModel()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "trainer = PyTorchTrainer(net, optimizer, criterion)\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "            epochs=10,\n",
    "            eval_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "mnist_trainset = MNIST(root=\"../exploratory/data/\", train=True, download=True, transform=None)\n",
    "mnist_testset = MNIST(root=\"../exploratory/data/\", train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = mnist_trainset\n",
    "num_labels = len(data.train_labels)\n",
    "train_labels = torch.zeros(num_labels, 10)\n",
    "for i in range(num_labels):\n",
    "    train_labels[i][data.train_labels[i]] = 1\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = mnist_testset\n",
    "num_labels = len(data.test_labels)\n",
    "test_labels = torch.zeros(num_labels, 10)\n",
    "for i in range(num_labels):\n",
    "    test_labels[i][data.test_labels[i]] = 1\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_trainset.train_data.type(torch.float32).unsqueeze(3) / 255.0\n",
    "mnist_test = mnist_testset.test_data.type(torch.float32).unsqueeze(3) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist_train\n",
    "X_test = mnist_test\n",
    "y_train = train_labels\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_perm = X_train.permute(0, 3, 1, 2)\n",
    "X_test_perm = X_test.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_ConvNet(PyTorchModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert_dim(x, 4)\n",
    "        \n",
    "        # num channgels\n",
    "        assert x.shape[1] == 1        \n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_ConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.1087, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "trainer = PyTorchTrainer(model, optimizer, criterion)\n",
    "\n",
    "trainer.fit(X_train_perm, mnist_trainset.train_labels, \n",
    "            X_test_perm, mnist_testset.test_labels,\n",
    "            epochs=1,\n",
    "            eval_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.forward(X_train_perm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a custom trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_data(X: Tensor, seed=1):\n",
    "    perm = torch.randperm(X.shape[0])\n",
    "    return X[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderTrainer(PyTorchTrainer):\n",
    "    def __init__(self,\n",
    "                 model: PyTorchModel,\n",
    "                 optim: Optimizer,\n",
    "                 criterion: _Loss):\n",
    "        super().__init__(model, optim, criterion)\n",
    "        self._check_optim_net_aligned()\n",
    "        \n",
    "    def _generate_batches(self,\n",
    "                          X: Tensor,\n",
    "                          size: int = 32) -> Tuple[Tensor]:\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for ii in range(0, N, size):\n",
    "            X_batch = X[ii:ii+size]\n",
    "\n",
    "            yield X_batch\n",
    "\n",
    "\n",
    "    def fit(self, X_train: Tensor,\n",
    "            X_test: Tensor,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32):\n",
    "\n",
    "        for e in range(epochs):\n",
    "            X_train = permute_data(X_train)\n",
    "\n",
    "            batch_generator = self._generate_batches(X_train, batch_size)\n",
    "\n",
    "            for ii, X_batch in enumerate(batch_generator):\n",
    "                \n",
    "                self.optim.zero_grad()   # zero the gradient buffers\n",
    "                encoding, output = self.model(X_batch)\n",
    "                loss = self.loss(output, X_batch)\n",
    "                loss.backward()\n",
    "                self.optim.step()    # Does the update\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            _, output = self.model(X_test)\n",
    "            loss = self.loss(output, X_test)\n",
    "            print(e, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model follows [here](https://github.com/L1aoXingyu/pytorch-beginner/blob/master/08-AutoEncoder/conv_autoencoder.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(PyTorchModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 3, 3\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 5, 5\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # b, 8, 15, 15\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # b, 1, 28, 28\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderSplit(PyTorchModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv2d1 = nn.Conv2d(1, 16, 3, stride=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv2d2 = nn.Conv2d(16, 8, 3, stride=2, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(2, stride=1)\n",
    "        \n",
    "        self.conv2dT1 = nn.ConvTranspose2d(8, 16, 3, stride=2)\n",
    "        self.conv2dT2 = nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1)\n",
    "        self.conv2dT3 = nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1)  # b, 1, 28, 28\n",
    "\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2d2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        encoded = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.conv2dT1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2dT2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2dT3(x)\n",
    "        decoded = self.tanh(x)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "model = AutoEncoderSplit()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0405, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "trainer = AutoEncoderTrainer(model, optimizer, criterion)\n",
    "\n",
    "trainer.fit(X_train_perm, X_train_perm, \n",
    "            epochs=1,\n",
    "            eval_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded, _ = model(X_test_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 32])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work! GANs could be done similarly. Will examine later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To really do AE or GAN, you'll need to write custom trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working backwards:\n",
    "\n",
    "* Want a character level model - predict next char."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to do it?\n",
    "\n",
    "Pass in sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New classes: `NextCharacterModel` and `LSTMTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3714, -0.1111,  0.7676, -0.9703,  0.0697]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3714, -0.1111,  0.7676, -0.9703,  0.0697]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 1, 5)\n",
    "print(a)\n",
    "a.repeat(1, 3, 1).shape\n",
    "a.repeat(1, 3, 1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextCharacterModel(PyTorchModel):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 neurons: int = 256,\n",
    "                 sequence_length: int = 25,\n",
    "                 reset_every: int = 100):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = neurons\n",
    "        self.lstm = nn.LSTM(vocab_size, neurons,\n",
    "                            batch_first=True)\n",
    "        self.fc_last = nn.Linear(neurons, vocab_size)\n",
    "        self.count = 0\n",
    "        self.reset_every = reset_every\n",
    "\n",
    "    def forward(self,\n",
    "                inputs: Tensor):\n",
    "        assert_dim(inputs, 3) # batch_size, sequence_length, vocab_size\n",
    "\n",
    "        if self.count % self.reset_every == 0:\n",
    "            self.hidden, self.cells = torch.randn(1, 1, self.hidden_size),\\\n",
    "                torch.randn(1, 1, self.hidden_size)\n",
    "        \n",
    "        self.count += 1\n",
    "        \n",
    "        hidden, cells = self.hidden.repeat(1, inputs.shape[0], 1),\\\n",
    "            self.cells.repeat(1, inputs.shape[0], 1)\n",
    "\n",
    "        out, (hidden_out, cells_out) = self.lstm(inputs, (hidden.data, cells.data))\n",
    "        \n",
    "        self.hidden.data, self.cells.data = hidden_out.data.mean(dim=1), cells_out.data.mean(dim=1)\n",
    "        \n",
    "        out = self.fc_last(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTrainer(PyTorchTrainer):\n",
    "    def __init__(self,\n",
    "                 model: NextCharacterModel,\n",
    "                 optim: Optimizer,\n",
    "                 criterion: _Loss):\n",
    "        super().__init__(model, optim, criterion)\n",
    "        self.vocab_size = self.model.vocab_size\n",
    "        self.max_len = self.model.sequence_length\n",
    "        \n",
    "    def fit(self,\n",
    "            data: str,\n",
    "            epochs: int=10,\n",
    "            eval_every: int=1,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 121718)-> None:\n",
    "        \n",
    "        self.data = data\n",
    "        self.train_data, self.test_data = self._train_test_split_text()\n",
    "        self.chars = list(set(self.data))\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        for e in range(epochs):\n",
    "\n",
    "            batch_generator = self.generate_batches_next_char(batch_size)\n",
    "\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "                print(ii)\n",
    "#                 if ii == 1:\n",
    "#                     import pdb; pdb.set_trace()\n",
    "                self.optim.zero_grad()                \n",
    "                outputs = self.model(X_batch)\n",
    "                loss = self.loss(outputs, y_batch)\n",
    "                print(loss)\n",
    "                loss.backward()\n",
    "                self.optim.step()    # Does the update\n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                X_test, y_test = self.generate_test_data()\n",
    "            \n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "                print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "\n",
    "    def _train_test_split_text(self, pct=0.8) -> Tuple[str]:\n",
    "\n",
    "        n = len(self.data)\n",
    "        return self.data[:int(n * pct)], self.data[int(n * pct):]\n",
    "\n",
    "    def generate_batches_next_char(self,\n",
    "                                   batch_size: int) -> Tuple[Tensor]:\n",
    "        N = len(self.train_data)\n",
    "        # add batch size\n",
    "        for ii in range(0, N, batch_size):\n",
    "\n",
    "            features_tensors = []\n",
    "            target_tensors = []\n",
    "\n",
    "            for char in range(batch_size):\n",
    "\n",
    "                features_str, target_str =\\\n",
    "                 self.train_data[ii+char:ii+char+self.max_len],\\\n",
    "                 self.train_data[ii+char+1:ii+char+self.max_len+1]\n",
    "\n",
    "                features_array, target_array =\\\n",
    "                    self._string_to_one_hot_array(features_str),\\\n",
    "                    self._string_to_one_hot_array(target_str)\n",
    "\n",
    "                features_tensors.append(features_array)\n",
    "                target_tensors.append(target_array)\n",
    "\n",
    "            yield torch.stack(features_tensors), torch.stack(target_tensors)\n",
    "\n",
    "    def _string_to_one_hot_array(self, input_string: str) -> Tuple[Tensor]:\n",
    "\n",
    "        ind = [self.char_to_idx[ch] for ch in input_string]\n",
    "\n",
    "        array = self._one_hot_text_data(ind)\n",
    "\n",
    "        return array\n",
    "\n",
    "    def _one_hot_text_data(self,\n",
    "                           sequence: List):\n",
    "\n",
    "        sequence_length = len(sequence)\n",
    "        batch = torch.zeros(sequence_length, self.vocab_size)\n",
    "        for i in range(sequence_length):\n",
    "            batch[i, sequence[i]] = 1.0\n",
    "\n",
    "        return Tensor(batch)\n",
    "\n",
    "    def generate_test_data(self) -> Tuple[Tensor]:\n",
    "\n",
    "        features_str, target_str = self.test_data[:-1], self.test_data[1:]\n",
    "\n",
    "        X_tensors = []\n",
    "        y_tensors = []\n",
    "\n",
    "        N = len(self.test_data)\n",
    "\n",
    "        for start in range(0, N, self.max_len):\n",
    "\n",
    "            features_str, target_str =\\\n",
    "             self.test_data[start:start+self.max_len],\\\n",
    "             self.test_data[start+1:start+self.max_len+1]\n",
    "\n",
    "            features_array, target_array =\\\n",
    "                self._string_to_one_hot_array(features_str),\\\n",
    "                self._string_to_one_hot_array(target_str)\n",
    "\n",
    "            X_tensors.append(features_array)\n",
    "            y_tensors.append(target_array)\n",
    "\n",
    "        return torch.stack(X_tensors), torch.stack(y_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data/input.txt', 'r').read()\n",
    "vocab_size = len(set(data))\n",
    "model = NextCharacterModel(vocab_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_trainer = LSTMTrainer(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(0.0187, grad_fn=<MseLossBackward>)\n",
      "1\n",
      "tensor(0.0155, grad_fn=<MseLossBackward>)\n",
      "2\n",
      "tensor(0.0160, grad_fn=<MseLossBackward>)\n",
      "3\n",
      "tensor(0.0158, grad_fn=<MseLossBackward>)\n",
      "4\n",
      "tensor(0.0154, grad_fn=<MseLossBackward>)\n",
      "5\n",
      "tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "6\n",
      "tensor(0.0151, grad_fn=<MseLossBackward>)\n",
      "7\n",
      "tensor(0.0157, grad_fn=<MseLossBackward>)\n",
      "8\n",
      "tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "9\n",
      "tensor(0.0152, grad_fn=<MseLossBackward>)\n",
      "10\n",
      "tensor(0.0160, grad_fn=<MseLossBackward>)\n",
      "11\n",
      "tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "12\n",
      "tensor(0.0154, grad_fn=<MseLossBackward>)\n",
      "13\n",
      "tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "14\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "15\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "16\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "17\n",
      "tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "18\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "19\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "20\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "21\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "22\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "23\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "24\n",
      "tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "25\n",
      "tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "26\n",
      "tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "27\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "28\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "29\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "30\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "31\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "32\n",
      "tensor(0.0152, grad_fn=<MseLossBackward>)\n",
      "33\n",
      "tensor(0.0126, grad_fn=<MseLossBackward>)\n",
      "34\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "35\n",
      "tensor(0.0129, grad_fn=<MseLossBackward>)\n",
      "36\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "37\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "38\n",
      "tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "39\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "40\n",
      "tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "41\n",
      "tensor(0.0158, grad_fn=<MseLossBackward>)\n",
      "42\n",
      "tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "43\n",
      "tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "44\n",
      "tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "45\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "46\n",
      "tensor(0.0130, grad_fn=<MseLossBackward>)\n",
      "47\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "48\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "49\n",
      "tensor(0.0141, grad_fn=<MseLossBackward>)\n",
      "50\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "51\n",
      "tensor(0.0132, grad_fn=<MseLossBackward>)\n",
      "52\n",
      "tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "53\n",
      "tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "54\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "55\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "56\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "57\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "58\n",
      "tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "59\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "60\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "61\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "62\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "63\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "64\n",
      "tensor(0.0155, grad_fn=<MseLossBackward>)\n",
      "65\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "66\n",
      "tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "67\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "68\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "69\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "70\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "71\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "72\n",
      "tensor(0.0135, grad_fn=<MseLossBackward>)\n",
      "73\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "74\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "75\n",
      "tensor(0.0141, grad_fn=<MseLossBackward>)\n",
      "76\n",
      "tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "77\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "78\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "79\n",
      "tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "80\n",
      "tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "81\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "82\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "83\n",
      "tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "84\n",
      "tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "85\n",
      "tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "86\n",
      "tensor(0.0156, grad_fn=<MseLossBackward>)\n",
      "87\n",
      "tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "88\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "89\n",
      "tensor(0.0132, grad_fn=<MseLossBackward>)\n",
      "90\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "91\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "92\n",
      "tensor(0.0141, grad_fn=<MseLossBackward>)\n",
      "93\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "94\n",
      "tensor(0.0152, grad_fn=<MseLossBackward>)\n",
      "95\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "96\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "97\n",
      "tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "98\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "99\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "100\n",
      "tensor(0.0156, grad_fn=<MseLossBackward>)\n",
      "101\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "102\n",
      "tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "103\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "104\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "105\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "106\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "107\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "108\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "109\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "110\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "111\n",
      "tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "112\n",
      "tensor(0.0141, grad_fn=<MseLossBackward>)\n",
      "113\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "114\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "115\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "116\n",
      "tensor(0.0131, grad_fn=<MseLossBackward>)\n",
      "117\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "118\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "119\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "120\n",
      "tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "121\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "122\n",
      "tensor(0.0132, grad_fn=<MseLossBackward>)\n",
      "123\n",
      "tensor(0.0130, grad_fn=<MseLossBackward>)\n",
      "124\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "125\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "126\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "127\n",
      "tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "128\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "129\n",
      "tensor(0.0135, grad_fn=<MseLossBackward>)\n",
      "130\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "131\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "132\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "133\n",
      "tensor(0.0158, grad_fn=<MseLossBackward>)\n",
      "134\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "135\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "136\n",
      "tensor(0.0153, grad_fn=<MseLossBackward>)\n",
      "137\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "138\n",
      "tensor(0.0151, grad_fn=<MseLossBackward>)\n",
      "139\n",
      "tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "140\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "141\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "142\n",
      "tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "143\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "144\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "145\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "146\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "147\n",
      "tensor(0.0141, grad_fn=<MseLossBackward>)\n",
      "148\n",
      "tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "149\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "150\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "151\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "152\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "153\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "154\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "155\n",
      "tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "156\n",
      "tensor(0.0141, grad_fn=<MseLossBackward>)\n",
      "157\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "158\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "159\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "160\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "161\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "162\n",
      "tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "163\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "164\n",
      "tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "165\n",
      "tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "166\n",
      "tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "167\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "168\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "169\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "170\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "171\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "172\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "173\n",
      "tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "174\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "175\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "176\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "177\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "178\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "179\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "180\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n",
      "tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "182\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "183\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "184\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "185\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "186\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "187\n",
      "tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "188\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "189\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "190\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "191\n",
      "tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "192\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "193\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "194\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "195\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "196\n",
      "tensor(0.0135, grad_fn=<MseLossBackward>)\n",
      "197\n",
      "tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "198\n",
      "tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "199\n",
      "tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "200\n",
      "tensor(0.0162, grad_fn=<MseLossBackward>)\n",
      "201\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "202\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "203\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "204\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "205\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "206\n",
      "tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "207\n",
      "tensor(0.0151, grad_fn=<MseLossBackward>)\n",
      "208\n",
      "tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "209\n",
      "tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "210\n",
      "tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "211\n",
      "tensor(0.0135, grad_fn=<MseLossBackward>)\n",
      "212\n",
      "tensor(0.0141, grad_fn=<MseLossBackward>)\n",
      "213\n",
      "tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "214\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "215\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "216\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "217\n",
      "tensor(0.0153, grad_fn=<MseLossBackward>)\n",
      "218\n",
      "tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "219\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "220\n",
      "tensor(0.0152, grad_fn=<MseLossBackward>)\n",
      "221\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "222\n",
      "tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "223\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "224\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "225\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "226\n",
      "tensor(0.0141, grad_fn=<MseLossBackward>)\n",
      "227\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "228\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "229\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "230\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "231\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "232\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "233\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "234\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "235\n",
      "tensor(0.0141, grad_fn=<MseLossBackward>)\n",
      "236\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "237\n",
      "tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "238\n",
      "tensor(0.0152, grad_fn=<MseLossBackward>)\n",
      "239\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "240\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "241\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "242\n",
      "tensor(0.0141, grad_fn=<MseLossBackward>)\n",
      "243\n",
      "tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "244\n",
      "tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "245\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "246\n",
      "tensor(0.0141, grad_fn=<MseLossBackward>)\n",
      "247\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "248\n",
      "tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "249\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "250\n",
      "tensor(0.0135, grad_fn=<MseLossBackward>)\n",
      "251\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n",
      "252\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "253\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "254\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "255\n",
      "tensor(0.0144, grad_fn=<MseLossBackward>)\n",
      "256\n",
      "tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "257\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "258\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "259\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "260\n",
      "tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "261\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "262\n",
      "tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "263\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "264\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "265\n",
      "tensor(0.0135, grad_fn=<MseLossBackward>)\n",
      "266\n",
      "tensor(0.0135, grad_fn=<MseLossBackward>)\n",
      "267\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "268\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "269\n",
      "tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "270\n",
      "tensor(0.0151, grad_fn=<MseLossBackward>)\n",
      "271\n",
      "tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "272\n",
      "tensor(0.0139, grad_fn=<MseLossBackward>)\n",
      "273\n",
      "tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "274\n",
      "tensor(0.0146, grad_fn=<MseLossBackward>)\n",
      "275\n",
      "tensor(0.0142, grad_fn=<MseLossBackward>)\n",
      "276\n",
      "tensor(0.0130, grad_fn=<MseLossBackward>)\n",
      "277\n",
      "tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "278\n",
      "tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "279\n",
      "tensor(0.0137, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-b07782b6ec45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-4fd60a25e580>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, epochs, eval_every, batch_size, seed)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Does the update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_trainer.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "* Write code to generate next character from this.\n",
    "* Write early stopping code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section name: Grokking Advanced Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same data source as LSTM: \n",
    "\n",
    "* TODO: Draw computational graph\n",
    "* TODO: Describe input and output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Turing Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
