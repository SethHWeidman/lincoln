{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lincoln imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lincoln\n",
    "from lincoln.numpy.layers import Dense\n",
    "from lincoln.numpy.losses import SoftmaxCrossEntropyLoss, MeanSquaredError\n",
    "from lincoln.numpy.optimizers import SGD\n",
    "from lincoln.numpy.activations import Sigmoid, Tanh, Linear\n",
    "from lincoln.numpy.network import NeuralNetwork\n",
    "from lincoln.numpy.train import Trainer\n",
    "from lincoln.data import mnist\n",
    "from lincoln.np_utils import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = mnist.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(y_train)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(y_train)\n",
    "train_labels = np.zeros((num_labels, 10))\n",
    "for i in range(num_labels):\n",
    "    train_labels[i][y_train[i]] = 1\n",
    "\n",
    "num_labels = len(y_test)\n",
    "test_labels = np.zeros((num_labels, 10))\n",
    "for i in range(num_labels):\n",
    "    test_labels[i][y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scaled to mean 0, variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train - np.mean(X_train), X_test - np.mean(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-33.318421449829934,\n",
       " 221.68157855017006,\n",
       " -33.318421449829934,\n",
       " 221.68157855017006)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_train), np.max(X_train), np.min(X_test), np.max(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train / np.std(X_train) , X_test / np.std(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.424073894391566, 2.821543345689335, -0.424073894391566, 2.821543345689335)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_train), np.max(X_train), np.min(X_test), np.max(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 25783.223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7596"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_soft = NeuralNetwork(\n",
    "    layers=[Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = SoftmaxCrossEntropyLoss(), \n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_soft, SGD(0.1))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 1,\n",
    "       eval_every = 1,\n",
    "       seed=190119,\n",
    "            batch_size=60);\n",
    "\n",
    "np.equal(np.argmax(mnist_soft.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 26405.305\n",
      "Validation loss after 2 epochs is 25951.760\n",
      "Validation loss after 3 epochs is 25813.492\n",
      "Validation loss after 4 epochs is 25728.564\n",
      "Validation loss after 5 epochs is 25607.527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7587"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_soft = NeuralNetwork(\n",
    "    layers=[Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = SoftmaxCrossEntropyLoss(), \n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_soft, SGD(0.01))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 5,\n",
    "       eval_every = 1,\n",
    "       seed=190119,\n",
    "            batch_size=60,\n",
    "           early_stopping=False);\n",
    "\n",
    "np.equal(np.argmax(mnist_soft.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_soft = NeuralNetwork(\n",
    "    layers=[Dense(neurons=10, \n",
    "                  activation=Sigmoid(),\n",
    "                  weight_init=\"xavier\")],\n",
    "            loss = SoftmaxCrossEntropyLoss(), \n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_soft, SGD(0.1))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 1,\n",
    "       eval_every = 1,\n",
    "       seed=190119);\n",
    "\n",
    "np.equal(np.argmax(mnist_soft.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 5428.275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5576"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_mse = NeuralNetwork(\n",
    "    layers=[Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = MeanSquaredError(),\n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_mse, SGD(0.1))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 1,\n",
    "       eval_every = 1,\n",
    "            batch_size=60,\n",
    "       seed=190119,\n",
    "           early_stopping=False);\n",
    "\n",
    "np.equal(np.argmax(mnist_mse.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 7197.050\n",
      "Validation loss after 2 epochs is 6446.113\n",
      "Validation loss after 3 epochs is 6214.973\n",
      "Validation loss after 4 epochs is 6104.685\n",
      "Validation loss after 5 epochs is 5907.799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5109"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_mse = NeuralNetwork(\n",
    "    layers=[Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = MeanSquaredError(),\n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_mse, SGD(0.01))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 5,\n",
    "       eval_every = 1,\n",
    "            batch_size=60,\n",
    "       seed=190119,\n",
    "           early_stopping=False);\n",
    "\n",
    "np.equal(np.argmax(mnist_mse.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lincoln.numpy.optimizers import SGDMomentum, AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 27797.341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4622"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_soft = NeuralNetwork(\n",
    "    layers=[Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = SoftmaxCrossEntropyLoss(),\n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_soft, SGDMomentum(lr=0.1, momentum=0.9))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 1,\n",
    "       eval_every = 1,\n",
    "       seed=190119,\n",
    "            batch_size=60);\n",
    "\n",
    "np.equal(np.argmax(mnist_soft.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 25626.662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7289"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_soft = NeuralNetwork(\n",
    "    layers=[Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = SoftmaxCrossEntropyLoss(),\n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_soft, SGDMomentum(lr=0.1, momentum=0.1))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 1,\n",
    "       eval_every = 1,\n",
    "       seed=190119,\n",
    "            batch_size=60);\n",
    "\n",
    "np.equal(np.argmax(mnist_soft.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 30474.395\n",
      "Validation loss after 2 epochs is 30346.151\n",
      "Validation loss after 3 epochs is 30436.737\n",
      "Validation loss after 4 epochs is 30373.557\n",
      "Validation loss after 5 epochs is 30337.268\n"
     ]
    }
   ],
   "source": [
    "mnist_soft = NeuralNetwork(\n",
    "    layers=[Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = LogSoftmaxLoss(),\n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_soft, AdaGrad(lr=1))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 5,\n",
    "       eval_every = 1,\n",
    "       seed=190119,\n",
    "            batch_size=60,\n",
    "           early_stopping=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.equal(np.argmax(mnist_soft.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.424073894391566, 2.821543345689335, -0.424073894391566, 2.821543345689335)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_train), np.max(X_train), np.min(X_test), np.max(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD LR = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 31221.351\n",
      "Validation loss after 2 epochs is 31199.774\n",
      "Validation loss after 3 epochs is 30899.924\n",
      "Loss increased after epoch 4, final loss was 30899.924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2192"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_soft_nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=256, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = SoftmaxCrossEntropyLoss(), \n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_soft_nn, SGD(1))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 10,\n",
    "       eval_every = 1,\n",
    "       seed=190119,\n",
    "            batch_size=60);\n",
    "\n",
    "np.equal(np.argmax(mnist_soft_nn.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD LR = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 25834.293\n",
      "Validation loss after 2 epochs is 25545.534\n",
      "Validation loss after 3 epochs is 25400.607\n",
      "Loss increased after epoch 4, final loss was 25400.607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7349"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_soft_nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=256, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = SoftmaxCrossEntropyLoss(), \n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_soft_nn, SGD(0.1))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 10,\n",
    "       eval_every = 1,\n",
    "       seed=190119,\n",
    "            batch_size=60);\n",
    "\n",
    "np.equal(np.argmax(mnist_soft_nn.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD LR = 0.1, Xavier init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 25942.976\n",
      "Validation loss after 2 epochs is 25739.234\n",
      "Validation loss after 3 epochs is 25549.220\n",
      "Loss increased after epoch 4, final loss was 25549.220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5828"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_soft_nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=256, \n",
    "                  activation=Tanh(),\n",
    "                  weight_init=\"xavier\"),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Sigmoid(),\n",
    "                  weight_init=\"xavier\")],\n",
    "            loss = SoftmaxCrossEntropyLoss(), \n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_soft_nn, SGD(0.1))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 10,\n",
    "       eval_every = 1,\n",
    "       seed=190119,\n",
    "            batch_size=60);\n",
    "\n",
    "np.equal(np.argmax(mnist_soft_nn.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 26124.486\n",
      "Validation loss after 2 epochs is 25740.280\n",
      "Validation loss after 3 epochs is 25576.542\n",
      "Validation loss after 4 epochs is 25520.847\n",
      "Validation loss after 5 epochs is 25452.980\n",
      "Validation loss after 6 epochs is 25382.852\n",
      "Validation loss after 7 epochs is 25352.703\n",
      "Validation loss after 8 epochs is 25297.260\n",
      "Validation loss after 9 epochs is 25279.931\n",
      "Loss increased after epoch 10, final loss was 25279.931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7481"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_soft_nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=256, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = SoftmaxCrossEntropyLoss(), \n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_soft_nn, SGD(0.01))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 10,\n",
    "       eval_every = 1,\n",
    "       seed=190119,\n",
    "            batch_size=60);\n",
    "\n",
    "np.equal(np.argmax(mnist_soft_nn.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD LR = 0.01, batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /Users/seth/development/lincoln/lincoln/numpy/losses.py(79)_output()\n",
      "-> return np.sum(log_loss)\n",
      "(Pdb) np.log(1 - softmax_preds)\n",
      "array([[-0.07720591, -0.07720591, -0.07935035, -0.07720591, -0.07720592,\n",
      "        -0.07725279, -0.07720599, -0.07725917, -0.22527014, -0.22541935],\n",
      "       [-0.15725056, -0.055034  , -0.05503732, -0.15729728, -0.05503261,\n",
      "        -0.15729728, -0.15722807, -0.12217989, -0.05503169, -0.09288367],\n",
      "       [-0.11863952, -0.06645641, -0.11863952, -0.11863952, -0.11863866,\n",
      "        -0.11863952, -0.11859823, -0.11819929, -0.04202631, -0.11863944],\n",
      "       [-0.15769931, -0.0551652 , -0.15769931, -0.05532155, -0.0551652 ,\n",
      "        -0.15769931, -0.15768186, -0.15769931, -0.05730951, -0.05518289],\n",
      "       [-0.16916345, -0.16916354, -0.05895718, -0.11577307, -0.05895712,\n",
      "        -0.14786738, -0.16916363, -0.05895712, -0.05895722, -0.05903248],\n",
      "       [-0.0682534 , -0.06825323, -0.1976483 , -0.06825323, -0.06825329,\n",
      "        -0.06845042, -0.06825354, -0.19764544, -0.19759986, -0.06825341],\n",
      "       [-0.06783629, -0.06730815, -0.06730849, -0.06730815, -0.06730816,\n",
      "        -0.06730848, -0.19468664, -0.19471924, -0.0814404 , -0.19467033],\n",
      "       [-0.06803372, -0.06803248, -0.19696479, -0.06803225, -0.0715468 ,\n",
      "        -0.06803225, -0.06803226, -0.19695482, -0.06803225, -0.19696479],\n",
      "       [-0.16955548, -0.059165  , -0.05950225, -0.15617542, -0.05916485,\n",
      "        -0.16979421, -0.10488503, -0.05916485, -0.05916485, -0.16978563],\n",
      "       [-0.16988973, -0.17112508, -0.05960952, -0.06032896, -0.05960952,\n",
      "        -0.17114497, -0.17114497, -0.05960952, -0.0596096 , -0.085402  ]])\n",
      "(Pdb) (1.0 - tgt) * np.log(1 - softmax_preds)\n",
      "array([[-0.07720591, -0.07720591, -0.07935035, -0.07720591, -0.07720592,\n",
      "        -0.07725279, -0.07720599, -0.07725917, -0.        , -0.22541935],\n",
      "       [-0.15725056, -0.055034  , -0.        , -0.15729728, -0.05503261,\n",
      "        -0.15729728, -0.15722807, -0.12217989, -0.05503169, -0.09288367],\n",
      "       [-0.11863952, -0.06645641, -0.11863952, -0.11863952, -0.11863866,\n",
      "        -0.11863952, -0.        , -0.11819929, -0.04202631, -0.11863944],\n",
      "       [-0.15769931, -0.0551652 , -0.15769931, -0.        , -0.0551652 ,\n",
      "        -0.15769931, -0.15768186, -0.15769931, -0.05730951, -0.05518289],\n",
      "       [-0.16916345, -0.16916354, -0.05895718, -0.11577307, -0.05895712,\n",
      "        -0.14786738, -0.16916363, -0.05895712, -0.05895722, -0.        ],\n",
      "       [-0.        , -0.06825323, -0.1976483 , -0.06825323, -0.06825329,\n",
      "        -0.06845042, -0.06825354, -0.19764544, -0.19759986, -0.06825341],\n",
      "       [-0.06783629, -0.06730815, -0.06730849, -0.06730815, -0.06730816,\n",
      "        -0.06730848, -0.19468664, -0.        , -0.0814404 , -0.19467033],\n",
      "       [-0.06803372, -0.06803248, -0.        , -0.06803225, -0.0715468 ,\n",
      "        -0.06803225, -0.06803226, -0.19695482, -0.06803225, -0.19696479],\n",
      "       [-0.16955548, -0.059165  , -0.05950225, -0.15617542, -0.05916485,\n",
      "        -0.16979421, -0.10488503, -0.05916485, -0.05916485, -0.        ],\n",
      "       [-0.16988973, -0.17112508, -0.05960952, -0.06032896, -0.05960952,\n",
      "        -0.17114497, -0.17114497, -0.        , -0.0596096 , -0.085402  ]])\n",
      "(Pdb) softmax_preds\n",
      "array([[0.07430078, 0.07430078, 0.07628375, 0.07430078, 0.07430078,\n",
      "        0.07434417, 0.07430085, 0.07435008, 0.20169946, 0.20181857],\n",
      "       [0.14551007, 0.05354703, 0.05355017, 0.14554999, 0.05354571,\n",
      "        0.14554999, 0.14549085, 0.11501085, 0.05354485, 0.08870049],\n",
      "       [0.1118721 , 0.0642963 , 0.1118721 , 0.1118721 , 0.11187134,\n",
      "        0.1118721 , 0.11183544, 0.11148104, 0.04115545, 0.11187203],\n",
      "       [0.14589343, 0.0536712 , 0.14589344, 0.05381915, 0.0536712 ,\n",
      "        0.14589344, 0.14587853, 0.14589344, 0.05569825, 0.05368794],\n",
      "       [0.15562912, 0.1556292 , 0.05725287, 0.10932268, 0.05725281,\n",
      "        0.1374545 , 0.15562927, 0.05725281, 0.0572529 , 0.05732385],\n",
      "       [0.06597624, 0.06597608, 0.17934158, 0.06597608, 0.06597614,\n",
      "        0.06616024, 0.06597637, 0.17933922, 0.17930182, 0.06597624],\n",
      "       [0.06558656, 0.06509293, 0.06509325, 0.06509293, 0.06509294,\n",
      "        0.06509325, 0.17690746, 0.17693429, 0.07821235, 0.17689403],\n",
      "       [0.06577103, 0.06576988, 0.17878045, 0.06576965, 0.06904729,\n",
      "        0.06576966, 0.06576967, 0.17877226, 0.06576966, 0.17878045],\n",
      "       [0.15596008, 0.05744876, 0.05776659, 0.14459088, 0.05744862,\n",
      "        0.15616155, 0.09957196, 0.05744862, 0.05744863, 0.1561543 ],\n",
      "       [0.15624215, 0.15728384, 0.05786766, 0.05854522, 0.05786766,\n",
      "        0.1573006 , 0.1573006 , 0.05786766, 0.05786773, 0.08185688]])\n",
      "(Pdb) exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-f067940ef2df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m        \u001b[0meval_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m190119\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             batch_size=10);\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_soft_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10000.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_test, y_test, epochs, eval_every, batch_size, seed, single_output, restart, early_stopping)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;31m# import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/network.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, X_batch, y_batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mloss_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/losses.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, prediction, target)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/losses.py\u001b[0m in \u001b[0;36m_output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_input_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/lincoln/lincoln/numpy/losses.py\u001b[0m in \u001b[0;36m_output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_input_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mnist_soft_nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=256, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = SoftmaxCrossEntropyLoss(), \n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_soft_nn, SGD(0.01))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 10,\n",
    "       eval_every = 1,\n",
    "       seed=190119,\n",
    "            batch_size=10);\n",
    "\n",
    "np.equal(np.argmax(mnist_soft_nn.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD LR = 0.01, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 1 epochs is 6843.348\n",
      "Validation loss after 2 epochs is 5461.411\n",
      "Validation loss after 3 epochs is 5199.329\n",
      "Validation loss after 4 epochs is 5065.602\n",
      "Validation loss after 5 epochs is 5005.743\n",
      "Validation loss after 6 epochs is 4935.914\n",
      "Validation loss after 7 epochs is 4873.672\n",
      "Validation loss after 8 epochs is 4840.920\n",
      "Validation loss after 9 epochs is 4803.903\n",
      "Validation loss after 10 epochs is 4801.747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5991"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_soft_nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=256, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Sigmoid())],\n",
    "            loss = MeanSquaredError(), \n",
    "seed=190119)\n",
    "\n",
    "trainer = Trainer(mnist_soft_nn, SGD(0.01))\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "       epochs = 10,\n",
    "       eval_every = 1,\n",
    "       seed=190119,\n",
    "            batch_size=60, early_stopping=False);\n",
    "\n",
    "np.equal(np.argmax(mnist_soft_nn.forward(X_test), axis=1), y_test).sum() / 10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
