{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Lincoln to system path\n",
    "import sys\n",
    "sys.path.append(\"/Users/seth/development/lincoln/\")\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "import torch\n",
    "\n",
    "from typing import List, Callable, Dict, Tuple\n",
    "\n",
    "from lincoln.autograd.tensor import Tensor\n",
    "from lincoln.autograd.layer import Layer\n",
    "from lincoln.autograd.model import Model\n",
    "from lincoln.autograd.param import Parameter\n",
    "from lincoln.autograd.optim import Optim, SGD\n",
    "from lincoln.autograd.train import Trainer\n",
    "from lincoln.autograd.activations import sigmoid, tanh, linear\n",
    "\n",
    "from lincoln.utils import permute_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences:\n",
    "\n",
    "* `Layer`: the actual forward function will be different.\n",
    "* `Trainer`: batch generator now will do all the transforming of text into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 neurons: int) -> None:\n",
    "        self.state_size = neurons\n",
    "        self.first = True\n",
    "        self.params: Dict[['str'], Tensor] = {}\n",
    "        self.h_init = Tensor(np.random.randn(1, self.state_size))\n",
    "        self.c_init = Tensor(np.random.randn(1, self.state_size))\n",
    "            \n",
    "    def _init_params(self, input_: Tensor) -> None:\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        self.params['Wf'] = Parameter(self.state_size + self.vocab_size, \n",
    "                                 self.state_size)\n",
    "        self.params['Wi'] = Parameter(self.state_size + self.vocab_size, \n",
    "                                 self.state_size)\n",
    "        self.params['Wo'] = Parameter(self.state_size + self.vocab_size, \n",
    "                                 self.state_size)\n",
    "        self.params['Wc'] = Parameter(self.state_size + self.vocab_size, \n",
    "                                 self.state_size)\n",
    "        self.params['Wv'] = Parameter(self.state_size, self.vocab_size)\n",
    "\n",
    "        self.params['Bf'] = Parameter(self.state_size)\n",
    "        self.params['Bi'] = Parameter(self.state_size)\n",
    "        self.params['Bo'] = Parameter(self.state_size)\n",
    "        self.params['Bc'] = Parameter(self.state_size)\n",
    "        self.params['Bv'] = Parameter(self.vocab_size)\n",
    "\n",
    "        hiddens = self.h_init.repeat(input_.shape[0])\n",
    "        cells = self.c_init.repeat(input_.shape[0])\n",
    "        \n",
    "        return hiddens, cells\n",
    "            \n",
    "    def forward(self, input_: Tensor) -> Tensor:\n",
    "        if self.first:\n",
    "            self.hiddens, self.cells = self._init_params(input_)\n",
    "            self.first = False\n",
    "\n",
    "        for i in range(input_.shape[1]): # sequence length\n",
    "            if i == 0:\n",
    "                outputs_single = self._lstm_node(input_.select_index_axis_1(i))\n",
    "                outputs = outputs_single.expand_dims_axis_1()\n",
    "\n",
    "            else:\n",
    "                output_single = self._lstm_node(input_.select_index_axis_1(i))\n",
    "                output = output_single.expand_dims_axis_1()\n",
    "                outputs = outputs.append_axis_1(output)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def _lstm_node(self,\n",
    "                   inputs: Tensor):\n",
    "\n",
    "        assert inputs.shape[0] == self.hiddens.shape[0] == self.cells.shape[0]\n",
    "\n",
    "        Z = inputs.concat(self.hiddens)\n",
    "\n",
    "        forget = sigmoid(Z @ self.params['Wf'] + self.params['Bf'])\n",
    "\n",
    "        ingate = sigmoid(Z @ self.params['Wi'] + self.params['Bi'])\n",
    "\n",
    "        outgate = sigmoid(Z @ self.params['Wo'] + self.params['Bo'])\n",
    "\n",
    "        change = tanh(Z @ self.params['Wc'] + self.params['Bc'])\n",
    "\n",
    "        self.cells = self.cells * forget + ingate * change\n",
    "\n",
    "        self.hiddens = outgate * tanh(self.cells)\n",
    "\n",
    "        outputs = self.hiddens @ self.params['Wv'] + self.params['Bv']\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _params(self) -> Tensor:\n",
    "\n",
    "        return list(self.params.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(Model):\n",
    "    def __init__(self, \n",
    "                 layers: List[Layer],\n",
    "                 vocab_size: int,\n",
    "                 sequence_length: int = 15,\n",
    "                 seed: int = 1) -> None:\n",
    "        super().__init__(layers, seed)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            setattr(layer, \"seed\", self.seed)\n",
    "            setattr(layer, \"vocab_size\", self.vocab_size)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for parameter in self.parameters():\n",
    "            parameter.zero_grad()\n",
    "\n",
    "    def predict(self, inputs: Tensor) -> Tensor:\n",
    "\n",
    "        output = Tensor(inputs.data, no_grad=True)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            for param in layer.params.values():\n",
    "                params.append(param)\n",
    "\n",
    "        return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text data\n",
    "data = open(\"../exploratory/data/input.txt\", 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup params\n",
    "sequence_length = 15\n",
    "vocab_size = len(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTrainer(object):\n",
    "    '''\n",
    "    Just a list of layers that runs forwards and backwards\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 net: LSTMModel,\n",
    "                 optim: Optim,\n",
    "                 data: str,\n",
    "                 sequence_length: int = 15) -> None:\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.data = data\n",
    "        self.train_data, self.test_data = self._train_test_split_text()\n",
    "        \n",
    "        self.max_len = self.net.sequence_length\n",
    "        \n",
    "        self.chars = list(set(self.data))\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        \n",
    "    def update_params(self) -> None:\n",
    "        self.optim.step(self.net)\n",
    "\n",
    "    def fit(self,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 1,\n",
    "            single_output: bool = False,\n",
    "            restart: bool = True)-> None:\n",
    "\n",
    "        if restart:\n",
    "            self.optim.first = True\n",
    "            \n",
    "        for e in range(epochs):\n",
    "\n",
    "            batch_generator = self._generate_batches(batch_size)\n",
    "\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "\n",
    "                self.net.zero_grad()\n",
    "                \n",
    "                prediction = self.net.predict(X_batch)\n",
    "                loss = self._loss_prediction(prediction, y_batch)\n",
    "                print(loss)\n",
    "                loss.backward()\n",
    "\n",
    "                self.update_params()\n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "                predicted = self.net.predict(X_test)\n",
    "                loss = self._loss_prediction(predicted, y_test)\n",
    "                print(f\"Validation loss after {e+1} epochs is {loss}\")\n",
    "\n",
    "    def _loss_prediction(self,\n",
    "                         prediction: Tensor,\n",
    "                         actual: Tensor,\n",
    "                         kind: str = \"mse\") -> None:\n",
    "        if kind == \"mse\":\n",
    "            errors = prediction - actual\n",
    "            loss = (errors * errors).sum()\n",
    "            return loss\n",
    "\n",
    "    def _generate_batches(self,\n",
    "                          batch_size: int) -> Tuple[Tensor]:\n",
    "        N = len(self.train_data)\n",
    "\n",
    "        for ii in range(0, N, batch_size):\n",
    "\n",
    "            features_tensors = []\n",
    "            target_tensors = []\n",
    "\n",
    "            for char in range(batch_size):\n",
    "\n",
    "                features_str, target_str =\\\n",
    "                 self.train_data[ii+char:ii+char+self.max_len],\\\n",
    "                 self.train_data[ii+char+1:ii+char+self.max_len+1]\n",
    "\n",
    "                features_array, target_array =\\\n",
    "                    self._string_to_one_hot_array(features_str),\\\n",
    "                    self._string_to_one_hot_array(target_str)\n",
    "\n",
    "                features_tensors.append(features_array)\n",
    "                target_tensors.append(target_array)\n",
    "\n",
    "#             import pdb; pdb.set_trace()\n",
    "            yield Tensor(np.stack(features_tensors), no_grad=True),\\\n",
    "            Tensor(np.stack(target_tensors), no_grad=True)\n",
    "\n",
    "    def _string_to_one_hot_array(self, input_string: str) -> Tensor:\n",
    "\n",
    "        ind = [self.char_to_idx[ch] for ch in input_string]\n",
    "\n",
    "        array = self._one_hot_text_data(ind)\n",
    "\n",
    "        return array\n",
    "\n",
    "    def _one_hot_text_data(self,\n",
    "                           sequence: List) -> Tensor:\n",
    "\n",
    "        sequence_length = len(sequence)\n",
    "        batch = np.zeros((sequence_length, self.net.vocab_size))\n",
    "        for i in range(sequence_length):\n",
    "            batch[i][sequence[i]] = 1.0\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def _train_test_split_text(self, pct=0.8) -> Tuple[str]:\n",
    "\n",
    "        n = len(self.data)\n",
    "        return self.data[:int(n * pct)], self.data[int(n * pct):]\n",
    "    \n",
    "    def generate_test_data(self) -> Tuple[Tensor]:\n",
    "\n",
    "        features_str, target_str = self.test_data[:-1], self.test_data[1:]\n",
    "\n",
    "        X_tensors = []\n",
    "        y_tensors = []\n",
    "\n",
    "        N = len(self.test_data)\n",
    "\n",
    "        for start in range(0, N, self.max_len):\n",
    "\n",
    "            features_str, target_str =\\\n",
    "             self.test_data[start:start+self.max_len],\\\n",
    "             self.test_data[start+1:start+self.max_len+1]\n",
    "\n",
    "            features_array, target_array =\\\n",
    "                self._string_to_one_hot_array(features_str),\\\n",
    "                self._string_to_one_hot_array(target_str)\n",
    "\n",
    "            X_tensors.append(features_array)\n",
    "            y_tensors.append(target_array)\n",
    "\n",
    "        return Tensor(np.stack(X_tensors), no_grad=True),\\\n",
    "    Tensor(np.stack(y_tensors), no_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(lr=0.001)\n",
    "batch_size = 100\n",
    "lstm_model = LSTMModel([LSTMLayer(128)],\n",
    "                       vocab_size,\n",
    "                       sequence_length=5,\n",
    "                       seed=112818)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LSTMTrainer(lstm_model, optimizer, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(5674.0475)\n",
      "Tensor(6516.8157)\n",
      "Tensor(6907.8247)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(epochs = 10,\n",
    "       eval_every = 1,\n",
    "       batch_size=1,\n",
    "       seed=102618,\n",
    "       single_output=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
