{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "import typing\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchError(Exception):\n",
    "    def __init__(self, message: str) -> None:\n",
    "        self.message = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionError(Exception):\n",
    "    def __init__(self, message: str) -> None:\n",
    "        self.message = message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's consider a simple model, logistic regression. The goal of logistic regression is to predict a binary choice, True or False, admitted or not admitted, etc., from some input features. Mathematically, we perform a linear (affine) combination of our input features and coefficients (called weights here). That result is passed to the sigmoid (logistic) function which returns a prediction probability.\n",
    "\n",
    "<img src=\"assets/logreg_forward.png\" alt=\"Logistic regression forward pass\" width=400px>\n",
    "\n",
    "We can represent logistic regression as a graph of tensors and operations. The first operation is a linear transformation, $\\ell$. This operation takes three input tensors: the input data $x$, the weights $w$, and the bias term $b$. It returns the linear transformation $a$:\n",
    "\n",
    "$$\n",
    "a = \\sum_i w_i x_i + b\n",
    "$$\n",
    "\n",
    "We then pass $a$ through the sigmoid operation:\n",
    "\n",
    "$$\n",
    "p = \\sigma(a) = \\frac{1}{1+e^{-a}}\n",
    "$$\n",
    "\n",
    "At this point, we can use the probability $p$ to make a prediction. Given some input features $x$, we get a probability $p$ that we should predict **True**. The weights we start with though are just random, they aren't set to make accurate predictions for any specific problem, especially not the problem we're interested in. To find the appropriate weights, we use a scheme called supervised learning. Here we need three things:\n",
    "\n",
    "* Example observations of features $x^k$ and true outcomes $y^k$, where $k$ denotes a single observation of $K$ total observations\n",
    "* Some measure of how wrong our prediction is compared to the true outcomes, often called the loss, cost, or error \n",
    "* A method to update our weights such that the loss is minimized \n",
    "\n",
    "Commonly used is one form of the log loss $L$:\n",
    "\n",
    "$$\n",
    "L = -y \\log{p} - (1-y)\\log{(1-p)}\n",
    "$$\n",
    "\n",
    "where $y$ is the true binary outcome, either 0 or 1. Since $y$ can ever only be 0 or 1, we can also write our loss as\n",
    "\n",
    "$$\n",
    "L = \\begin{cases}\n",
    "    -\\log{p},     & \\text{if } y = 1\\\\\n",
    "    -\\log{(1-p)},  & \\text{if } y = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This loss makes sense looking at the $-\\log{p}$ function, plotted below.\n",
    "<img src=\"assets/log_loss.png\" width=400px>\n",
    "\n",
    "If our observed label is True ($y = 1$), then we want our model's prediction $p$ to be as close to 1 as possible. We see as $p$ increases to 1, $-\\log{p}$ goes to zero and as $p$ decreases to 0, $-\\log{p}$ goes to infinity. This way, if our prediction is vastly different than our observed outcomes, the loss will be high. Conversely, if our predictions are similar to the observed outcomes, we'll have low losses.\n",
    "\n",
    "We'll often give our model multiple observations at once so we sum up the loss for each of those observations to get to total loss\n",
    "$$\n",
    "L = \\sum_k^K -y^k \\log{p^k} - (1-y^k)\\log{(1-p^k)}\n",
    "$$\n",
    "\n",
    "Finally, we use a method called **gradient descent** to adjust the weights such that the loss is as low as possible. The idea is to iteratively modify the weights such that the loss calculated from the observations is lower for each step. This process is called \"training\", as our model is learning the best weights from examples.\n",
    "\n",
    "With gradient descent, we update our weights using the gradient of the loss with respect to the weights\n",
    "\n",
    "$$\n",
    "w' = w + \\eta \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate, some small factor that scales the weight updates. The gradient is a measure of how much the loss changes when we change our weights. It also always points in the direction of greatest change.  \n",
    "\n",
    "<img src=\"assets/logreg_backward.png\" alt=\"Logistic regression forward pass\" width=400px>\n",
    "\n",
    "Now we have two processes here, a forward pass and a backward pass. In the forward pass, each operation or layer performs its calculations and passes the results on to the next layer. In the backward pass, each operation takes the gradient from the previous layer, multiplies it by it's own gradient, then passes it backward through the network.\n",
    "\n",
    "Keeping these concepts in mind - forward and backward passes through a sequence of layers or operations - we can build a framework for constructing any neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a concept I wish had been explained to me when I started learning about neural nets, and makes understanding how to implement them much more clear:\n",
    "\n",
    "* Each layer has a `forward` and `backward` method as before.\n",
    "* The `forward` method receives `input` as (you guessed it) input and outputs `self.output`. It stores as class variables:\n",
    "    * `input` as `self.last_input`.\n",
    "    * `self.output` as `self.output`.\n",
    "* The `backward` method receives `output_grad` as input and returns `input_grad` as its output. Along the way, it checks that:\n",
    "    * `output_grad` has the same shape as `self.output`.\n",
    "    * `input_grad` has the same shape as `self.last_input`.\n",
    "    \n",
    "When you try to trace what is going on in neural nets, it can often get confusing what layers are sending to and receiving from each other. This should make it clearer.\n",
    "\n",
    "This also gives us a template for `Layer`s in general. They should all look like:\n",
    "\n",
    "```python\n",
    "def forward(self, input: Tensor) -> Tensor:\n",
    "    \n",
    "    self.last_input = input\n",
    "    \n",
    "    ###############\n",
    "    # stuff happens\n",
    "    ###############\n",
    "    \n",
    "    return self.output\n",
    "```\n",
    "\n",
    "```python\n",
    "def backward(self, output_grad: Tensor) -> Tensor:\n",
    "    \n",
    "    assert_same_shape(self.output, output_grad)\n",
    "    \n",
    "    ###############\n",
    "    # stuff happens\n",
    "    ###############\n",
    "    \n",
    "    assert_same_shape(self.last_input, input_grad)    \n",
    "    return input_grad\n",
    "```\n",
    "\n",
    "Writing batch norm, convolutions (messy but already done), transformers etc. can be done using this structure!\n",
    "\n",
    "**Question**: is there a way to do this using decorators?\n",
    "\n",
    "I think introducing all of these concepts will help students generalize from implementing the \"basic, fully connected\" neural nets from below, to the more complicated stuff like convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Layer` base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    '''\n",
    "    Defining basic functions that all classes inheriting from Layer must implement.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def parameters(self):\n",
    "        yield from ()\n",
    "    \n",
    "    def grads(self):\n",
    "        yield from ()\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        return self.forward(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Linear` layer\n",
    "\n",
    "#### Forward pass\n",
    "The forward pass of the linear layer is fairly simple. We have three inputs: the features and our two parameters, the weights and the bias. We'll pass forward the linear combination of these.\n",
    "\n",
    "$$\n",
    "a = \\sum_i w_i x_i + b\n",
    "$$\n",
    "\n",
    "#### Backward pass\n",
    "\n",
    "The backwards pass is more complicated. Here we need to pass our gradients backwards to all of our inputs. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial a}{\\partial w_i} &= x_i \\\\\n",
    "\\frac{\\partial a}{\\partial b} &= 1 \\\\\n",
    "\\frac{\\partial a}{\\partial x_i} &= w_i \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We get $\\frac{\\partial L}{\\partial a}$ as input gradient into this layer, so we just need to multiply that by this linear layer's gradient to update the weights. We'll want to pass the gradient for $x$ backwards because these inputs could potentially be coming from another layer. But the gradients for $w$ and $b$ are used to update the parameters directly.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_i} &= \\frac{\\partial a}{\\partial w_i}\\frac{\\partial L}{\\partial a} = x_i\\frac{\\partial L}{\\partial a} \\\\ \\\\\n",
    "\\frac{\\partial L}{\\partial b} &= \\frac{\\partial a}{\\partial b}\\frac{\\partial L}{\\partial a} = \\frac{\\partial L}{\\partial a} \\\\ \\\\\n",
    "\\frac{\\partial L}{\\partial x_i} &= \\frac{\\partial a}{\\partial x_i}\\frac{\\partial L}{\\partial a} = w_i\\frac{\\partial L}{\\partial a}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, we need to consider cases where we have multiple examples and multiple outputs of our linear layer. This gets pretty difficult to understand without a lot of linear algebra experience, so I'll go through it step by step. If we have $K$ examples and $N$ features, then we would represent $\\mathbf{x}$ (bold means matrix) like so:\n",
    "\n",
    "<img src='assets/Features.png' width=400px>\n",
    "\n",
    "Each row in the matrix is one example from our dataset and each column corresponds to one of the features in our data set. Often, we'll want multiple outputs from our linear layer, we'll use $M$ to denote the number of outputs. The layer output $a$ from one example is now a vector instead of just a single number and we need a set of weights for each output. The weights can again be represented as a matrix:\n",
    "\n",
    "<img src='assets/Weights.png' width=400px>\n",
    "\n",
    "Here, each row corresponds to the weights for one feature and the columns correspond to the weights for one output. Our outputs will be the linear combination of the features _for each output_, _for each example_. \n",
    "\n",
    "$$\n",
    "\\mathbf{a} = \\mathbf{x} \\mathbf{W} + \\vec{b}\n",
    "$$\n",
    "\n",
    "<img src='assets/Outputs.png' width=400px>\n",
    "\n",
    "To do this, we take the matrix multiplication between the features $\\mathbf{x}$ and the weights $\\mathbf{W}$ (ignoring the bias term for simplicity). With matrix multiplication, you take the linear transformation of a _row_ in the first matrix with each _column_ in the second matrix to get the first _row_ of the resulting matrix. As shown below, we take the first example of $\\mathbf{x}$ and do a linear transformation with the first output column of $\\mathbf{W}$ to get the first output of the first example $a_{11}$. If you continue this, multiplying the first row of $\\mathbf{x}$ by each column in $\\mathbf{W}$, then you'll get all of the Linear layer outputs for the first example. If you do this for each example, you'll get all the outputs for all the examples. This is what a matrix multiplication does, and it is the basis for all computations in neural networks. You should note here that matrix multiplication only works when the number of columns in the first matrix equal the number of rows in the second matrix.\n",
    "\n",
    "<img src='assets/MatrixMult.png' width=700px>\n",
    "\n",
    "Finally, we should consider the gradients we receive in the backward pass. The gradients should be exactly the same shape as the layer output (otherwise something is wrong further on in the network). \n",
    "\n",
    "<img src='assets/Gradients.png' width=500px>\n",
    "\n",
    "Here, $\\nabla_{a_{km}}L$ means the gradient of the loss with respect to $a_{km}$. The gradient for the weights is now a matrix multiplication that looks like\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{W}}L = \\mathbf{x}\\nabla_{\\mathbf{a}} L\n",
    "$$\n",
    "\n",
    "However, $\\nabla_a L$ and $\\mathbf{x}$ have the same first dimension, $K$. And we need the resulting matrix to match the shape of $\\mathbf{W}$, which is $N\\times M$. What we can do is _transpose_ $\\mathbf{x}$, that is, swap the rows and columns to get a $N \\times K$ matrix. This can be multiplied with $\\nabla_{\\mathbf{a}} L$, with size $K \\times M$ to get the appropriate matrix for our weights. Then our appropriate calculation is:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{W}}L = \\mathbf{x}^T\\nabla_{\\mathbf{a}} L\n",
    "$$\n",
    "\n",
    "Similarly, we'll need to pass the input gradient backward through the network:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial a}{\\partial x_i}\\frac{\\partial L}{\\partial a} = w_i\\frac{\\partial L}{\\partial a}$$\n",
    "\n",
    "which you might write as $\\nabla_{\\mathbf{x}}L = \\mathbf{W}\\,\\nabla_{\\mathbf{a}}L$. However, to get the matrix multiplication to work out right, we need to take the transpose of $\\mathbf{W}$:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{x}}L = \\nabla_{\\mathbf{a}}L\\,\\mathbf{W}^T\n",
    "$$\n",
    "\n",
    "Programmatically, we'll take advantage of matrix multiplications using `torch.mm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, size: int) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.first = True\n",
    "        self.parameters_ = {'W': None, 'B': None}\n",
    "        self.grads_ = {'W': None, 'B': None}\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\" Takes a tensor and performs a linear (affine) transformation \"\"\"\n",
    "        \n",
    "        if input.dim() != 2:\n",
    "            raise DimensionError(f\"Tensor should have dimension 2, instead it has dimension {input.dim()}\")\n",
    "\n",
    "        self.last_input = input\n",
    "        \n",
    "        # Sets up the weights on the first iteration. Doing this so the\n",
    "        # input size isn't defined until we pass in our first tensor\n",
    "        if self.first:\n",
    "            n_input = input.size()[1]\n",
    "            \n",
    "            # Intialize a 2D tensor for the weights\n",
    "            self.W = torch.randn((n_input, self.size))*0.01\n",
    "            # Register the weight parameter\n",
    "            self.parameters_.update({'W': self.W})\n",
    "            \n",
    "            # Intialize the bias terms (one for each output value)\n",
    "            self.B = torch.randn((1, self.size))*0.01\n",
    "            # Register the bias parameter\n",
    "            self.parameters_.update({'B': self.B})\n",
    "            \n",
    "            self.first = False\n",
    "        \n",
    "        # The linear transformation here\n",
    "        self.output = torch.mm(self.last_input, self.W) + self.B\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, in_grad: Tensor) -> Tensor:\n",
    "        \"\"\" Takes a gradient from another operation, then calculates the gradients\n",
    "            for this layer's parameters, and returns the gradient for this layer to pass\n",
    "            backwards in the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Key assertion\n",
    "        if self.output.shape != in_grad.shape:\n",
    "            message = (f\"Two tensors should have the same shape; instead, first Tensor's shape \"\n",
    "                       f\"is {in_grad.shape} and second Tensor's shape is {self.output.shape}.\")\n",
    "            raise MatchError(message)\n",
    "        \n",
    "        # Number of examples\n",
    "        n = in_grad.shape[0]\n",
    "        \n",
    "        # Parameter gradients\n",
    "        x = self.last_input\n",
    "        dW = torch.mm(x.t(), in_grad)   # dL/dW\n",
    "        dB = torch.sum(in_grad, dim=0).view(*self.B.shape)   # dL/dB\n",
    "        \n",
    "        # Register parameter gradients\n",
    "        self.grads_.update({'W': dW})\n",
    "        self.grads_.update({'B': dB})\n",
    "        \n",
    "        # This layer's gradient which we'll pass on to previous layers, for dL/dx\n",
    "        backward_grad = torch.mm(in_grad, self.W.t())\n",
    "        \n",
    "        # Key assertion\n",
    "        if self.last_input.shape != backward_grad.shape:\n",
    "            message = (f\"Two tensors should have the same shape; instead, first Tensor's shape \"\n",
    "                       f\"is {self.last_input.shape} and second Tensor's shape is {backward_grad.shape}.\")\n",
    "            raise MatchError(message)\n",
    "\n",
    "        return backward_grad\n",
    "    \n",
    "    def parameters(self):\n",
    "        for param in self.parameters_.values():\n",
    "            yield param\n",
    "    \n",
    "    def grads(self):\n",
    "        for param in self.parameters_:\n",
    "            yield self.grads_[param]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Linear({self.size})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-02 *\n",
      "       [[-0.3811],\n",
      "        [-2.4835],\n",
      "        [ 3.9679],\n",
      "        [-0.3602],\n",
      "        [-1.0610],\n",
      "        [ 2.2594],\n",
      "        [ 0.9376],\n",
      "        [-0.1753],\n",
      "        [ 0.3777],\n",
      "        [ 1.2145]])\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 9.4571, -3.8411, -1.7153, -8.7004, -4.9365, -9.1825, -3.0859],\n",
      "        [ 5.0161, -2.0374, -0.9098, -4.6148, -2.6183, -4.8705, -1.6368],\n",
      "        [ 2.8525, -1.1586, -0.5174, -2.6243, -1.4890, -2.7697, -0.9308],\n",
      "        [ 8.9318, -3.6278, -1.6200, -8.2171, -4.6622, -8.6724, -2.9145],\n",
      "        [ 3.4333, -1.3945, -0.6227, -3.1585, -1.7921, -3.3336, -1.1203],\n",
      "        [ 4.9930, -2.0280, -0.9056, -4.5934, -2.6062, -4.8480, -1.6292],\n",
      "        [ 7.6109, -3.0913, -1.3805, -7.0019, -3.9728, -7.3899, -2.4835],\n",
      "        [ 7.9371, -3.2238, -1.4396, -7.3020, -4.1430, -7.7066, -2.5899],\n",
      "        [ 5.2587, -2.1359, -0.9538, -4.8379, -2.7449, -5.1060, -1.7159],\n",
      "        [ 1.9620, -0.7969, -0.3559, -1.8050, -1.0241, -1.9051, -0.6402]])\n"
     ]
    }
   ],
   "source": [
    "# Testing out our new Linear layer\n",
    "linear = Linear(1)\n",
    "x = torch.randn((10, 7))\n",
    "a = linear.forward(x)\n",
    "grad = torch.rand_like(a)\n",
    "print(a)\n",
    "print(linear.backward(grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Sigmoid` layer\n",
    "\n",
    "#### Forward pass\n",
    "The forward pass of the sigmoid layer should calculate\n",
    "\n",
    "$$\n",
    "\\sigma(a) = \\frac{1}{1+e^{-a}}\n",
    "$$\n",
    "\n",
    "For our logistic regression problem, this will be $p$ the probability of a \"successful\" outcome. This layer can also be used in multilayer networks as the activation function for hidden layers. We'll perform this calculation element-wise.\n",
    "\n",
    "#### Backward pass\n",
    "\n",
    "Denoting the output of this layer as $p$, the sigmoid layer should receive $\\partial L \\mathbin{/} \\partial p$ in the backward pass. Then it should return\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a} = \\frac{\\partial p}{\\partial a} \\frac{\\partial L}{\\partial p}\n",
    "$$\n",
    "\n",
    "to send backward through the network.\n",
    "\n",
    "The gradient of the sigmoid layer is then\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a} = p\\,(1-p)\\frac{\\partial L}{\\partial p}\n",
    "$$\n",
    "\n",
    "which I'll let you work out if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    '''\n",
    "    Sigmoid activation function\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \n",
    "        self.last_input = input\n",
    "        \n",
    "        self.output = 1.0/(1.0+torch.exp(-1.0 * input))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, in_grad: Tensor) -> Tensor:\n",
    "\n",
    "        # Key assertion\n",
    "        if self.output.shape != in_grad.shape:\n",
    "            message = (f\"Two tensors should have the same shape; instead, first Tensor's shape \"\n",
    "                       f\"is {in_grad.shape} and second Tensor's shape is {self.output.shape}.\")\n",
    "            raise MatchError(message)           \n",
    "        \n",
    "        sigmoid_backward = self.output*(1.0-self.output)\n",
    "        backward_grad = sigmoid_backward * in_grad\n",
    "        \n",
    "        # Key assertion\n",
    "        if self.last_input.shape != backward_grad.shape:\n",
    "            message = (f\"Two tensors should have the same shape; instead, first Tensor's shape \"\n",
    "                       f\"is {self.last_input.shape} and second Tensor's shape is {backward_grad.shape}.\")\n",
    "            raise MatchError(message)\n",
    "        \n",
    "        return backward_grad\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2199],\n",
       "        [ 0.1166],\n",
       "        [ 0.0663],\n",
       "        [ 0.2077],\n",
       "        [ 0.0798],\n",
       "        [ 0.1161],\n",
       "        [ 0.1770],\n",
       "        [ 0.1846],\n",
       "        [ 0.1223],\n",
       "        [ 0.0456]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing out new Sigmoid layer\n",
    "sigmoid = Sigmoid()\n",
    "p = sigmoid.forward(a)\n",
    "grads = torch.rand_like(p)\n",
    "sigmoid.backward(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the layers necessary for our little network, we can stack them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameter gradients:  [None, None]\n",
      "Parameter gradients after backward pass:  [tensor([[ 0.0377],\n",
      "        [-0.8707],\n",
      "        [-0.5948],\n",
      "        [-0.0844],\n",
      "        [ 0.2132],\n",
      "        [-0.4861],\n",
      "        [-1.9651]]), tensor([[-4.9994]])]\n"
     ]
    }
   ],
   "source": [
    "# Generate some data, 20 examples, 7 features\n",
    "features = x = torch.randn((20, 7))\n",
    "labels = torch.randint(0, 2, (20, 1))\n",
    "\n",
    "layers = [Linear(1), Sigmoid()]\n",
    "\n",
    "print('Initial parameter gradients: ', [grad for grad in layers[0].grads()])\n",
    "\n",
    "# Forward pass through our network\n",
    "for layer in layers:\n",
    "    x = layer.forward(x)\n",
    "\n",
    "# Backward pass through our network\n",
    "grad = -torch.ones(20, 1)\n",
    "for layer in reversed(layers):\n",
    "    grad = layer.backward(grad)\n",
    "    \n",
    "print('Parameter gradients after backward pass: ', [grad for grad in layers[0].grads()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a new class called `Sequential` that can do what we wrote above, take sequential layers and build them into a sequential graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Layer):\n",
    "    \n",
    "    def __init__(self, *layers: typing.Type[Layer]):\n",
    "        super().__init__()\n",
    "        self.layers = tuple(layers)\n",
    "              \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def backward(self, grad: Tensor = None) -> Tensor:\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "    \n",
    "    def parameters(self):\n",
    "        for layer in self.layers:\n",
    "            yield from layer.parameters()\n",
    "        \n",
    "    def grads(self):\n",
    "        for layer in self.layers:\n",
    "            yield from layer.grads()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.layers)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        layer_strs = [str(layer) for layer in self.layers]\n",
    "        return f\"{self.__class__.__name__}(\\n  \" + \",\\n  \".join(layer_strs) + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential layers:  (Linear(1), Sigmoid)\n",
      "Initial parameters and grads: \n",
      " [None, None] \n",
      " [None, None]\n",
      "Forward pass loss:  tensor([[ 0.4912],\n",
      "        [ 0.5006],\n",
      "        [ 0.4809],\n",
      "        [ 0.4872],\n",
      "        [ 0.4886],\n",
      "        [ 0.4903],\n",
      "        [ 0.5047],\n",
      "        [ 0.4983],\n",
      "        [ 0.4957],\n",
      "        [ 0.5039]])\n",
      "New parameters: \n",
      " [tensor(1.00000e-02 *\n",
      "       [[ 0.0225],\n",
      "        [-1.6493],\n",
      "        [-0.4493],\n",
      "        [ 0.4861],\n",
      "        [-0.2228],\n",
      "        [ 2.1634],\n",
      "        [ 0.0527]]), tensor(1.00000e-02 *\n",
      "       [[-2.8281]])]\n",
      "Backward pass input gradient: \n",
      " tensor(1.00000e-03 *\n",
      "       [[ 0.0382, -2.8037, -0.7638,  0.8262, -0.3787,  3.6775,  0.0895],\n",
      "        [ 0.0520, -3.8222, -1.0413,  1.1264, -0.5163,  5.0134,  0.1221],\n",
      "        [ 0.0082, -0.6009, -0.1637,  0.1771, -0.0812,  0.7882,  0.0192],\n",
      "        [ 0.0009, -0.0672, -0.0183,  0.0198, -0.0091,  0.0882,  0.0021],\n",
      "        [ 0.0327, -2.3983, -0.6534,  0.7068, -0.3239,  3.1458,  0.0766],\n",
      "        [ 0.0004, -0.0319, -0.0087,  0.0094, -0.0043,  0.0418,  0.0010],\n",
      "        [ 0.0505, -3.7099, -1.0107,  1.0933, -0.5011,  4.8662,  0.1185],\n",
      "        [ 0.0327, -2.3984, -0.6534,  0.7068, -0.3240,  3.1459,  0.0766],\n",
      "        [ 0.0298, -2.1876, -0.5960,  0.6447, -0.2955,  2.8694,  0.0699],\n",
      "        [ 0.0558, -4.0991, -1.1168,  1.2080, -0.5537,  5.3767,  0.1309]])\n",
      "New parameter gradients: \n",
      " [tensor([[-0.1429],\n",
      "        [-0.5382],\n",
      "        [-0.5502],\n",
      "        [ 0.9996],\n",
      "        [ 0.4050],\n",
      "        [ 0.5748],\n",
      "        [-0.0084]]), tensor([[ 1.3411]])]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "features = torch.randn(n, 7)\n",
    "seq = Sequential(Linear(1), Sigmoid())\n",
    "\n",
    "print(\"Sequential layers: \", seq.layers)\n",
    "print(\"Initial parameters and grads: \\n\", [param for param in seq.parameters()], \n",
    "      \"\\n\", [param for param in seq.parameters()])\n",
    "print(\"Forward pass loss: \", seq.forward(features))\n",
    "print(\"New parameters: \\n\", [param for param in seq.parameters()])\n",
    "print(\"Backward pass input gradient: \\n\", seq.backward(torch.rand(n,1)))\n",
    "print(\"New parameter gradients: \\n\", [grad for grad in seq.grads()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this `Sequential` class to construct new layers that are a combination of layers. For example, we can combine the Linear and Sigmoid layers into one layer we'll call `Dense`, following after Keras. With this layer, you'll be able to define what operation you want to use as an activation function following the linear transformation in one class.\n",
    "\n",
    "<img src='assets/dense_layer.png' width=300px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Sequential):\n",
    "    def __init__(self, size: int, activation: typing.Any = 'sigmoid') -> None:\n",
    "        \n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = Sigmoid()\n",
    "        else:\n",
    "            self.activation = activation\n",
    "        \n",
    "        super().__init__(Linear(size), self.activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4992,  0.4926,  0.5045,  0.5021,  0.4944,  0.5023,  0.5013,\n",
       "          0.5000,  0.5137,  0.5029]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = Dense(10)\n",
    "dense(torch.rand((1,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `LogLoss` layer\n",
    "\n",
    "Finally we'll build a layer for the log loss, specifically for the logistic regression problem. Sometimes this is also called the cross-entropy loss, but this loss updates based on positive and negative labels, while the cross-entropy loss only updates on positive labels. \n",
    "\n",
    "#### Forward pass\n",
    "Our loss here for true labels $y$ and prediction probabilities $y$ is \n",
    "\n",
    "$$\n",
    "L = \\sum_k^K -y^k \\log{p^k} - (1-y^k)\\log{(1-p^k)}\n",
    "$$\n",
    "\n",
    "#### Backward pass\n",
    "\n",
    "For the backward pass, we need the gradient of $L$ with respect to $p$, which we'll send backward in the network.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial p} = \\sum_k -\\frac{y^k}{p^k} + \\frac{1 - y^k}{1 - p^k}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \"\"\" Base class for losses \"\"\"\n",
    "    def __init__(self, network: typing.Type[Layer]):\n",
    "        self.network = network\n",
    "    \n",
    "    def forward(self, input: Tensor, targets: Tensor) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self) -> Tensor:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def __call__(self, input: Tensor, targets: Tensor) -> float:\n",
    "        return self.forward(input, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLoss(Loss):\n",
    "    \"\"\" Log loss error specifically for logistic regression, requires a sequence of layers as input \"\"\"\n",
    "    def __init__(self, network: typing.Type[Layer], eta=1e-9):\n",
    "        super().__init__(network)\n",
    "        \n",
    "        # Small parameter to avoid explosions when our probabilities get small\n",
    "        # A better way to do this is use log probabilities everywhere\n",
    "        self.eta = eta\n",
    "        \n",
    "    def forward(self, features: Tensor, labels: Tensor) -> float:\n",
    "        \n",
    "        self.last_input = p = self.network(features)\n",
    "        self.labels = y = labels\n",
    "        \n",
    "        loss = torch.sum(-y*torch.log(p + self.eta) - (1-y)*torch.log(1 - p + self.eta))\n",
    "        return loss.item()\n",
    "    \n",
    "    def backward(self) -> None:\n",
    "        y, p = self.labels, self.last_input\n",
    "        n = y.shape[0]\n",
    "        \n",
    "        backward_grad = torch.sum(-y/(p + self.eta) + (1-y)/(1 - p  + self.eta), dim=1).view(n, -1)\n",
    "        \n",
    "        # Calculate gradients for the network\n",
    "        self.network.backward(backward_grad)\n",
    "        return None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"LogLoss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forward loss:  13.897012710571289\n",
      "Parameter gradients before backward pass: \n",
      " [None, None]\n",
      "\n",
      "Parameter gradients after backward pass: \n",
      " [tensor([[-2.3485],\n",
      "        [ 0.4376],\n",
      "        [-0.9617],\n",
      "        [ 0.7514],\n",
      "        [ 1.8838],\n",
      "        [ 1.8393],\n",
      "        [-0.3215]]), tensor([[-0.9540]])]\n"
     ]
    }
   ],
   "source": [
    "# Testing out new LogLoss layer\n",
    "# Create some fake data\n",
    "x = torch.randn((20, 7))\n",
    "y = torch.randint(0, 2, (x.shape[0], 1))\n",
    "\n",
    "# Create our network\n",
    "net = Sequential(Linear(1), Sigmoid())\n",
    "\n",
    "# Define the loss\n",
    "loss = LogLoss(net)\n",
    "\n",
    "# Forward pass to get our loss\n",
    "print(\"\\nForward loss: \", loss.forward(x, y))\n",
    "\n",
    "# Backward pass to calculate gradients\n",
    "print(\"Parameter gradients before backward pass: \\n\", [grad for grad in net.grads()])\n",
    "loss.backward()\n",
    "print(\"\\nParameter gradients after backward pass: \\n\", [grad for grad in net.grads()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these building blocks, you can theoretically construct any possible neural network architecture. We can now build a complete network for a logistic regression model. We can pass data forward through the network, and pass our gradients backwards to update our parameters. Now we'll see how we can use these gradients for the actual update step, and train our network to accurately [diagnose breast cancer][1].\n",
    "\n",
    "[1]: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use stochastic gradient descent (SGD) to update our parameters:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_{new} = \\mathbf{W} - \\eta \\nabla_{\\mathbf{W}}L\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, network: typing.Type[Layer], lr: float = 0.003):\n",
    "        self.network = network\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        for param, grad in zip(self.network.parameters(), self.network.grads()):\n",
    "            param.sub_(self.lr*grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters before update step: \n",
      " [tensor(1.00000e-02 *\n",
      "       [[ 0.5434],\n",
      "        [-1.0467],\n",
      "        [-3.0068],\n",
      "        [ 1.1508],\n",
      "        [-0.1886],\n",
      "        [-0.5810],\n",
      "        [-0.9466]]), tensor(1.00000e-03 *\n",
      "       [[-1.2028]])]\n",
      "\n",
      "Parameters after update step: \n",
      " [tensor(1.00000e-02 *\n",
      "       [[-0.0601],\n",
      "        [-0.9353],\n",
      "        [-2.1915],\n",
      "        [-0.3362],\n",
      "        [ 1.2908],\n",
      "        [-1.1787],\n",
      "        [-1.3754]]), tensor(1.00000e-03 *\n",
      "       [[ 7.8129]])]\n"
     ]
    }
   ],
   "source": [
    "# Testing out new LogLoss layer\n",
    "# Create some fake data\n",
    "x = torch.randn((20, 7))\n",
    "y = torch.randint(0, 2, (x.shape[0], 1))\n",
    "\n",
    "# Define network, loss, and optimizer\n",
    "net = Sequential(Linear(1), Sigmoid())\n",
    "loss = LogLoss(net)\n",
    "optim = SGD(net)\n",
    "\n",
    "# Forward pass to get our loss\n",
    "forward_loss = loss.forward(x, y)\n",
    "loss.backward()\n",
    "print(\"\\nParameters before update step: \\n\", [param for param in net.parameters()])\n",
    "optim.step()\n",
    "print(\"\\nParameters after update step: \\n\", [param for param in net.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In this part, we'll be using the components we've created to train a logistic regression model to predict breast cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep breast cancer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sklearn` loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()\n",
    "features = breast_cancer.data\n",
    "labels = breast_cancer.target\n",
    "feature_names = breast_cancer.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(arr: np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    means = arr.mean(axis=0)\n",
    "    stds = arr.std(axis=0)\n",
    "    \n",
    "    return (arr - means) / stds\n",
    "\n",
    "features = standardize(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(features: np.ndarray, \n",
    "                     labels: np.ndarray,\n",
    "                     size: int = 32,\n",
    "                     shuffle: bool = True) -> Tuple[Tensor, Tensor]:\n",
    "    \n",
    "    if features.shape[0] != labels.shape[0]:\n",
    "        raise ValueError('feature and label arrays must have the same first dimension')\n",
    "    \n",
    "    n = features.shape[0]\n",
    "    \n",
    "    if shuffle:\n",
    "        idx = np.arange(n)\n",
    "        shuffled = np.random.shuffle(idx)\n",
    "        features = features[shuffled].reshape((n, -1)) \n",
    "        labels = labels[shuffled].reshape((n, 1))\n",
    "    \n",
    "    for ii in range(0, n, size):\n",
    "        out_features = torch.from_numpy(features[ii:ii+size, :]).type(torch.FloatTensor)\n",
    "        out_labels = torch.from_numpy(labels[ii:ii+size, :]).type(torch.FloatTensor)\n",
    "        yield out_features, out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0601\n",
      "0.0547\n",
      "0.0522\n",
      "0.0507\n",
      "0.0496\n",
      "0.0487\n",
      "0.0479\n",
      "0.0473\n",
      "0.0467\n",
      "0.0462\n",
      "0.0457\n",
      "0.0453\n",
      "0.0449\n",
      "0.0446\n",
      "0.0442\n",
      "0.0439\n",
      "0.0436\n",
      "0.0433\n",
      "0.0431\n",
      "0.0428\n",
      "0.0426\n",
      "0.0424\n",
      "0.0422\n",
      "0.0420\n",
      "0.0418\n"
     ]
    }
   ],
   "source": [
    "network = Dense(1)\n",
    "loss = LogLoss(network)\n",
    "optim = SGD(network, lr=0.01)\n",
    "\n",
    "epochs = 500\n",
    "print_every = 100\n",
    "steps = 0\n",
    "for e in range(epochs):\n",
    "    train_loss = 0\n",
    "    for x, y in generate_batches(features, labels, size=128):\n",
    "        steps += 1\n",
    "        train_loss += loss(x, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    if steps % print_every == 0: \n",
    "        print(f\"{train_loss/len(features):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a metric to determine how well our network is performing. A common metric for classification problems like this is accuracy, correct predictions divided by all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions: np.ndarray, labels: np.ndarray) -> float:\n",
    "    accuracy = np.mean(predictions.squeeze() == labels.squeeze())\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 98.946%\n"
     ]
    }
   ],
   "source": [
    "ps = network(torch.from_numpy(features).type(torch.FloatTensor))\n",
    "predictions = np.round(ps.numpy())\n",
    "acc = accuracy(predictions, labels)\n",
    "print(f\"Accuracy on training data: {acc*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, network: typing.Type[Layer], \n",
    "                       loss: Loss=LogLoss, \n",
    "                       optimizer: typing.Any=SGD, \n",
    "                       metric: typing.Callable=accuracy, \n",
    "                       batch_gen: typing.Callable=generate_batches):\n",
    "        self.network = network\n",
    "        if loss is not LogLoss:\n",
    "            self.loss = loss\n",
    "        else:\n",
    "            self.loss = LogLoss(network)\n",
    "        \n",
    "        if optimizer is not SGD:\n",
    "            self.optim = optimizer\n",
    "        else:\n",
    "            self.optim = optimizer(network)\n",
    "        \n",
    "        self.metric = metric\n",
    "        self.batch_gen = batch_gen\n",
    "        \n",
    "    def fit(self, features: np.ndarray, labels: np.ndarray, \n",
    "                  epochs: int=500, print_every: int=100, \n",
    "                  batch_size: int=32)-> None:\n",
    "        steps = 0\n",
    "        for e in range(epochs):\n",
    "            running_loss = 0\n",
    "            for ii, (x, y) in enumerate(self.batch_gen(features, labels, size=batch_size)):\n",
    "                steps += 1\n",
    "                running_loss += self.loss(x, y)\n",
    "                self.loss.backward()\n",
    "                self.optim.step()\n",
    "            \n",
    "                if steps % print_every == 0:\n",
    "                    ps = self.network(torch.from_numpy(features).type(torch.FloatTensor))\n",
    "                    predictions = np.round(ps.numpy())\n",
    "                    acc = accuracy(predictions, labels)\n",
    "                    print(f\"Epoch {e+1}.. Train loss: {running_loss/print_every:.4f}.. \", f\"Accuracy: {acc*100:.3f}%\")\n",
    "                    running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20.. Train loss: 0.9173..  Accuracy: 95.958%\n",
      "Epoch 40.. Train loss: 0.5047..  Accuracy: 98.067%\n",
      "Epoch 60.. Train loss: 0.4117..  Accuracy: 98.418%\n",
      "Epoch 80.. Train loss: 0.3696..  Accuracy: 98.594%\n",
      "Epoch 100.. Train loss: 0.3446..  Accuracy: 98.594%\n",
      "Epoch 120.. Train loss: 0.3276..  Accuracy: 98.594%\n",
      "Epoch 140.. Train loss: 0.3152..  Accuracy: 98.594%\n",
      "Epoch 160.. Train loss: 0.3056..  Accuracy: 98.594%\n",
      "Epoch 180.. Train loss: 0.2980..  Accuracy: 98.594%\n",
      "Epoch 200.. Train loss: 0.2917..  Accuracy: 98.770%\n",
      "Epoch 220.. Train loss: 0.2864..  Accuracy: 98.770%\n",
      "Epoch 240.. Train loss: 0.2819..  Accuracy: 98.770%\n",
      "Epoch 260.. Train loss: 0.2779..  Accuracy: 98.770%\n",
      "Epoch 280.. Train loss: 0.2744..  Accuracy: 98.770%\n",
      "Epoch 300.. Train loss: 0.2712..  Accuracy: 98.770%\n",
      "Epoch 320.. Train loss: 0.2682..  Accuracy: 98.770%\n",
      "Epoch 340.. Train loss: 0.2655..  Accuracy: 98.770%\n",
      "Epoch 360.. Train loss: 0.2630..  Accuracy: 98.770%\n",
      "Epoch 380.. Train loss: 0.2606..  Accuracy: 98.770%\n",
      "Epoch 400.. Train loss: 0.2583..  Accuracy: 98.770%\n",
      "Epoch 420.. Train loss: 0.2562..  Accuracy: 98.770%\n",
      "Epoch 440.. Train loss: 0.2541..  Accuracy: 98.770%\n",
      "Epoch 460.. Train loss: 0.2520..  Accuracy: 98.770%\n",
      "Epoch 480.. Train loss: 0.2499..  Accuracy: 98.946%\n",
      "Epoch 500.. Train loss: 0.2476..  Accuracy: 98.946%\n"
     ]
    }
   ],
   "source": [
    "network = Sequential(\n",
    "            Dense(10),\n",
    "            Dense(1))\n",
    "model = Classifier(network, loss=LogLoss(network), optimizer=SGD(network))\n",
    "model.fit(features, labels, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I moved all of this code into a package called `Lincoln`, which we can easily use to build neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lincoln as lnc\n",
    "from lincoln.layers import Dense, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20.. Train loss: 0.8092..  Accuracy: 96.661%\n",
      "Epoch 40.. Train loss: 0.4813..  Accuracy: 98.243%\n",
      "Epoch 60.. Train loss: 0.3998..  Accuracy: 98.418%\n",
      "Epoch 80.. Train loss: 0.3611..  Accuracy: 98.594%\n",
      "Epoch 100.. Train loss: 0.3376..  Accuracy: 98.594%\n",
      "Epoch 120.. Train loss: 0.3216..  Accuracy: 98.594%\n",
      "Epoch 140.. Train loss: 0.3098..  Accuracy: 98.594%\n",
      "Epoch 160.. Train loss: 0.3006..  Accuracy: 98.594%\n",
      "Epoch 180.. Train loss: 0.2933..  Accuracy: 98.770%\n",
      "Epoch 200.. Train loss: 0.2873..  Accuracy: 98.770%\n",
      "Epoch 220.. Train loss: 0.2821..  Accuracy: 98.770%\n",
      "Epoch 240.. Train loss: 0.2777..  Accuracy: 98.770%\n",
      "Epoch 260.. Train loss: 0.2738..  Accuracy: 98.770%\n",
      "Epoch 280.. Train loss: 0.2702..  Accuracy: 98.770%\n",
      "Epoch 300.. Train loss: 0.2670..  Accuracy: 98.770%\n",
      "Epoch 320.. Train loss: 0.2639..  Accuracy: 98.770%\n",
      "Epoch 340.. Train loss: 0.2611..  Accuracy: 98.770%\n",
      "Epoch 360.. Train loss: 0.2584..  Accuracy: 98.770%\n",
      "Epoch 380.. Train loss: 0.2558..  Accuracy: 98.770%\n",
      "Epoch 400.. Train loss: 0.2534..  Accuracy: 98.770%\n",
      "Epoch 420.. Train loss: 0.2510..  Accuracy: 98.946%\n",
      "Epoch 440.. Train loss: 0.2487..  Accuracy: 98.946%\n",
      "Epoch 460.. Train loss: 0.2464..  Accuracy: 98.946%\n",
      "Epoch 480.. Train loss: 0.2442..  Accuracy: 98.946%\n",
      "Epoch 500.. Train loss: 0.2420..  Accuracy: 98.946%\n"
     ]
    }
   ],
   "source": [
    "network = Sequential(\n",
    "            Dense(10),\n",
    "            Dense(1))\n",
    "model = lnc.models.Classifier(network, loss=lnc.losses.LogLoss(network), optimizer=lnc.optim.SGD(network))\n",
    "model.fit(features, labels, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
