{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "from numpy import ndarray as array\n",
    "from numpy import random\n",
    "\n",
    "from helper import (to_2d,\n",
    "                    one_col_to_two)\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shape(output: Tensor, \n",
    "                      output_grad: Tensor):\n",
    "    assert output.shape == output_grad.shape, \\\n",
    "    '''\n",
    "    Two tensors should have the same shape; instead, first Tensor's shape is {0}\n",
    "    and second Tensor's shape is {1}.\n",
    "    '''.format(tuple(output_grad.shape), tuple(output.shape))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_dim(tensor: Tensor, \n",
    "               dim: int):\n",
    "    assert tensor.dim() == dim, \\\n",
    "    '''\n",
    "    Tensor should have dimension {0}, instead it has dimension {1}\n",
    "    '''.format(dim, tensor.dim())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Layer` base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    '''\n",
    "    Defining basic functions that all classes inheriting from Layer must implement.\n",
    "    '''\n",
    "\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Loss` base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    '''\n",
    "    Defining basic functions that all classes inheriting from Loss must implement.\n",
    "    '''\n",
    "\n",
    "    def loss_grad(self, input):\n",
    "        \"\"\"\n",
    "        This function should\n",
    "        * Return the loss as a number.\n",
    "        * Store the gradient of the loss as self.grad\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a concept I wish had been explained to me when I started learning about neural nets, and makes understanding how to implement them much more clear:\n",
    "\n",
    "* Each layer has a `forward` and `backward` method as before.\n",
    "* The `forward` method receives `input` as (you guessed it) input and outputs `self.output`. It stores as class variables:\n",
    "    * `input` as `self.last_input`.\n",
    "    * `self.output` as `self.output`.\n",
    "* The `backward` method receives `output_grad` as input and returns `input_grad` as its output. Along the way, it checks that:\n",
    "    * `output_grad` has the same shape as `self.output`.\n",
    "    * `input_grad` has the same shape as `self.last_input`.\n",
    "    \n",
    "When you try to trace what is going on in neural nets, it can often get confusing what layers are sending to and receiving from each other. This should make it clearer.\n",
    "\n",
    "This also gives us a template for `Layer`s in general. They should all look like:\n",
    "\n",
    "```python\n",
    "def forward(self, input: Tensor) -> Tensor:\n",
    "    \n",
    "    self.last_input = input\n",
    "    \n",
    "    ###############\n",
    "    # stuff happens\n",
    "    ###############\n",
    "    \n",
    "    return self.output\n",
    "```\n",
    "\n",
    "```python\n",
    "def backward(self, output_grad: Tensor) -> Tensor:\n",
    "    \n",
    "    assert_same_shape(self.output, output_grad)\n",
    "    \n",
    "    ###############\n",
    "    # stuff happens\n",
    "    ###############\n",
    "    \n",
    "    assert_same_shape(self.last_input, input_grad)    \n",
    "    return input_grad\n",
    "```\n",
    "\n",
    "Writing batch norm, convolutions (messy but already done), transformers etc. can be done using this structure!\n",
    "\n",
    "**Question**: is there a way to do this using decorators?\n",
    "\n",
    "Finally, there's a similar assertion with the `loss_grad` function in the `Loss` class. This function will take in `prediction` as input and calculate `self.grad`, and it must assert that these are the same shape.\n",
    "\n",
    "```python\n",
    "def loss_grad(self, prediction: Tensor) -> Tensor:\n",
    "    \n",
    "    \n",
    "    ###############\n",
    "    # stuff happens\n",
    "    ###############\n",
    "    \n",
    "    assert_same_shape(prediction, self.grad)\n",
    "    return loss_grad\n",
    "```\n",
    "\n",
    "I think introducing all of these concepts will help students generalize from implementing the \"basic, fully connected\" neural nets from below, to the more complicated stuff like convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Linear` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, \n",
    "                 n_out: int, \n",
    "                 bias: bool = False) -> None:\n",
    "        self.n_out = n_out\n",
    "        self.first = True\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "\n",
    "        assert_dim(input, 2)\n",
    "\n",
    "        self.last_input = input\n",
    "        \n",
    "        # Sets up the weights on the first iteration\n",
    "        if self.first:\n",
    "            n_input = input.size()[1]\n",
    "            self.W = torch.randn((n_input, self.n_out))\n",
    "            self.B = torch.randn((1, self.n_out))\n",
    "                \n",
    "            self.first = False\n",
    "\n",
    "        self.output = torch.mm(self.last_input, self.W) + self.B\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: Tensor) -> Tensor:\n",
    "\n",
    "        # Key assertion\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        \n",
    "        n = output_grad.shape[0]\n",
    "        \n",
    "        self.dW = torch.mm(self.last_input.transpose(0, 1), output_grad)\n",
    "        self.dB = to_2d(torch.sum(output_grad, dim=0), \"row\")\n",
    "        \n",
    "        input_grad = output_grad.mm(self.W.transpose(0, 1))\n",
    "        \n",
    "        # Key assertion        \n",
    "        assert_same_shape(self.last_input, input_grad)\n",
    "        \n",
    "        return input_grad\n",
    "    \n",
    "    def update_params(self,  \n",
    "                      method: str, \n",
    "                      learning_rate: float) -> None:\n",
    "        \n",
    "        methods = [\"sgd\"]\n",
    "        \n",
    "        assert method in methods,\\\n",
    "        \"Method must be one of {0}\".format(methods)\n",
    "\n",
    "        if method == \"sgd\":\n",
    "            self.W = self.W - learning_rate * self.dW\n",
    "            self.B = self.B - learning_rate * self.dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Linear(ParamLayer):\n",
    "#     def __init__(self, n_out: int) -> None:\n",
    "#         self.n_out = n_out\n",
    "#         self.first = True\n",
    "\n",
    "#     def forward(self, input: Tensor) -> Tensor:\n",
    "        \n",
    "#         assert input.dim() == 2, \\\n",
    "#         \"Input to Linear layer must have dimension 2, instead input has dimension {0}\".\\\n",
    "#         format(input.dim())\n",
    "        \n",
    "#         if self.first:\n",
    "#             n_input = input.size()[1]\n",
    "#             self.W = torch.randn((n_input, self.n_out))\n",
    "#             self.b = torch.ones(self.n_out)\n",
    "#             self.first = False\n",
    "#         self.last_input = input\n",
    "#         self.output = torch.mm(self.last_input, self.W) + self.b\n",
    "        \n",
    "#         return self.output\n",
    "\n",
    "#     def backward(self, output_grad: Tensor) -> Tensor:\n",
    "        \n",
    "#         assert_grad_shape(self.output, output_grad)\n",
    "        \n",
    "#         n = output_grad.shape[0]\n",
    "        \n",
    "#         self.dW = torch.mm(self.last_input.T, output_grad)/n\n",
    "#         self.db = torch.mean(output_grad, axis=0)\n",
    "        \n",
    "#         return output_grad.mm(self.W.T)\n",
    "    \n",
    "#     def params(self) -> Tuple[Tensor]:\n",
    "#         return self.W, self.b\n",
    "    \n",
    "#     def param_derivs(self) -> Tuple[Tensor]:\n",
    "#         return self.dW, self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    '''\n",
    "    Sigmoid activation function\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \n",
    "        self.last_input = input\n",
    "        \n",
    "        self.output = 1.0/(1.0+torch.exp(-1.0 * input))\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: Tensor) -> Tensor:\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)           \n",
    "        \n",
    "        sigmoid_backward = self.output*(1.0-self.output)\n",
    "        \n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        \n",
    "        assert_same_shape(self.last_input, input_grad)\n",
    "        \n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLogistic(Layer):\n",
    "    '''\n",
    "    This layer is just for logistic regression:\n",
    "    The forward pass transforms the one column output of logistic regression into two \n",
    "    columns, with the second column as \"1 minus\" the first.\n",
    "    The backward pass - in keeping with the forward-backward structure established above -\n",
    "    receives two dimensional input from the loss function above it and passes one\n",
    "    dimensional input to the layer below it.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \n",
    "        self.last_input = input\n",
    "        \n",
    "        input_2d = one_col_to_two(input)        \n",
    "\n",
    "        def _softmax_row(row: Tensor) -> Tensor:\n",
    "\n",
    "            exp_obs = torch.exp(row)\n",
    "            sum_exp_obs = exp_obs.sum().item()\n",
    "            softmax_obs = exp_obs / sum_exp_obs\n",
    "\n",
    "            return softmax_obs\n",
    "        \n",
    "        output_rows = []\n",
    "        for obs in range(input_2d.shape[0]):\n",
    "            output_row = to_2d(_softmax_row(input_2d[obs]), \"row\")\n",
    "            output_rows.append(output_row)\n",
    "\n",
    "        self.output = torch.cat(output_rows)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: Tensor) -> Tensor:\n",
    "        '''\n",
    "        For the purposes of this logistic regression, we'll select just the first \n",
    "        column of the output_grad.\n",
    "        '''\n",
    "        \n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        \n",
    "        input_grad = to_2d(output_grad[:, 0], \"col\")\n",
    "    \n",
    "        assert_same_shape(self.last_input, input_grad)\n",
    "        \n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Loss):\n",
    "    '''\n",
    "    Receives the two column output of a logistic regression where the output has been\n",
    "    fed through softmax.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def loss_grad(self, \n",
    "                  prediction: Tensor, \n",
    "                  actual: Tensor) -> float:\n",
    "        \n",
    "        assert_dim(prediction, 2)\n",
    "        \n",
    "        # Compute log loss\n",
    "        log_loss = -1.0 * actual * torch.log(prediction) - (1.0 - actual) * torch.log(1 - prediction)        \n",
    "        log_loss_val = torch.mean(log_loss).item()\n",
    "        \n",
    "        # Compute derivative - see calculations in book chapter\n",
    "        self.grad = prediction - actual\n",
    "        assert_same_shape(prediction, self.grad)\n",
    "        \n",
    "        return log_loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NeuralNetwork` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    '''\n",
    "    Neural network class. All the functions here are pretty straightforward.\n",
    "    '''\n",
    "    def __init__(self, layers: List[Layer], \n",
    "                 loss: Loss, \n",
    "                 learning_rate: float = 0.01, \n",
    "                 update_rule: str = \"sgd\") -> None:\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_rule = update_rule\n",
    "        \n",
    "    def forward(self, x_batch: Tensor) -> Tensor:\n",
    "        \n",
    "        assert_dim(x_batch, 2)\n",
    "        \n",
    "        x_out = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def batch_loss(self, \n",
    "                   prediction: Tensor, \n",
    "                   y_batch: Tensor) -> Tensor:\n",
    "        \n",
    "        # Key assertion\n",
    "        assert_same_shape(prediction, y_batch)\n",
    "        self.loss_val = self.loss.loss_grad(prediction, y_batch)\n",
    "        return self.loss_val\n",
    "    \n",
    "    def backward(self, loss_grad: Tensor) -> None:\n",
    "    \n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def train_batch(self, \n",
    "                    x_batch: Tensor,\n",
    "                    y_batch: Tensor) -> float:\n",
    "        \n",
    "        predictions = self.forward(x_batch)\n",
    "        \n",
    "        loss = self.loss.loss_grad(predictions, y_batch)\n",
    "        \n",
    "        self.backward(self.loss.grad)\n",
    "        \n",
    "        self.update_params()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _get_param_layers(self) -> List[Layer]:\n",
    "        return [layer for layer in self.layers if hasattr(layer, \"update_params\")]\n",
    "            \n",
    "    def update_params(self) -> None:\n",
    "        param_layers = self._get_param_layers()\n",
    "        for layer in param_layers:\n",
    "            layer.update_params(self.update_rule, \n",
    "                                self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep breast cancer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sklearn` loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()\n",
    "data = breast_cancer.data\n",
    "target = breast_cancer.target\n",
    "features = breast_cancer.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing to `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor, target_tensor = Tensor(data), Tensor(target).resize_(569, 1)\n",
    "target_tensor = one_col_to_two(target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate random batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to clean these functions up but they'll do for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = Tuple[Tensor, Tensor]\n",
    "\n",
    "def generate_batch(x: Tensor, \n",
    "                   y: Tensor, \n",
    "                   batch_size: int = 10) -> Batch:\n",
    "    \n",
    "    assert (x.dim() == 2) and (y.dim() == 2), \\\n",
    "    \"X and Y must be 2 dimensional\"\n",
    "    \n",
    "    indices = choose_random_row_indices(x, batch_size)\n",
    "    x_batch = select_rows(x, indices)\n",
    "    y_batch = select_rows(y, indices)\n",
    "    \n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_row_indices(x: Tensor, \n",
    "                              num: int = 10) -> array:\n",
    "\n",
    "    assert x.dim() == 2, \\\n",
    "    'x must be a 2D array'\n",
    "    \n",
    "    return random.choice(range(x.shape[0]), num, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_rows(data: Tensor, \n",
    "                indices: array) -> Tensor:\n",
    "\n",
    "    return Tensor(data.numpy()[indices, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(arr: array) -> array:\n",
    "    \n",
    "    assert arr.ndim == 2\n",
    "    \n",
    "    means = arr.mean(axis=0)\n",
    "    stds = arr.std(axis=0)\n",
    "    \n",
    "    return (arr - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standard = standardize_data(data)\n",
    "data_standard = Tensor(data_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nn: NeuralNetwork,\n",
    "          X: Tensor,\n",
    "          y: Tensor,\n",
    "          num_iter: int = 1000, \n",
    "          print_every: int = 200,\n",
    "          batch_size: int = 64) -> NeuralNetwork:\n",
    "\n",
    "    assert_dim(X, 2)\n",
    "    assert_dim(y, 2)\n",
    "    \n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    \n",
    "    random.seed(63018)\n",
    "    torch.manual_seed(63018)\n",
    "    for i in range(num_iter):        \n",
    "        x_batch, y_batch = generate_batch(X, y, batch_size)\n",
    "        nn.train_batch(x_batch, y_batch)\n",
    "        if print_every:\n",
    "            if i % print_every == 0:\n",
    "                predictions = nn.forward(data_standard)\n",
    "                loss = nn.batch_loss(predictions, target_tensor)\n",
    "                print(\"Loss after iteration {0} is {1}\".\\\n",
    "                      format(i, round(loss, 3)))\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(nn: NeuralNetwork, \n",
    "                  X: Tensor, \n",
    "                  y: Tensor) -> float:\n",
    "\n",
    "    assert_dim(X, 2)\n",
    "    assert_dim(y, 2)\n",
    "    \n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    \n",
    "    preds = nn.forward(X)[:, 0] > 0.5\n",
    "    y = y[:, 0]\n",
    "    \n",
    "    return round(accuracy_score(y, preds), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "num_iter = 500\n",
    "print_every = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 is 0.603\n",
      "Loss after iteration 100 is 0.345\n",
      "Loss after iteration 200 is 0.339\n",
      "Loss after iteration 300 is 0.337\n",
      "Loss after iteration 400 is 0.335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.986"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork(\n",
    "    layers=[Linear(n_out=1), \n",
    "            Sigmoid(),\n",
    "            SoftmaxLogistic()],\n",
    "    loss=CrossEntropy(),\n",
    "    learning_rate=0.01,\n",
    ")\n",
    "\n",
    "nn = train(nn, data_standard, target_tensor, \n",
    "           num_iter = num_iter, \n",
    "           print_every = print_every, \n",
    "           batch_size = batch_size)\n",
    "eval_accuracy(nn, data_standard, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 is 0.375\n",
      "Loss after iteration 200 is 0.326\n",
      "Loss after iteration 400 is 0.324\n",
      "Loss after iteration 600 is 0.323\n",
      "Loss after iteration 800 is 0.323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.991"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn2 = NeuralNetwork(\n",
    "    layers=[Linear(n_out=1), \n",
    "            Sigmoid(),\n",
    "            SoftmaxLogistic()],\n",
    "    loss=CrossEntropy(),\n",
    "    learning_rate=1,\n",
    ")\n",
    "\n",
    "nn2 = train(nn2, data_standard, target_tensor)\n",
    "eval_accuracy(nn2, data_standard, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real neural network with hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 is 0.474\n",
      "Loss after iteration 200 is 0.322\n",
      "Loss after iteration 400 is 0.321\n",
      "Loss after iteration 600 is 0.321\n",
      "Loss after iteration 800 is 0.32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.993"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn3 = NeuralNetwork(\n",
    "    layers=[Linear(n_out=10), \n",
    "            Sigmoid(),\n",
    "            Linear(n_out=1),\n",
    "            Sigmoid(),\n",
    "            SoftmaxLogistic()],\n",
    "    loss=CrossEntropy(),\n",
    "    learning_rate=1,\n",
    ")\n",
    "\n",
    "nn3 = train(nn3, data_standard, target_tensor)\n",
    "eval_accuracy(nn3, data_standard, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network with one hidden layer with 10 neurons and a learning rate of 1 does slightly better than a simple logistic regression.\n",
    "\n",
    "But more importantly - the code works! We have a framework we can use for Deep Learning! I hope it isn't hard to see how you could add in Droput, learning rate momentum, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
